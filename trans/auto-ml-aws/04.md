

# *第三章*:用自动旋转技术自动化复杂的模型开发

在 [*第 1 章*](B17649_01_ePub.xhtml#_idTextAnchor015) 、*AWS 上的自动化机器学习入门*中，向您介绍了 *ACME Fishing Logistics* 用例，其中您使用典型的 ML 流程创建了一个生产级 MLP 模型。虽然该示例仅突出了基本的人工神经网络架构，但它也提供了对深度学习概念的适当介绍。

深度学习是一种先进的 ML 技术，可用于解决复杂和具有挑战性的用例，如客户情绪分析、语言翻译和对象检测图像和视频。这些复杂的用例通常需要 ML 从业者创建非常复杂的，以及异常大的神经网络架构。这些架构中的一些可能有几十万甚至几十亿个可训练参数。网络越复杂，训练就越困难，因此自动化也就越困难。

正如我们在前一章强调的，SageMaker 自动驾驶器只支持表格数据。因此，不支持需要图像数据的更复杂的深度学习用例。那么，我们如何应用 AutoML 方法来自动化这些复杂用例的 ML 过程，这些用例需要深度学习模型？

在本章中，我们将研究如何通过使用 AutoGluon Python 库来实现这一点，并说明如何仍然可以使用 Amazon SageMaker 来有效地将 AutoML 方法应用于一些更复杂的深度学习用例。我们还将利用这个机会向您展示 SageMaker 的一些前沿功能，向您介绍 SageMaker 的**自带容器** ( **BYOC** )功能，以及 AWS **深度学习容器**。我们将在整本书中广泛使用这种能力。具体来说，我们将关注以下主题:

*   介绍 AutoGluon 库
*   对表格数据使用自动登录
*   对图像数据使用自动旋转

到本章结束时，你将对什么是自动增长库以及如何使用它有一个实际的理解。

# 技术要求

由于我们将再次使用 SageMaker Studio UI，本章的技术要求与第 2 章 、*中的 [*相同，使用 SageMaker Autopilot* :](B17649_02_ePub.xhtml#_idTextAnchor032)*

*   web 浏览器(为了获得最佳体验，建议您使用 Chrome 或 Firefox 浏览器)。
*   访问您在 [*第二章*](B17649_02_ePub.xhtml#_idTextAnchor032) 、*使用 SageMaker Autopilot* 自动化机器学习模型开发中使用的 AWS 帐户。
*   我们将再次在 AWS 免费层的使用限制内工作，以避免超出不必要的成本。

# 介绍自动增长库

AutoGluon 是由 AWS 开发的 Python 库，并在 2019 年的年度 *re:Invent conference* 上开源。自动旋转背后的主要设计目标类似于 SageMaker 的自动驾驶模块——解决一个 ML 从业者在典型的 ML 过程中面临的所有复杂性和挑战，并通过一个 Python 库解决这些问题。本质上，AutoGluon 使 ML 从业者能够组织他们的训练数据，并应用几种 ML 方法来生成优化的模型，所有这些都只需要几行代码。

AutoGluon 克服了 AutoPilot 的一些限制，因为它可以处理更复杂的 ML 用例，这些用例涉及复合类型的数据，如杂乱的文本数据和图像。当然，AutoGluon 也可以处理表格数据。AutoGluon 通过为每种数据类型创建单独的预测器，从而为数据类型支持的每种 ML 用例类型创建预测器来实现这一点。例如，自动引导包括以下预测值:

*   **表格预测器**:该预测器类似于自动驾驶器的功能，因为它用于创建优化的模型，以从表格数据中预测列值，就像自动驾驶器一样，这适用于分类和回归用例。
*   **图像预测器**:该预测器集中于生成模型，该模型预测整个图像的命名类别。例如，如果我们有一个包含猫和狗的标记图像的数据集，图像预测器将创建一个优化的模型来预测新图像是否属于*猫*或*狗*类别。
*   **物体检测器**:使用这个预测器建立一个优化的模型，能够清楚地识别单个图像中的不同物体。例如，如果我们提供一个使用物体探测器训练的模型一个*男孩*和他的*狗*的图像，这个模型将能够区分两个单独的物体。
*   **Text Predictor**: This predictor provides functionality similar to the Tabular Predictor in that it creates an optimized model to perform regression and classification prediction tasks on text data. For example, if a model were optimized using the Text Predictor, given a string of text, it would be able to classify the sentiment of the sentence.

    注意

    文本预测器使用表格训练数据进行分类和回归，其方式类似于表格预测器使用表格数据的方式。主要区别在于，表格预测器将显示文本，而文本预测器将直接适合原始文本。换句话说，表格预测器会将表格数据的文本列转换成一个向量(或数字)表示。另一方面，文本预测器将直接处理原始文本。

虽然这只是对 AutoGluon 的高级介绍，但了解其实际功能的最佳方式是通过一个实际操作的示例。让我们在 *ACME Fishing Logistics* 用例上试验一下自动旋转。

小费

有关这些内置预测器的更多信息，以及如何进行单个 Python 调用以将这些预测器拟合到原始数据中，您可以查看 AutoGluon 文档([https://auto.gluon.ai/stable/api/autogluon.predictor.html](https://auto.gluon.ai/stable/api/autogluon.predictor.html))。

# 对表格数据使用自动生成

在前一章中，我们使用自动驾驶查看了一个应用于 *ACME 渔业物流*用例的示例 AutoML 实验。在这个例子中，我们将使用旋翼飞行器来重现这个实验。因此，让我们看看如何使用自动旋转来自动完成这项任务。

注意

AutoGluon 表格库受益于具有尽可能多的内存的计算实例。因此，建议使用 AWS https://aws.amazon.com/ec2/instance-types/m5/ M5 实例()进行表格实验。在这个例子中，我们将使用一个`m5.xlarge`实例，因此，运行这个例子将会产生 AWS 资源成本。

## 先决条件

在我们开始之前，有几个基本主题需要考虑，即:

*   在撰写本文时，AutoGluon 库还没有作为 SageMaker 的内置估算器之一包含在内。这意味着我们将不得不使用 SageMaker BYOC 方法和 AWS 深度学习容器来为 AutoGluon 创建自己的 Docker 容器。
*   除非我们使用 SageMaker 管理的笔记本实例(https://docs . AWS . Amazon . com/sage maker/latest/DG/nbi . html ),这些实例对 Docker 守护进程具有本机支持，否则在使用 SageMaker Studio 时，不存在用于构建 Docker 容器的固有功能。

为了解决这些限制，同时仍然使用本示例的 Studio UI，AWS 提供了一个名为`sagemaker-studio-image-build`的开源 CLI 实用程序(https://github . com/AWS-samples/sage maker-Studio-image-build-CLI)。这个实用程序允许我们在 Studio UI 中构建一个兼容 SageMaker 的容器,这样，我们现在可以在 Studio 环境中构建和运行 AutoGluon 示例。在后台，`sagemaker-studio-image-build`库使用完全托管的构建服务 AWS CodeBuild 来构建 Docker 映像。要访问服务，Studio 执行角色需要适当的访问权限。

注意

如果你对 AWS CodeBuild 及其工作方式不熟悉，你可以参考产品网页([https://aws.amazon.com/codebuild](https://aws.amazon.com/codebuild))。

### 配置服务权限

以下步骤将带您了解如何为 SageMaker 执行角色配置适当的权限:

1.  Log in to your AWS account, navigate to the **Amazon SageMaker** management console, and click the **SageMaker Domain** link in the left-hand navigation panel.

    注意

    你应该已经登上了 SageMaker Studio。如果没有，参考 [*第二章*](B17649_02_ePub.xhtml#_idTextAnchor032) 、*使用 SageMaker 自动驾驶器自动化机器学习模型开发*中的*SageMaker Studio*部分入门。

2.  一旦 **SageMaker 域**屏幕打开，在**域**部分记下**执行**角色的名称。我们将使用执行角色的 **Amazon 资源名称** ( **ARN** )为其分配必要的权限。
3.  现在，在一个新的浏览器标签中打开身份和访问管理**控制台**(**IAM**)([https://console.aws.amazon.com/iam/home](https://console.aws.amazon.com/iam/home))。
4.  在左侧导航面板上，点击**访问管理**部分下的**角色**，打开**角色**仪表板。
5.  找到您在*步骤 2* 中记下的执行角色，并点击它。角色名应该以**Amazon management maker-execution role-XXX**开头。
6.  在角色的**摘要**仪表板中，点击**信任关系**选项卡，然后点击**编辑信任关系**按钮。
7.  删除现有的**策略文件**名称，并将以下策略粘贴到窗口中。这将为执行角色提供对 SageMaker 服务和 CodeBuild 服务的信任访问:

    ```
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Principal": {
            "Service": [
              "codebuild.amazonaws.com",
              "sagemaker.amazonaws.com"
            ]
          },
          "Action": "sts:AssumeRole"
        }
      ]
    }
    ```

8.  点击**更新信任策略**按钮。

既然我们已经有了访问 CodeBuild 服务的必要权限，现在我们可以使用 Studio UI 来准备定制的 SageMaker 容器。

### 构建深度学习容器

为了构建一个用于自动增长的定制容器，我们需要提供详细的构建说明。对于容器，这些构建指令包含在一个名为 Dockerfile 的文件中。

注意

如果你不熟悉如何构建 Docker 容器或者如何构造 Docker 文件，你可以参考 Docker 文件参考文档([https://docs.docker.com/engine/reference/builder](https://docs.docker.com/engine/reference/builder)/)。

然而，AWS 并没有从头构建一个`Dockerfile`，而是为 SageMaker 提供了预先构建的容器映像和`Dockerfile`引用，称为深度学习容器，或 DL 容器([https://aws.amazon.com/machine-learning/containers/](https://aws.amazon.com/machine-learning/containers/))。这些 DL 容器映像由 AWS 设计，以支持多个深度学习框架(TensorFlow、PyTorch 和 Apache MXNet)，并针对在 AWS 云上运行 ML 用例进行了优化。使用这些容器映像意味着您不必担心配置这些框架通常需要的所有必要的 Python 依赖项和版本。除了这些深度学习框架，AWS 还提供了一个预打包的 DL 容器用于自动登录([https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD # autoglon-inference-containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#autogluon-inference-containers))。

以下步骤将带您了解如何使用预构建的 AutoGluon 容器，并根据我们的要求对其进行定制:

1.  使用**亚马逊 SageMaker** 管理控制台，点击**打开 SageMaker Studio** 按钮。
2.  点击**打开工作室**链接，启动工作室 UI。
3.  在 Studio UI 中，点击左侧工具条中的文件夹图标。
4.  在文件夹导航面板中点击右键，点击**新建文件夹**。
5.  将新文件夹命名为 **Tabular** ，双击打开文件夹。
6.  In the menu bar, click **File** | **New** |**Notebook** and, when prompted, select the **Python 3 (Data Science)** kernel from the dropdown. Click the **Select** button to launch the **KernelGateway**. After a couple of minutes, the kernel will be ready.

    小费

    您可以创建一个新的 Jupyter 笔记本或使用来自配套 GitHub 存储库的示例笔记本([https://GitHub . com/packt publishing/Automated-Machine-Learning-on-AWS/blob/main/chapter 03/Tabular/AutoGluon % 20 Tabular % 20 example . ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-on-AWS/blob/main/Chapter03/Tabular/AutoGluon%20Tabular%20Example.ipynb))。

7.  在第一个代码单元中，我们将安装`sagemaker-studio-image-build`实用程序。使用下面的代码，我们调用 Python 可执行文件来运行 Python 包管理器并安装实用程序:

    ```
    %%capture
    import sys
    import warnings
    warnings.filterwarnings('ignore')
    %matplotlib inline
    !{sys.executable} -m pip install -U pip sagemaker-studio-image-build
    ```

8.  Next, we open a new code cell to build the AutoGluon training and test script. This script is, in essence, the runtime that will be executed with the custom SageMaker container to generate and test the various AutoGluon tabular models to determine the best-fitting model. Using the following code, we won't execute this script inside the Jupyter notebook code cell, but rather use the `%%writefile` Jupyter magic command to create a script file called `train.py`:

    ```
    %%writefile train.py
    import os
    import json
    import boto3
    import json
    import warnings
    import numpy as np
    import pandas as pd
    from autogluon.tabular import TabularDataset, TabularPredictor
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    prefix = "/opt/ml"
    input_path = os.path.join(prefix, "input/data")
    output_path = os.path.join(prefix, "output")
    model_path = os.path.join(prefix, "model")
    param_path = os.path.join(prefix, 'input/config/hyperparameters.json')
    ```

    小费

    如果你不熟悉 Jupyter 内置的魔法命令，比如`%%writefile`，你可以参考 Jupyter 文档网站([https://ipython . readthe docs . io/en/stable/interactive/magics . html # built-in-magic-commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html#built-in-magic-commands))来了解更多关于这些命令以及它们的用法。

9.  使用下面的代码，在当前代码单元中，我们现在定义`train()`函数。该函数接受各种参数，例如预测目标标签，并使`TabularPredictor()`适合该目标，而不是`training_dataset`。一旦预测器自动确定了最佳模型，我们将结果保存为一个`Fit_Summary.txt`文件:

    ```
    def train(params):
        label = params["label"]
        channel_name = "training"
        training_path = os.path.join(input_path, channel_name)
        training_dataset = TabularDataset(os.path.join(training_path, "training.csv"))
        predictor = TabularPredictor(label=label, path=model_path).fit(training_dataset)
        with open(os.path.join(model_path, "Fit_Summary.txt"), "w") as f:
            print(predictor.fit_summary(), file=f)
        return predictor
    ```

10.  在下面的代码中，虽然仍在当前代码单元格中，但我们定义了`test()`函数。该函数再次获取目标标签以及训练好的预测器，并且评估在`testing_data`上生成的值。评估结果保存为一个`Model_evaluation.txt`文件。`test()`功能还会生成最佳车型排行榜，并将该列表保存为`Leaderboard.csv`文件:

    ```
    def test(params, predictor):
        label = params["label"]
        channel_name = "testing"
        testing_path = os.path.join(input_path, channel_name)
        testing_dataset = TabularDataset(os.path.join(testing_path, "testing.csv"))
        ground_truth = testing_dataset[label]
        testing_data = testing_dataset.drop(columns=label)
        predictions = predictor.predict(testing_data)
        with open(os.path.join(model_path, "Model_Evaluation.txt"), "w") as f:
            print(
                json.dumps(
                    predictor.evaluate_predictions(
                        y_true=ground_truth,
                        y_pred=predictions,
                        auxiliary_metrics=True
                    ),
                    indent=4
                ),
                file=f
            )
        leaderboard = predictor.leaderboard(testing_dataset, silent=True)
        leaderboard.to_csv(os.path.join(model_path, "Leaderboard.csv"))
    ```

11.  在下面的代码中，仍然在当前代码单元中，我们定义了加载执行参数的主程序例程，调用`train()`函数来训练训练数据上的各种预测器，然后调用`test()`函数来评估测试数据集上的预测器性能:

    ```
    if __name__ == "__main__":
        print("Loading Parameters\n")
        with open(param_path) as f:
            params = json.load(f)
        print("Training Models\n")
        predictor = train(params)
        print("Testing Models\n")
        test(params, predictor)
        print("AutoGluon Job Complete")
    ```

12.  接下来，我们创建一个新的代码单元并应用相同的技术，使用`%%writefile` magic 命令来创建定制的容器构建指令或`Dockerfile`。下面的代码包含了 Docker 守护进程将用来构建容器的指令:

    ```
    %%writefile Dockerfile
    ARG REGION
    FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/autogluon-training:0.3.1-cpu-py37-ubuntu18.04
    RUN pip install -U pip
    RUN pip install bokeh==2.0.1
    RUN mkdir -p /opt/program
    RUN mkdir -p /opt/ml
    COPY train.py /opt/program
    WORKDIR /opt/program
    ENTRYPOINT ["python", "train.py"]
    ```

13.  现在，我们可以使用 build CLI 来创建客户容器:

    ```
    import boto3
    import sagemaker
    aws_region = sagemaker.Session().boto_session.region_name
    !sm-docker build --build-arg REGION={aws_region} .
    ```

容器应该需要大约 10 分钟来构建，来自 CodeBuild 执行的日志被重定向到并显示在 Jupyter 笔记本中。

注意

确保捕捉到`Image URI: 123456789012.dkr.ecr.us-west-2.amazonaws.com/sagemaker-studio-d-abcdefghijkl:default-1234567890123`。

您可能想知道前面的代码单元到底完成了什么。首先，让我们浏览一下`train.py`文件。在这个文件中，我们创建了两个主要的 Python 函数，`train()`和`test()`:

*   `train()`函数获取一个名为`training.csv`的训练数据集，并创建一个名为`predictor`的默认自动生成表格预测器。默认预测器产生几种不同类型的 ML 模型，这些模型通过对数据集的其他列进行训练来预测目标标签。这个过程类似于 Autopilot 的 *Auto* 设置，在 [*第 2 章*](B17649_02_ePub.xhtml#_idTextAnchor032) 、*使用 SageMaker Autopilot* 自动化机器学习模型开发的例子中使用。稍后，当我们执行自动旋转实验时，我们将看到这些默认模型到底是什么，以及它们在这些训练数据上的表现如何。
*   训练过程完成后，`train()`函数将这些模型作为自动旋转`TabularPredictor`对象返回。

`test()`函数将训练好的模型作为输入，然后评估由`train()`函数产生的各种自动生成的表格模型，并在一个名为`testing.csv`的测试数据集上评估它们。作为该过程的结果，`test()`函数存储每个模型的总体评估结果以及最佳模型的得分汇总。正如我们将在后面看到的，这些资产最终存储在 S3 以供审查。

现在，我们来回顾一下`Dockerfile`。如前所述，`Dockerfile`包含构建 Docker 容器的指令，该容器将执行`train.py`脚本。要执行的第一个构建命令是下载 AutoGluon DL 容器的命令。Docker 守护进程从一个名为

`763104351884.dkr.ecr.${REGION}.amazonaws.com/autogluon-training:0.3.1-cpu-py37-ubuntu18.04`，其中`${REGION}`是指定您当前使用的 AWS 区域的构建参数。

小费

有关包含最新 DLC 图像的公共 ECR 存储库的列表，请参考该项目的 GitHub 存储库([https://GitHub . com/AWS/deep-learning-containers/blob/master/available _ images . MD](https://github.com/aws/deep-learning-containers/blob/master/available_images.md))。

Docker 守护进程然后为 AutoGluon 安装必要的 Python 包，设置 SageMaker 所需的代码路径，并将训练脚本复制到容器中。

注意

为了在容器中安装自动旋翼机，我们使用了自动旋翼机文档中的默认安装要求([https://auto.gluon.ai/stable/index.html#installation](https://auto.gluon.ai/stable/index.html#installation))。

在 Dockerfile 文件的最后一部分，我们指定了容器的`ENTRYPOINT`，从而指示容器在启动时执行训练脚本。

最后，我们执行了`sm-docker build`命令，指定当前 AWS 区域作为构建参数，并指定 Dockerfile 文件在当前目录中的位置。由于我们没有提供任何进一步的参数，`sm-docker`采用默认设置。

小费

要查看可以用来代替默认设置的一些附加设置，请参考实用程序文档网站([https://github . com/AWS-samples/sage maker-studio-image-build-CLI](https://github.com/aws-samples/sagemaker-studio-image-build-cli))。

使用默认设置，CLI 会在后台自动调用 AWS CodeBuild。反过来，CodeBuild 执行下列任务:

*   它创建一个 ECR 存储库，以 Studio 域 ID 命名，例如，`sagemaker-studio-d-abcdefghijkl`。
*   它用一个惟一的图像标签构建容器图像，例如，`default-1234567890123`。
*   It uploads the image to the newly created repository.

    小费

    若要查看 CodeBuild 服务执行的生成过程和配置设置，可以在 CodeBuild 控制台(https://console . AWS . Amazon . com/code suite/code build/home)中查看和管理该过程。

既然构建过程已经完成，本质上，我们已经向 SageMaker 展示了我们自己的 AutoGluon 容器。在下一节中，我们将使用这个容器为 *ACME Fishing Logistics* 用例进行 AutoML 实验。

## 用自动旋转创建 AutoML 实验

与我们在 [*第二章*](B17649_02_ePub.xhtml#_idTextAnchor032)*中创建 AutoML 实验的方式相同，使用 SageMaker Autopilot* 自动化机器学习模型开发，使用 SageMaker SDK，我们将通过使用 AutoGluon 来重现类似的实验。以下步骤将指导您在现有的 Jupyter 笔记本中创建实验:

1.  首先，我们需要再次下载鲍鱼数据集。使用下面的 Python 代码，从 UCI 存储库中下载数据集，添加相关的列名，并将数据分成两个单独的 CSV 文件，`training.csv`和`testing.csv`。训练文件包含 90%的数据，而测试文件包含剩余的 10%。正如已经强调的，这两个数据集将被我们容器中的`train()`和`test()`函数使用。在新的代码单元格中，执行下面的示例代码:

    ```
    import numpy as np
    import pandas as pd 
    from sklearn.model_selection import train_test_split
    column_names = ["sex", "length", "diameter", "height", "whole_weight", "shucked_weight", "viscera_weight", "shell_weight", "rings"]
    abalone_data = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", names=column_names)
    training_data, testing_data = train_test_split(abalone_data, test_size=0.1)
    training_data.to_csv("training.csv")
    testing_data.to_csv("testing.csv")
    ```

2.  现在我们有了数据集，我们可以为实验配置各种参数。使用下面的代码，我们可以定义重要的参数，比如实验的名称(`job_name`)、跟踪实验的具体版本(`job_version`)、存储数据集的 SageMaker 默认 S3 桶、输出工件(`bucket`)和容器 URI ( `image_uri` ):

    ```
    import sagemaker
    import datetime
    image_uri = image_uri parameter, enter the URI from the output of the sm-docker code cell we executed earlier in *step 10* of the previous section.
    ```

3.  Using a new code cell, we use these parameters and the SageMaker SDK to create a SageMaker estimator. An estimator is a high-level interface for a SageMaker training job. The following code uses the generic `sagemaker.estimator.Estimator()` class, allowing us to create a training job using our custom AutoGluon container. As you can see, we also supply additional hyperparameters where we specify the type of compute instance (`ml.m5.xlarge`) to use to execute the training job as well as the parameters to be supplied to the `train.py` script, such as the target label in our dataset (`rings`):

    ```
    from sagemaker.estimator import Estimator
    autogluon = Estimator(
        image_uri=image_uri,
        role=role,
        output_path=f"s3://{bucket}/{job_name}",
        base_job_name=job_name,
        instance_count=1,
        instance_type="ml.m5.xlarge",
        hyperparameters={
            "label": "rings",
            "bucket": bucket,
            "training_job": job_name
        },
        volume_size=20
    )
    ```

    小费

    有关泛型`Estimator()`类的更多信息，请参考 SageMaker SDK 文档([https://SageMaker . readthe docs . io/en/stable/API/training/estimators . html # SageMaker . estimator . estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator))。

4.  既然已经定义了估计器，我们可以使用`fit()`方法调用 SageMaker，并让它使用我们的自定义 AutoGluon 容器执行训练作业。正如您在下面的代码中看到的，我们告诉 SageMaker 从哪里获得训练和测试数据，方法是使用`upload_data()`方法:

    ```
    autogluon.fit(
        inputs={
            "training": session.upload_data(
                "training.csv",
                bucket=bucket,
                key_prefix=f"{job_name}/input"
            ),
            "testing": session.upload_data(
                "testing.csv",
                bucket=bucket,
                key_prefix=f"{job_name}/input"
            )
        }
    )
    ```

    将这些数据集上传到 S3

在调用了`fit()`方法之后，SageMaker 将使用 **4vCPUs** 和 **16 GB** 的 RAM 实例化一个`ml.m5.xlarge`实例来执行我们定制的 AutoGluon 容器。容器运行时环境内部的输出被重定向到并显示在 Jupyter 记事本中。您可以查看输出的每一行，看看发生了什么。或者，*图 3.1* 提供了 SageMaker 培训工作输出的高级概述:

![Figure 3.1 – AutoGluon process overview
](img/B17649_03_01.jpg)

图 3.1–自动登录流程概述

从*图 3.1* 中可以看出，`train()`函数执行了五个具体步骤，`test()`函数执行了一个步骤。让我们检查每个步骤，并将其与输出相关联:

1.  The first step that AutoGluon does is to preprocess the data. Here, AutoGluon analyzes the data to try and determine the type of ML problem. For example, AutoGluon may determine that the ML problem is multiclass because the target label's data type is an integer and there are very few unique values observed. Once AutoGluon has determined the ML problem type, it further performs preprocessing of the input data for the specific model.

    小费

    如果你已经知道你要解决的 ML 的类型，你可以把它作为一个参数添加到 AutoGluon 的方法中。

2.  第二步涉及使用几个预构建的数据生成器来清理数据和设计新功能的自动登录。例如，AutoGluon 使用`FillNaFeatureGenerator`自动确定值的类型，以替换数据集中存在的任何缺失值，并使用`CategoryFeatureGenerator`对分类特征进行编码。这一步的最后一部分包括将处理过的数据集分成单独的训练集和测试集。
3.  第三步，AutoGluon 根据训练数据集为表格数据训练其预构建的 ML 模型。有关 AutoGluon Tabular 适合训练集的 10 个特定模型和 3 个集合模型的列表，请参考 auto glon 文档([https://auto . gluon . ai/stable/API/auto gluon . Tabular . models . html # module-auto gluon . Tabular . models](https://auto.gluon.ai/stable/api/autogluon.tabular.models.html#module-autogluon.tabular.models))。
4.  The final step of the `train()` process evaluates the trained models against the validation dataset to determine the model's overall validation score and see how each model generalizes to accurately predict the target label. The default metric for evaluation is `pickle` library.

    小费

    要更改默认评估指标，您可以将其指定为 AutoGluon 的`fit()`方法的一个名为`eval_metric`的参数。

5.  为了完成这个过程，然后执行`test()`函数，使用`testing.csv`数据集对看不见的数据进行最终评估。这最后一步为我们提供了每个已训练模型的总体性能，以生成最佳模型。评估的最终结果作为输出工件与*腌制的*模型一起被捕获。

一旦 SageMaker 执行了`train.py`脚本，模型和评估概要工件就被压缩并上传到 S3。SageMaker 显示执行培训任务花费了多长时间，因此显示了总的**计费秒数**。

现在，我们已经在鲍鱼数据集上创建并实现了自动生长表格实验，我们可以评估生成的模型，以确定哪些模型可以用于生产。下一节将展示如何做到这一点。

## 评估实验结果

正如上一节所强调的，模型评估结果和*腌制的*模型被捕获作为输出工件，称为`model.tar.gz`，并上传到 S3。使用现有的 Jupyter 笔记本，让我们看看这些工件，以评估 AutoML 实验的结果，并确定哪个模型最适合生产用例:

1.  下面的示例代码使用 SageMaker SDK 的`S3Downloader`类下载并提取 AutoGluon estimator 的输出工件到一个名为`extract`的文件夹，使用`model_data`属性:

    ```
    !mkdir extract sagemaker.s3.S3Downloader.download(autogluon.model_data, "./")
    !tar xfz ./model.tar.gz -C extract
    ```

2.  你可以翻看提取的内容，在`extract`文件夹中，可以看到各种评测报告，双击`models`文件夹可以看到*腌制的*模型神器。*图 3.2* 显示了提取的工件文件:

![Figure 3.2 – Extracted artifact folder structure
](img/B17649_03_02.jpg)

图 3.2–提取的工件文件夹结构

1.  我们将查看的第一个文件是`Leaderboard.csv`文件，并查看每个训练模型的总体性能评估。以下代码将模型排行榜作为熊猫数据框架打开，并按降序对模型进行排序:

    ```
    df = pd.read_csv("./extract/Leaderboard.csv")
    df = df.filter(["model", "score_test", "score_val"]).sort_values(by="score_val", ascending=False).reset_index().drop(columns="index")
    df
    ```

2.  现在，您可以查看自动生成表格预测器训练的模型。基于在测试数据集上预测准确性的最佳模型首先显示。*图 3.3* 显示了一个排行榜表的例子，正如你所看到的，`WeightedEnsemble_L2`([https://auto . gluon . ai/stable/API/autogluon . tabular . models . html # weighted densemble model](https://auto.gluon.ai/stable/api/autogluon.tabular.models.html#weightedensemblemodel))提供了*最佳*验证准确度得分(`score_val`)。

![Figure 3.3 – Example model leaderboard 
](img/B17649_03_03.jpg)

图 3.3–型号排行榜示例

注意

自动登录以*越高越好*的形式生成评估指标分数。所以评价分数越高，模型越好。

1.  Just like the Autopilot example in [*Chapter 2*](B17649_02_ePub.xhtml#_idTextAnchor032), *Automating Machine Learning Model Development Using SageMaker Autopilot*, we can visually compare the models in the leaderboard with code. However, AutoGluon Tabular automatically constructs a model comparison plot, as an output artifact, called `SummaryOfModels.html`. The following example code will display the plot in the Jupyter notebook:

    ```
    import IPython
    IPython.display.HTML(filename="./extract/SummaryOfModels.html")
    ```

    注意

    如果在*步骤 5* 中运行代码时`SummaryofModels.html`文件没有立即显示，请再次运行代码单元。

2.  *图 3.4* 显示了一个显示的`SummaryOfModels.html`文件的例子。通过将鼠标悬停在生成的散点图上并查看每个模型的元数据，与图进行交互。

![Figure 3.4 – Example summary of the models' scatterplot
](img/B17649_03_04.jpg)

图 3.4–模型散点图示例总结

我们再次使用了 AutoML 方法，这一次使用了 AutoGluon 表格预测器，来为我们的用例创建一个可行的生产级 ML 模型。与*典型的* ML 流程一样，ML 从业者可以向应用团队提供*最佳*模型，用于测试和集成到*年龄计算器*应用中。

你可能想知道的一件事是，为什么我们不在现有的 Jupyter 笔记本中简单地执行自动旋翼训练和评估过程。*为什么我们要创建一个定制容器，并作为 SageMaker 培训工作来运行整个流程？*

这个问题的答案基本上是成本。为了详细说明，在下一个例子中，我们将回顾一个需要`ImagePredictor`的用例。

# 对图像数据使用自动摄影

到目前为止，我们一直在探索基于**人工神经网络** ( **ANN** )算法的 AutoML 方法。然而，许多用例可能需要更复杂的算法，例如**卷积神经网络**(**CNN**)用于图像分类和图像识别，或者**长短期记忆** ( **LSTM** )网络，用于语音识别和文本数据。由于这些算法的复杂性，许多 ML 实践者可能不得不利用多台机器进行分布式训练，并可能利用多个GPU 来处理多维矩阵计算。在本节中，我们将从*年龄计算器*用例继续探讨如何使用自动增长将 AutoML 方法应用于图像分类用例。

注意

由于我们将利用基于 GPU 的 AWS 实例，运行该示例将超出 AWS 自由层的使用限制，因此会产生额外的成本。

## 先决条件

和前面的例子一样，为了利用 GPU 完成图像分类任务，我们需要构建一个自定义容器。AWS 再次为 Apache MXNet 提供了一个带有 GPU 支持的 DL 容器。因此，我们需要做的就是将适当的自动登录运行时构建到预构建的容器中。

小费

您可以创建一个新的 Jupyter 笔记本或使用来自配套 GitHub 存储库的示例笔记本([https://GitHub . com/packt publishing/Automated-Machine-Learning-on-AWS/blob/main/chapter 03/Image/AutoGluon % 20 Image % 20 example . ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-on-AWS/blob/main/Chapter03/Image/AutoGluon%20Image%20Example.ipynb))。

以下步骤将带您了解如何做到这一点:

1.  在前面示例的同一个 Studio 环境中，单击左侧边栏中的文件夹图标。
2.  在文件夹导航面板中点击右键，点击**新建文件夹**。
3.  将新文件夹命名为`Image`，双击打开文件夹。
4.  在菜单栏中点击**文件** | **新建** | **笔记本**，出现提示时，从下拉菜单中选择 **Python 3(数据科学)**内核。点击**选择**按钮启动**内核滑槽**。几分钟后，内核就准备好了。
5.  在第一个代码单元格中，我们将通过执行以下代码再次安装`sagemaker-studio-image-build`实用程序:

    ```
    %%capture
    import sys
    import warnings
    warnings.filterwarnings('ignore')
    %matplotlib inline
    !{sys.executable} -m pip install -U pip sagemaker-studio-image-build
    ```

6.  接下来，我们将构建自动驾驶训练和测试脚本。本质上，这个脚本是一个运行时，它将与定制的 SageMaker 容器一起执行，以生成和测试各种 AutoGluon `ImagePredictor`模型，从而确定最适合的模型。因为我们将代码单元的内容捕获到一个文件中，所以我们使用`%%writefile` Jupyter magic 命令来创建一个名为`train.py` :

    ```
    %%writefile train.py
    import os
    import json
    import boto3
    import json
    import warnings
    import numpy as np
    import pandas as pd
    from autogluon.vision import ImagePredictor
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    prefix = "/opt/ml"
    input_path = os.path.join(prefix, "input/data")
    output_path = os.path.join(prefix, "output")
    model_path = os.path.join(prefix, "model")
    param_path = os.path.join(prefix, "input/config/hyperparameters.json")
    ```

    的脚本文件
7.  在同一个代码单元中，我们定义了一个`train()`函数来捕获输入参数，并将一个`ImagePredictor()`拟合到`training_data`。我们还在一个名为`FitSummary.csv`的文件中捕获训练结果的摘要，并保存训练好的模型:

    ```
    def train(params):
        time_limit = int(params["time_limit"])
        presets = "".join([str(i) for i in list(params["presets"])])
        channel_name = "training"
        training_path = os.path.join(input_path, channel_name)
        training_dataset = ImagePredictor.Dataset.from_folder(training_path)
        predictor = ImagePredictor().fit(training_dataset, time_limit=time_limit, presets=presets)
        with open(os.path.join(model_path, "FitSummary.json"), "w") as f:
            json.dump(predictor.fit_summary(), f)
        predictor.save(os.path.join(model_path, "ImagePredictor.Autogluon"))
        return "AutoGluon Job Complete"
    ```

8.  最后，在同一个代码单元中，我们定义主程序来加载输入参数，通过调用`train()`函数来执行模型的训练，并捕获结果:

    ```
    if __name__ == "__main__":
        print("Loading Parameters\n")
        with open(param_path) as f:
            params = json.load(f)
        print("Training Models\n")
        result = train(params)
        print(result)
    ```

9.  As with the tabular example, we provide the build instructions for the custom container by creating a `Dockerfile`:

    ```
    %%writefile Dockerfile
    ARG REGION
    FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/autogluon-training:0.3.1-gpu-py37-cu102-ubuntu18.04
    RUN mkdir -p /opt/program
    RUN mkdir -p /opt/ml
    COPY train.py /opt/program
    WORKDIR /opt/program
    ENTRYPOINT ["python", "train.py"]
    ```

    注意

    我们再次使用 AWS 提供的`autogluon-training`容器(https://github . com/AWS/deep-learning-containers/blob/master/available _ images . MD)，但这一次，我们将使用图像的 GPU 版本，由`0.3.1-gpu-py37-cu102-ubuntu18.04`图像标记表示。使用 DL 容器意味着我们不必手动构建和配置 GPU 环境、CUDA 库()和运行时，因为 AWS 已经为我们做了这些。

10.  现在，我们可以使用构建 CLI 来创建客户容器:

    ```
    import boto3
    import sagemaker
    aws_region = sagemaker.Session().boto_session.region_name
    !sm-docker build --build-arg REGION={aws_region} .
    ```

构建容器大约需要 10 分钟，CodeBuild 执行的日志被重定向到 Jupyter 笔记本，并显示在其中。

注意

和前面的例子一样，一定要捕捉到`Image URI: 123456789012.dkr.ecr.us-west-2.amazonaws.com/sagemaker-studio-d-abcdefghijkl:default-1234567890123`。

如您所见，示例代码中执行的过程与我们为表格示例运行的过程非常相似，除了`train.py`脚本。在这里，我们使用 AutoGluon 的`ImagePredictor`类，而不是`TabularPredictor`类，因此本例中的`train()`函数采用一个预设列表，并在提供的图像数据集上拟合多个预训练和高度精确的 CNN 模型。`fit()`方法通过采用额外的超参数优化技术，自动尝试提高预训练模型的分类准确性，结果是优化的模型和一组用于再现性的模型优化参数。

与表格示例不同，我们没有使用`test()`函数，因为图像预测器使用 *90%/10%* 分割比率自动将图像数据集分割为训练和验证数据集。

让我们在下一节中通过创建一个实验来看看这一点。

## 创建图像预测实验

对于这个实验，我们将使用由*劳伦斯·莫罗尼*([https://laurencemoroney.com/datasets.html](https://laurencemoroney.com/datasets.html))好心提供的*石头剪刀布*数据集。

注意

该数据集根据 Creative Commons 2.0 attribute 2.0 un ported 许可证(https://Creative Commons . org/licenses/by/2.0/)进行许可。

这个数据集包括不同手势的**计算机生成图像** ( **CGI** )，表明是石头、布还是剪刀的姿势。*图 3.5* 显示了这些姿势的一个例子:

![Figure 3.5 – Examples of the Rock Paper Scissors dataset
](img/B17649_03_05.jpg)

图 3.5-石头剪子布数据集示例

与我们使用 SageMaker SDK 创建表格实验的方式相同，我们将通过在现有的 Jupyter 笔记本中完成以下步骤来重现一个类似的实验:

1.  第一步是从劳伦斯·莫罗尼的网站([https://storage . Google APIs . com/laurencemoroney-blog . app spot . com/RPS . zip](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip))下载训练图像数据。由于数据集是以压缩 ZIP 文件的形式提供的，我们还需要在本地提取它。下面的示例代码展示了这是如何实现的:

    ```
    import io
    import urllib
    import zipfile
    dataset_url = "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip"
    with urllib.request.urlopen(dataset_url) as rps_zipfile:
        with zipfile.ZipFile(io.BytesIO(rps_zipfile.read())) as z:
            z.extractall("data")
    ```

2.  下载完数据集后，您应该会看到一个`data`文件夹，其中包含每种图像类型或分类的各个子目录。例如，您将看到一个名为`rock`的子目录，其中包含描述岩石姿势的训练图像。AutoGluon 将自动使用这些子目录作为目标标签来对图像进行分类。接下来，我们为实验配置各种参数。使用下面的代码，我们可以定义重要的参数，比如实验的名称(`job_name`)、跟踪实验的具体版本(`job_version`)、存储数据集和输出工件的 SageMaker 默认 S3 桶(`bucket`)，以及容器 URI ( `image_uri` ):

    ```
    import sagemaker
    import datetime
    image_uri = sm-docker code cell we executed in *step 8* of the previous section.
    ```

3.  现在我们已经配置了各种实验参数，我们可以创建自动增长估计器。以下示例代码应用了一个类似于表格示例的过程，除了一些超参数:

    ```
    from sagemaker.estimator import Estimator
    autogluon = Estimator(
        image_uri=image_uri,
        role=role,
        output_path=f"s3://{bucket}/{job_name}",
        base_job_name=job_name,
        instance_count=1,
        instance_type="ml.p2.xlarge",
        hyperparameters={
            "presets": "medium_quality_faster_train",
            "time_limit": "600",
            "bucket": bucket,
            "training_job": job_name
        },
        volume_size=50
    )
    ```

4.  执行实验的最后一步是调用 SageMaker 并让它执行训练作业。就像表格示例一样，下面的代码在估计器上执行`fit()`方法，并通过将数据集上传到 S3 来告诉 SageMaker 从哪里获取图像数据:

    ```
    autogluon.fit(
        inputs={
            "training": session.upload_data(
                "data/rps",
                bucket=bucket,
                key_prefix=f"{job_name}/input"
            )
        }
    )
    ```

一旦调用了`fit()`方法，SageMaker 将提供一个基于 GPU 的实例(`ml.p2.xlarge`)，初始化基于 GPU 的容器映像，并执行`train()`函数。作为该过程的一部分，AutoGluon `ImagePredictor`将根据数据集中的子目录确定独立图像类别的数量，并开始下载各种预训练的 CNN 模型来执行超参数优化任务。特定的预训练模型和超参数由`medium_quality_faster_train`控制，作为预设之一。该预设将仅使用 *resnet50* 预训练模型来提供中等预测精度以及非常快的推断和训练时间。

注意

我们选择使用`medium_quality_faster_train`预设，并将时间限制设置为 10 分钟(`600`秒)，以减少与运行实验相关的 AWS 使用成本。AutoGluon 提供了许多替代预设，这些预设将提供质量更好的模型，但会产生额外的 AWS 使用成本。您可以通过参考`ImagePredictor`文档(https://auto . gluon . ai/dev/API/autogluon . task . html # autogluon . vision . image predictor . fit)了解更多关于可用的附加预设配置。

当培训工作完成后，下一步是评估结果。

## 评估实验结果

正如我们在表格示例中看到的，SageMaker 将在 S3 存储最终的模型工件。继续阅读 Jupyter 笔记本，以下步骤将带您了解如何评估 AutoML 实验:

1.  通过运行下面的代码将`model.tar.gz`工件下载并解压到 Studio 环境中的一个名为`extract`的文件夹中:

    ```
    !mkdir extract
    sagemaker.s3.S3Downloader.download(autogluon.model_data, "./")
    !tar xfz ./model.tar.gz -C extract
    ```

2.  模型工件包含两个文件，`ImagePredictor.Autogluon`和`FitSummary.json`。我们可以通过运行下面的代码并查看`FitSummary.json`文件来研究模型训练总结:

    ```
    import json
    with open("extract/FitSummary.json", "r") as f:
        fit_summary = json.load(f)
    print(json.dumps(fit_summary, indent=4))
    print(f"""Best Model Training Accuracy: {fit_summary["train_acc"]} \nBest Model Validation Accuracy: {fit_summary["valid_acc"]}""")
    ```

在执行了前面的代码单元之后，您应该会看到类似于以下 JSON 代码片段的输出:

```
...
    "best_config": {
        "model": "resnet50d",
        "lr": 0.01,
        "num_trials": 1,
        "epochs": 50,
        "batch_size": 64,
        "nthreads_per_trial": 128,
        "ngpus_per_trial": 8,
        "time_limits": 600,
        "search_strategy": "random",
        "dist_ip_addrs": null,
        "log_dir": "/opt/program/85cde890",
        "searcher": "random",
        "scheduler": "local",
        "early_stop_patience": 5,
        "early_stop_baseline": -Infinity,
        "early_stop_max_value": Infinity,
        "num_workers": 4,
        "gpus": [0],
        "seed": 206,
        "final_fit": false,
        "wall_clock_tick": 1640286670.9024706,
        "problem_type": "multiclass"
    },
...
```

从这个 JSON 片段中可以看出，`resnet50d`模型提供了最好的配置。JSON 片段中包含的是最佳的超参数配置，我们可以在无需运行另一个 AutoML 实验的情况下再次重现模型。此外，如果您查看 JSON 输出的最后几行，您将看到最佳模型的评估结果。以下代码片段显示了模型准确性指标的一个示例:

```
...
Best Model Training Accuracy: 0.8929924242424242 
Best Model Validation Accuracy: 0.996031746031746
...
```

从这个最后的片段中，您可以看到`resnet50d`模型实现了一个`ImagePredictor.Autogluon`工件，存储在`extract`文件夹中，可以提供给应用程序开发团队。

因此，通过这个例子，我们实现了两个主要目标:

*   我们已经创建了一个 AutoML 实验来解决一个复杂的用例(计算机视觉)，需要一个更复杂的 ML 算法，比如 CNN 模型。正如表格示例一样，我们使用 AutoGluon 库为图像数据生成最佳模型。
*   虽然 Studio UI 提供了运行基于 GPU 的**kernel gateway**([https://docs . aws . Amazon . com/sagemaker/latest/DG/notebooks-available-instance-types . html](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html))的能力，但我们进一步减轻了运行 Jupyter 笔记本的不必要的 AWS 成本，同时探索了如何运行 AutoGluon 模型和配置用于 GPU 管理的各种 CUDA 库。相反，我们创建了一个训练*运行时工件*，作为来自预构建的 DL 容器的定制映像，并将 AutoML 处理卸载给 SageMaker 训练作业。

在后面的章节中，我们将利用构建运行时工件的技术来进一步简化 ML 自动化过程。

# 总结

本章向您介绍了使用 AutoGluon Python 库创建 AutoML 流程的开源替代方案。我们还使用 AutoGluon 的表格预测器来推进*年龄计算器*用例，并演示了如何为表格数据集找到最合适的模型。

我们进一步扩展了 AutoML 方法，通过为*石头剪子布*数据集找到最适合的 CNN 模型来解决复杂的计算机视觉用例。这是使用 AutoGluon 的图像预测器完成的，并使用 SageMaker 基于 GPU 的 ML 实例进一步优化。本章还介绍了容器映像形式的运行时流程工件的概念。

在下一章中，我们将继续阐述这个概念，并介绍 ML 运行时工件如何进一步简化 ML 过程，尤其是当工件与其他 AWS 服务结合使用时。