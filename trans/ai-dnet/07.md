# 7.与语音 API 交互

如今，我们与设备交互的方式已经发生了巨大的变化。每天都有许多新方法出现，它们足以影响我们的日常生活。这些互动中的大多数在十年前或更早的时候会是一个奇迹。在这些创新中，语音变得非常受欢迎。一个有趣的事情是，作为一种交流媒介，声音的优势在于它更快，当然也更自然。今天，越来越多的设备内置了语音识别和语音合成功能。事实上，已经有很多利用这些智能系统和交互的系统可用并投入使用。想想像 Cortana 和 Siri 这样的个人数字助理，它们完全改变了我们与移动应用程序的交互方式。再举一个智能汽车的例子。我们已经有语音控制的汽车很长时间了，但创新的是自然的交互。所有这些集成的东西正在使系统变得更智能，更具交互性。

如果您曾经考虑过使用语音创建一个基于可访问性的应用程序；如果你曾经想过创造基于语音识别的软件；如果你曾经想过，识别人的声音有多难；如果你曾经想过和一台机器说话，然后让它和你说话；或者，如果你曾经想过创建一个免提应用程序会有多复杂，那么这一章将会解决你的大部分疑问。由于认知服务世界的不断发展，这些交互很容易在任何应用程序中实现。事实上，在这一章中你会看到一些使用微软语音的交互。微软语音 API 伴随着 20 多年的语音研究。

Note

早在 1995 年，微软的第一个语音 API 版本(也称为 SAPI)是 Windows 95 的一部分。从那以后，人们对语音有了极大关注，各种版本的语音已经发布，各种操作系统提供了语音到文本和文本到语音的功能。随着认知服务中语音 API 的出现，微软提供了云上可用的最佳语音功能，它们可以在易于使用的 REST APIs 中使用。

让我们了解一下语音 API 的各种风格。在本章结束时，您将了解如何

*   使用语音 API 来识别语音
*   使用语音 API 让机器说话
*   使用语音 API 来识别声音

## 与语音互动的方式

根据您希望在哪里托管应用程序及其使用情况，Visual Studio 2017 中有几个语音风格选项。

1.  如果您正在使用 Windows 10，并且希望开发一个主要部署在通用 Windows 平台上的应用程序，并且在断开连接的情况下工作，您应该主要使用语音识别器类。
2.  如果您的应用程序需要使用一些其他平台，或者您希望将其部署在云环境中，微软有两种选择:
    1.  下载认知语音 SDK
    2.  通过调用认知 REST API 来使用功能 

为了使用客户端应用程序中的语音功能，您必须安装 Bing 语音 NuGet 包。右键单击您的项目并选择 Manage NuGet Packages 选项，如图 [7-1](#Fig1) 所示。

![A440212_1_En_7_Fig1_HTML.jpg](A440212_1_En_7_Fig1_HTML.jpg)

图 7-1。

Managing a NuGet package in Visual studio

在下一个窗口中，将包源设置为 Nuget.org。搜索“必应语音”您必须安装 Microsoft。Bing.speech 包如图 [7-2](#Fig2) 所示。

![A440212_1_En_7_Fig2_HTML.jpg](A440212_1_En_7_Fig2_HTML.jpg)

图 7-2。

The Bing NuGet package found in NuGet Package Manager Note

认知服务来自牛津项目。如果你在 Oxford speech 上搜索，你可能会得到一个 x86 版本和一个 X64 版本的 NuGet 包。一些玩认知语音的开发者可能对 Oxford Speech NuGet 包很熟悉。我们强烈建议使用`Microsoft.Bing.speech`。

安装 NuGet 语音包提供了一个使用强类型对象创建语音应用程序的机会。一旦选择了项目，整个语音库就驻留在名称空间`Microsoft.Bing.speech`中。对客户端库的访问没有详细介绍，但是大部分功能的工作方式与 REST API 相似，我们将在本章中详细介绍。

### 认知搜索 API

Speech API 提供了用语音驱动的场景增强应用程序的最简单的方法。Speech API 通过抽象所有复杂的语音算法提供了易用性，并提供了一个易于使用的 REST API。概括地说，语音 API 的功能可以分为三个主要方面:

*   Bing 语音 API
    *   语音识别:识别语音并将其转换为文本
    *   语音合成:将文本转换成音频
    *   理解语音的意图
*   自定义语音服务(以前称为 CRIS)
    *   自定义语言模型
    *   定制音频模型
    *   将模型部署到端点
*   说话人识别

### 语音识别

语音识别，也称为语音到文本，允许您通过识别引擎在应用程序中处理用户所说的话，并将其转换为文本形式。这种文本转录最终可以用于各种目的。例如，语音到文本 API 的一个很好的用例是为聋人创建一个可访问的移动解决方案，这样他们就可以阅读他人所说的文本。事实上，语音识别的领域可以是你希望用户说话而不是打字的任何地方。任何免提应用程序都需要语音识别功能。这些口语单词可以直接来自麦克风、扬声器，甚至来自音频文件。所有这些都是可能的，甚至不需要输入一个字符。

这里有一个关于语音识别的苦涩事实:全球大多数语音识别引擎都不是 100%正确的。然而，语音识别在最近几年确实有了很大的改进。让这些识别引擎理解会话语音还有一段路要走。事实上，当我们在写这本书的时候，马克·扎克伯格(脸书的首席执行官)正在创造贾维斯。语音识别系统最近有所改进，但是还没有一个人工智能系统能够很好地理解对话语音。语音识别既依赖于听你说什么，也依赖于预测你接下来会说什么，因此结构化的语音仍然比非结构化的对话更容易理解。”—马克·扎克伯格

这个有趣的引用有很多有意义的证词要解决，当然不是让你担心，而是提出一个关于演讲的观点。这个领域每天都有新的研究和发明发生，所以当你读到这本书的时候，这个评论可能已经不正确了。手指交叉！

## 入门指南

认知服务完成了抽象算法所有内部用法的惊人工作。所有调用都通过 API 进行，该 API 托管在 Azure 云服务器上，由非常强大的深度学习和幕后工作的神经网络的组合来支持。为了使用 Bing Speech REST API，您需要遵循以下步骤:

1.  通过调用令牌服务获取 JSON Web 令牌(JWT)。
2.  将 JWT 令牌放在头部，并调用 Bing 语音 API。
3.  解析文本。

## 首先获取 JSON Web 令牌

在响应用户之前，对 API 的每个调用都需要进行身份验证。所有对语音端点的请求都要求`access_token`作为授权头传递。使用语音端点的第一步是获取您的访问令牌。`access_token`是在语音请求报头中作为 base 64 字符串传递的 JWT 令牌。要获得 JWT 令牌，用户需要向令牌服务发送 POST 消息以及订阅密钥，如下所示:

```py
POST https://api.cognitive.microsoft.com/sts/v1.0/issueToken HTTP/1.1
Host: api.cognitive.microsoft.com
Ocp-Apim-Subscription-Key:••••••••••••••••••••••••••••••••

```

清单 [7-1](#Par39) 展示了一种在 C#中获取新令牌的方法。你可以重用在第 [2](02.html) 章开发的相同解决方案。在代码中，首先创建一个 HTTP 客户端的实例，然后在对令牌站点进行 POST 调用之前，将您的订阅密钥添加为标头。

```py
private static string GetFreshToken ()
        {
            using (var client = new HttpClient())
            {
                client.DefaultRequestHeaders.Add("Ocp-Apim-Subscription-Key", ApiKey);

                var response = client.PostAsync("https://api.cognitive.microsoft.com/sts/v1.0/issueToken", null).Result;

                return response.Content.ReadAsStringAsync().Result;
            }
        }

Listing 7-1.Getting a New Token

```

令牌的到期时间为 10 分钟，因此您的解决方案需要确保每 10 分钟检索并激活一次令牌。有多种方法可以解决这个问题。清单 [7-2](#Par41) 在当前令牌终止之前刷新它。这个策略也将刷新当前令牌。它同样会计划自己在最近获得的令牌过期一分钟之前再次运行。

```py
private const int TokenExpiryInSeconds = 600;
private string Token;

private void getValidToken()
        {
            this.token = GetFreshToken();
            this.timer?.Dispose();
            this.timer = new Timer(
                x => this. getValidToken(),
                null,
//Specify that token should run after 9 minsTimeSpan.FromSeconds(TokenExpiryInSeconds).Subtract(TimeSpan.FromMinutes(1)),
TimeSpan.FromMilliseconds(-1));
// Indicates that this function will only run once
        }

Listing 7-2.Refreshing the Current Token

```

您尚未添加代码来获得对当前令牌的独占访问。如果您有一个多线程应用程序，您可能希望在调用`getValidToken`方法之前锁定令牌。

使用正确的订阅密钥调用令牌服务将返回内容类型为`application/jwt`的 JWT 令牌`text/Plain`。然后，JWT 令牌返回可用于呼叫语音端点以进行后续呼叫。

在写这本书的时候，自由层的使用一直在变化；每月高达 5K 的交易呼叫是免费的，每分钟 20 次呼叫。前往 [`www.microsoft.com/cognitive-services/en-us/subscriptions`](http://www.microsoft.com/cognitive-services/en-us/subscriptions) 了解更多详情。

## 消费者语音 API

所有音频都通过 HTTP POST 传递到语音端点。收到访问令牌后，第二步是使用附加的 HTTP 查询参数向 [`https://speech.platform.bing.com/recognize`](https://speech.platform.bing.com/recognize) 发出另一个 POST 请求。所有这些参数在表 [7-1](#Tab1) 中有详细描述。

表 7-1。

Parameters Used to Call the Bing Speech API

<colgroup><col> <col></colgroup> 
| 名字 | 描述 |
| --- | --- |
| `Version` | 客户端使用的 API 版本。对于大部分需求，我们一直使用 API 3.0 版，这是最新的版本。 |
| `RequestId` | API 调用的全局唯一 id。使用 GUID 是个好主意。 |
| `appID` | 必须是 d4d 52672-91d 7-4c 74-8ad 8-42b1d 98141 a 5。 |
| `Format` | 您希望返回的数据的格式。对于大多数场景，我们建议使用`format=json`。您还可以选择其他选项，如 XML。 |
| `Locale` | 正在传递的音频的语言代码。它是大小写不可知的。支持 28 种语言环境，这将在一段时间内增加。对于我们的大多数演示，我们使用`locale=en-us`。 |
| `Device.os` | 指定发出调用的操作系统。选项有 Window OS、Window Phone OS、Android、iPhone OS 和 Xbox。 |
| `Instanceid` | 唯一标识发出请求的设备的 GUID |

清单 [7-3](#Par47) 展示了如何使用适当的参数调用语音端点，获取音频，将其转换为文本，然后返回给用户。

```py
private GUID instanceID;
public async Task<string> GetTextFromAudioAsync(Stream audiostream)
        {
            var requestUri = @"https://speech.platform.bing.com/recognize?scenarios=smd&appid=D4D52672-91D7-4C74-8AD8-42B1D98141A5&locale=en-US&device.os= WindowsOS&version=3.0&format=json&instanceid=instanceID3&requestid=" + Guid.NewGuid();

            using (var client = new HttpClient())
            {
                var token = this.getValidToken();
                client.DefaultRequestHeaders.Add("Authorization", "Bearer " + token);

                using (var binaryContent = new ByteArrayContent(StreamToBytes(audiostream)))
                {
                    binaryContent.Headers.TryAddWithoutValidation("content-type", "audio/wav; codec=\"audio/pcm\"; samplerate=18000");

                    var response = await client.PostAsync(requestUri, binaryContent); 

                    var responseString = await response.Content.ReadAsStringAsync();
                    dynamic data = JsonConvert.DeserializeObject(responseString);
                    return data.header.name;
                }
            }

Listing 7-3.Calling a Speech Endpoint

```

如果您查看这段代码，首先要做的是创建一个 URL，指向一个带有必要参数的语音端点。确保每个参数只使用一次很重要，否则您将得到 HTTP 400 的错误响应。在呼叫语音端点之前，确保您的订阅密钥有效也很重要；否则，您将得到 HTTP 403 的错误响应。如前所述，JWT 令牌每 10 分钟到期一次，因此在调用上述方法之前调用前面创建的`getValidToken`方法以确保您的令牌有效是很重要的。一旦验证了 JWT 令牌，就需要将它作为以字符串 Bearer 为前缀的授权头进行传递。所有的音频都需要从模拟转换成数字版本。为了将音频从模拟版本转换为数字版本以传递给语音识别 API，使用了编解码器。语音识别 API 支持三种类型的编解码器:

*   脉冲编码调制
*   汽笛
*   汽笛

您应该在大多数情况下使用 PCM，除非您需要在会议等场景中进行近乎实时的转录，在这种情况下，您需要使用 Siren 编解码器。然后调用 Speech API，它返回一个 JSON 响应。您可以使用任何 JSON 转换器将其反序列化为 in。NET 对象。

## 语音合成

语音合成，也称为文本到语音(TTS)并不是一个新概念。自 18 世纪以来，机器一直在以这样或那样的形式使用语音合成功能。TTS 允许你通过一个语音合成引擎向用户复述单词或短语。现在几乎每台 Windows 机器都有一个内置的语音合成器，可以将文本转换成语音。这个内置的合成器对那些看不懂屏幕上打印的文字的人特别有用。然而，由于这些系统不是经常升级，并且依赖于系统的内存，它们更适合处理简单到稍微复杂的场景。为了构建企业系统，基于语音的模型应该符合以下先决条件:

*   应该很容易使用
*   应该在一段时间内提高速度
*   应该不断升级，以提高性能和准确性
*   应该能够进行复杂的计算
*   应该性价比高
*   应该统一适用于所有平台，甚至是低端移动设备

Bing 文本到语音 API 在一个易于使用的 REST API 中提供了所有这些特性。到目前为止，您已经熟悉了使用文本到语音转换 API。

与 TTS 类似，Bing 语音到文本的所有交互都是通过 HTTP Post 完成的。对文本到语音转换 API 的所有请求都需要一个 JWT 令牌。我们已经在前一节中介绍了如何获得 JWT。需要理解的一个关键点是，Bing 文本到语音 API 支持语音合成标记语言(SSML)。

Note

在当今世界，语音合成可以通过多种方式应用于各个地方。回顾历史，大多数大型软件公司都致力于使用与特定平台紧密结合的专有协议来开发语音。这个软件受制于语音相关方面的问题，如发音和音高不一致。启用语音相关系统的能力是许多组织的关键需求。此外，新开发的应用程序需要使现有平台可以统一使用语音，企业需要实现语音的统一。早在 21 世纪初，这是所有主要软件供应商的主要问题，他们都希望使用一套被广泛接受和采用的协议。为了解决这个问题，微软、IBM、BEA 和 Sun 等行业领导者提出了名为语音合成标记语言的 W3C 规范。SSML 提供了创建基于语音标记文本的统一方法。在 [`www.w3.org/TR/speech-synthesis`](http://www.w3.org/TR/speech-synthesis) 查看完整的 SSML 语法的官方规范。

调用文本到语音 API 类似于使用语音到文本 API。您首先需要获得一个有效的令牌。为此，您可以重用您的`getValidToken()`方法。接下来，向 [`https://speech.platform.bing.com/synthesize`](https://speech.platform.bing.com/synthesize) 发出 POST 请求。在本书中你已经多次看到了 POST 和 GET 请求是如何使用`HttpClient`在 C#中工作的。对于您的文本到语音代码示例，您将使用一个流行的第三方 HTTP 库 RestSharp。通过 NuGet 支持在 Visual Studio 中轻松安装 RestSharp。考虑上市 [7-4](#Par64) 。

```py
public byte[] convertTextToSpeech(string ssml)
{
    var token = this.getValidToken();
    var client = new RestClient("https://speech.platform.bing.com/synthesize");
    var request = new RestRequest(Method.POST);
    request.AddHeader("authorization", "Bearer " + token);
    request.AddHeader("x-search-clientid", "8ae9b9546ebb49c98c1b8816b85779a1");
    request.AddHeader("x-search-appid", "1d51d9fa3c1d4aa7bd4421a5d974aff9");
    request.AddHeader("x-microsoft-outputformat", "riff-16khz-16bit-mono-pcm");
    request.AddHeader("user-agent", "MyCoolApp");
    request.AddHeader("content-type", "application/ssml+xml");
    request.AddParameter("application/ssml+xml", ssml, ParameterType.RequestBody);
    IRestResponse response = client.Execute(request);
    return response.RawBytes;
}
Listing 7-4.Code

```

正如你在上面看到的，RestSharp 有一个外观简洁的 API 来设置 HTTP 请求对象。请求体是 SSML 格式的字符串。没有要设置的请求参数。相反，您设置了几个标题。可能的标题细分如下:

*   `Authorization`:必选。由 IssueToken API 生成的有效令牌。
*   `x-search-appid`:可选。唯一标识应用程序的无连字符 GUID。
*   `x-search-clientid`:可选。一个无连字符的 GUID，它唯一地标识每个安装的应用程序。
*   `x-microsoft-outputformat`:必选。输出音频文件的格式。可以是下列之一
    *   `ssml` `-16khz-16bit-mono-tts`
    *   `raw-16khz-16bit-mono-pcm`
    *   `audio-16khz-16kbps-mono-siren`
    *   `riff-16khz-16kbps-mono-siren`
    *   `riff-16khz-16bit-mono-pcm`
    *   `audio-16khz-128kbitrate-mono-mp3`
    *   `audio-16khz-64kbitrate-mono-mp3`
    *   `audio-16khz-32kbitrate-mono-mp3`
*   用户代理:必需。您的应用程序的名称。
*   内容类型:可选。唯一推荐的值是“`application` `/ssml+xml`”。

`convertTextToSpeech()`方法返回由文本到语音 API 作为响应发送回来的音频的字节数组。你用字节数组做什么？让我们先看看如何调用上述方法，以及有效的 SSML 字符串是什么样子的:

```py
string ssml = "<speak version='1.0' xml:lang='en-US'><voice xml:lang='en-US' xml:gender='Female' name='Microsoft Server Speech Text to Speech Voice (en-US, ZiraRUS)'>Hello, how may I help you?</voice></speak>";
byte[] TTSAudio = this.convertTextToSpeech(ssml);
SoundPlayer player = new SoundPlayer(new MemoryStream(TTSAudio));
player.PlaySync();

```

最常见的用例是在收到响应时立即播放音频。上面的代码将转换文本“您好，我能为您做些什么？”变成语言。`SoundPlayer`是内置的。NET 类来播放音频文件和流。该音频文件的格式由`x-microsoft-outputformat`头的值决定。由于 SoundPlayer 只支持 WAV 音频文件，所以您使用“`riff-16khz-16bit-mono-pcm`”作为`outputformat`头的值。

您可以编写一个单独的方法来生成有效的 SSML 字符串。清单 [7-5](#Par84) 展示了一种生成 SSML 的方法。

```py
private string GenerateSsml(string locale

, string gender, string name, string text)
{
    var ssmlDoc = new XDocument(
                        new XElement("speak",
                            new XAttribute("version", "1.0"),
                            new XAttribute(XNamespace.Xml + "lang", "en-US"),
                            new XElement("voice",
                                new XAttribute(XNamespace.Xml + "lang", locale),
                                new XAttribute(XNamespace.Xml + "gender", gender),
                                new XAttribute("name", name),
                                text)));
    return ssmlDoc.ToString();
}

Listing 7-5.Generating SSML

```

`text`是要转换成语音的文本。`gender`可以是女性也可以是男性。要获得区域设置和名称可能值的完整列表，请查看位于 [`https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoiceoutput`](https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoiceoutput) 的官方文本到语音 API 参考文档。

### 语音识别内部

在高层次上，识别引擎从用户的麦克风获取音频输入或语音，并通过尝试将音频信号与不同数据库中的模式进行匹配来处理它。然后，这些信号模式与已知的单词相关联，如果引擎在数据库中找到匹配的模式，它会将相关的单词作为文本返回(参见图 [7-3](#Fig3) )。演讲术语简要介绍如下:

*   语音中的单个声音被称为音素。音位也可以被称为一种语言的单位。例如，单词“Nishith”由三个音素组成:“Ni”、“shi”和“th”
*   将声波转换成音素的过程称为声学建模。这类似于将声波映射到一个或多个语音单元。如图 [7-3](#Fig3) 所示，这是任何语音识别过程的第一步。
*   一种语言的词汇目录叫做词典。
*   将这些单词组合成有意义的句子的规则系统叫做语法。
*   The language model is a combination of lexicon and grammar. A language model also helps in identifying the right context between words and phrases. This is important as sometimes different words and phrases sound the same. For example, “the stuffy nose” and “the stuff he knows” sound similar but have different meanings.

    ![A440212_1_En_7_Fig3_HTML.jpg](A440212_1_En_7_Fig3_HTML.jpg)

    图 7-3。

    The entire flow of speech recognition

如您所见，了解该环境的领域和上下文非常重要。为了使任何语音识别系统表现良好，需要对其进行良好的训练。微软语音转文本引擎经过大量语音训练，训练有素。是班上最好的。它在一般情况下工作得非常好。然而，有时您需要在封闭领域或特定环境中使用语音识别系统。例如，需要在有背景噪声、特定行话和不同口音的环境中进行语音识别。这种场景要求对声学和语言模型进行定制，以获得良好的性能。这就是定制语音服务的用武之地。

## 定制语音服务

自定义语音服务(CSS)，以前称为自定义识别智能服务(CRIS)，使您能够通过创建特定于您的域、环境和口音的自定义语言模型和声音模型来自定义语音识别系统。在写这本书的时候，它已经可以公开预览了。除了 URL 之外，大多数概念都不太可能改变。

### 定制声学模型

创建定制声学模型需要四个步骤:

*   创建一组语音数据的音频文件。
*   创建一个包含每个音频文件副本的文本文件。
*   在 CSS 门户网站上传音频文件和抄本，使声学数据集“准备就绪”
*   使用声学数据集创建自定义声学模型。

音频文件应该在相同的环境中由实际使用这个自定义语音识别系统的同一批人录制。这很重要，因为有些人有不同的口音。人们可能在嘈杂的环境中使用它。背景噪声是现代定制语音识别系统最困难的挑战之一。在相同的环境中录制音频有助于底层识别平台熟悉生产环境，并且性能良好。

上传的音频文件需要遵循一些基本规则:

*   所有音频文件必须是 WAV 格式。这是 CSS 目前的一个局限。我们希望微软将来能很快接受其他类型的音频。
*   音频必须以 8KHz 或 16KHz 的采样速率进行编码。大多数 VOIP 通信、电话和无线传输都使用其中一种速率，因此它绝对适合人类语音。
*   仅支持单通道(单声道)。
*   每个音频文件不应超过 1 分钟。在 CRIS 的最初几天，它是 10 秒钟。我们希望这一数额将增加以后，也可以根据大小。
*   每个文件应该以至少 1 毫秒的静默开始和结束。
*   特定数据集的所有音频文件都应该有一个唯一的名称。
*   所有音频文件应压缩在单个存档文件，这应该只有音频文件，没有文件夹/子文件夹。
*   压缩后的文件大小不应超过 2GB。

Tip

Microsoft 自定义语音服务不提供任何录制工具来录制音频。您将需要使用一些现有的 Windows 工具。我个人使用 Audacity ( [`www.audacityteam.org/download/`](http://www.audacityteam.org/download/) )进行录音。它还可以导出。WAV 格式。不要忘记将通道设置为单声道，采样率为 8KHz，因为 CSS 不会提供详细的描述，只会将其称为“无效音频”你可以自由使用任何音频工具，只要你遵守上述音频先决条件。

一旦你按照上面的规则录制了所有的音频文件，下一步就是创建一个简单的文本文件，包含所有音频文件的转录。这个转录文件需要遵循一些基本的书写和规范化规则:

*   应该只有一个转录文件。WAV 文件已创建。
*   转录文件中的每一行都应该有音频文件的名称，并用制表符(/t)分隔转录。例如，Audio01.wav 这是我的第一个音频文件 Audio02.wav 我很高兴使用 CRIS API
*   转录文本应仅使用 ASCII 可打印字符集。
*   避免 Unicode，并在适用的情况下用其 ASCII 替换进行替换。比如不是说 Nishith 的作品(右单引号)，应该是 Nishith 的作品(撇号)。
*   缩写要用文字写出来。比如桑杰医生就应该是桑杰医生。
*   没有字母字符的单词应该发音。比如 363，SW 应该写成三六三，西南。

一旦准备好音频文件和转录文件，就可以将它们导入到自定义语音门户中。为了使用自定义演讲，你需要去 CRIS 网站( [`https://cris.ai`](https://cris.ai) )。(由于 CRIS 的名称已更改为自定义语音服务，门户网站 URL 可能会更改。我们建议您去认知服务主页获取实际的 URL。)当前主页如图 [7-4](#Fig4) 所示。

![A440212_1_En_7_Fig4_HTML.jpg](A440212_1_En_7_Fig4_HTML.jpg)

图 7-4。

The portal page of Custom Speech Service

如果您在 public preview 上访问它，您需要单击“创建帐户”选项，将您重定向到登录页面。如果您有权访问语音门户，您需要通过单击右上角的“登录”按钮登录系统。登录系统后，您将看到如图 [7-5](#Fig5) 所示的页面。

![A440212_1_En_7_Fig5_HTML.jpg](A440212_1_En_7_Fig5_HTML.jpg)

图 7-5。

The Custom Speech Service portal after signing in

单击自定义语音下拉菜单并选择自适应数据。由于这是您第一次访问该页面，您将看到名为 Acoustic Datasets 的空表，如图 [7-6](#Fig6) 所示。

![A440212_1_En_7_Fig6_HTML.jpg](A440212_1_En_7_Fig6_HTML.jpg)

图 7-6。

The Adaptation Datasets page when clicked for the first time

默认情况下，区域设置为 en-us。在撰写本书时，自定义语音服务支持两种语言环境(美国和中国)。如果您选择更改区域设置，请使用右上角的下拉菜单。请注意，en-US 地区支持所有三个数据集(声学、语言和发音)，而 Zh-CN 仅支持声学和语言数据集。这可能会随着时间的推移而改变。若要导入音频和抄本数据，请点按声学数据集旁边的“导入”按钮。然后，它会验证您的订阅密钥，并提示您将其与免费订阅关联，或者与任何现有订阅关联，如图 [7-7](#Fig7) 所示。

![A440212_1_En_7_Fig7_HTML.jpg](A440212_1_En_7_Fig7_HTML.jpg)

图 7-7。

Subscription options to select before importing data

验证您的订阅密钥后，您将被带到导入页面以导入数据，如图 [7-8](#Fig8) 所示。

![A440212_1_En_7_Fig8_HTML.jpg](A440212_1_En_7_Fig8_HTML.jpg)

图 7-8。

The Import Acoustic Data page

输入名称和描述。点击选择文件按钮，选择合适的转录文件和包含所有音频的 zip 文件。单击导入按钮将您的数据上传到自定义语音数据中心。如果您上传了有效文件，您将被重定向回声学数据集表，该表现在有一个条目，如图 [7-9](#Fig9) 所示。

![A440212_1_En_7_Fig9_HTML.jpg](A440212_1_En_7_Fig9_HTML.jpg)

图 7-9。

The Datasets page with one entry after importing data

您可以通过单击编辑链接来更改音频的名称和描述。如果您上传了大量音频，您可能会得到“等待”或“正在处理”的状态。等待意味着您已经上传了视频，但它正在队列中等待处理。处理意味着语音服务引擎正在对音频和转录进行内部验证，包括检查文件长度、采样率、转录数据的标准化等。一旦文件按照正确的规则上传，它就会将状态设置为完成。图 [7-10](#Fig10) 显示其中一个数据集的状态标记为失败，另一个标记为完成。

![A440212_1_En_7_Fig10_HTML.jpg](A440212_1_En_7_Fig10_HTML.jpg)

图 7-10。

Two records in the Acoustic dataset , one marked as Failed and the other as Complete

现在点击标记为完整记录的数据集的细节链接，将显示该记录的发声是成功的。这意味着音频已经成功通过 CSS 验证，如图 [7-11](#Fig11) 所示。

![A440212_1_En_7_Fig11_HTML.jpg](A440212_1_En_7_Fig11_HTML.jpg)

图 7-11。

The successful utterances of the dataset marked as complete

您可能会忘记遵守音频和转录文件的规则。如果转录文件和 zip 文件都不正确，并且不遵守规则，您将得到错误消息“处理您的请求时出错”这将伴随着页面 URL 中提到的确切原因，您将最终看到类似于 [`https://cris.ai/AcousticDatasets/Create?errorMessage=The%20name%20and%20the%20two%20files%20are%20required`](https://cris.ai/AcousticDatasets/Create%3FerrorMessage=The%20name%20and%20the%20two%20files%20are%20required.) 的内容。您的数据集状态将被标记为失败，如图 [7-8](#Fig8) 所示。单击详细信息链接将为您提供关于失败和成功话语数量的附加上下文。如果状态被标记为失败，这意味着没有一个用于记录的话语是成功的。失败可能有多种原因。主要原因是您没有在 8KHZ 下创建文件。另一个潜在的原因可能是脚本文件没有遵循规则，因此，您可以逐行得到一个错误，说明“该行的格式不正确，应该是<wave file><tab><transcription>。”

为了创建自定义声学数据模型，您的声学数据集应该处于完成状态。要创建自定义声音模型，请单击自定义语音，然后选择声音模型作为选项。为了创建任何定制的声学模型，您需要使用一些声学模型作为定制的基础模型。由于这是您第一次访问此链接，您将看到 CSS 附带的两个基本声学模型，如图 [7-12](#Fig12) 所示。

![A440212_1_En_7_Fig12_HTML.jpg](A440212_1_En_7_Fig12_HTML.jpg)

图 7-12。

Two base predefined acoustic models that come with CSS

Microsoft Search and Dictation AM-1.1 是基本的声学模型，在大多数情况下充当定制的起点。这个模型有一个语音识别服务的 REST 端点。这款和 Cortana 用的差不多。Microsoft 会话模型是用于识别会话语音的基本模型。当构建任何需要自定义语音场景的基于 CUI 的应用程序时，可以使用此模型。你不能删除这些模型。您始终可以选择创建额外的定制声学模型，并将其用作其他定制声学模型的基础模型。为了创建一个新模型，点击图 [7-12](#Fig12) 所示的创建新选项，你将被重定向到一个新的屏幕，如图 [7-13](#Fig13) 所示。

![A440212_1_En_7_Fig13_HTML.jpg](A440212_1_En_7_Fig13_HTML.jpg)

图 7-13。

The form to create the new acoustic model

输入名称和描述。由于这是您第一次创建声学模型，您的基本声学模型下拉列表应该有两个选项:Microsoft 对话模型和 Microsoft 搜索和听写模型。一旦您创建了一个额外的定制模型，您也可以在这个下拉列表中看到它。选择测试数据作为声学数据集来评估定制模型。这是唯一一个状态为完成的数据集。您的所有订阅也会显示在订阅下拉菜单中。如果您只是免费订阅的一部分，您还将获得一个定制语音服务的选项-免费订阅。

单击“创建”按钮创建模型。创建声学模型所需的时间与创建数据集所需的时间相同或更长。一旦点击创建，您将被重定向到声学模型主页，如图 [7-14](#Fig14) 所示。现在您将看到三条记录。

![A440212_1_En_7_Fig14_HTML.jpg](A440212_1_En_7_Fig14_HTML.jpg)

图 7-14。

The creation of new acoustic model

如您所见，状态被标记为正在处理。事实上，如果它在队列中，它应该在等待。一旦处理完成，状态被标记为完成，如图 [7-15](#Fig15) 所示。单击编辑以更改名称和描述。只有当您想要查看离线测试的结果时,“查看结果”选项才有效。

![A440212_1_En_7_Fig15_HTML.jpg](A440212_1_En_7_Fig15_HTML.jpg)

图 7-15。

The acoustic model marked as Complete

点击详细信息将显示自定义声学模型创建的详细信息，如图 [7-16](#Fig16) 所示。

![A440212_1_En_7_Fig16_HTML.jpg](A440212_1_En_7_Fig16_HTML.jpg)

图 7-16。

New acoustic model details

CSS 还允许您根据一些测试数据来测试您的“新”适应模型。如果您想要测试您的模型，请选中准确性测试复选框。这给了你直接测试你的模型的优势。您不需要将数据发送到托管您的适应模型的新端点。在私有预览(现在公开)期间，准确性测试复选框被命名为离线测试。我是早期的反馈提供者之一，他建议将离线测试的标题更改为最终用户可能认为离线是本地的。事实上，您总是需要一个互联网连接来使用自定义语音。

选中准确性测试复选框要求您选择语言模型和测试数据进行测试。如果您尚未创建任何语言模型，您将获得 Microsoft Universal LM 模型作为下拉列表中唯一的语言模型。Microsoft Universal LM 是一个基准语言模型，也用于各种其他认知服务中。您还需要选择测试数据来测试您的模型，如图 [7-17](#Fig17) 所示。选择不同于用于创建的数据是很重要的。这也将有助于对您的模型性能进行真正的测试。如果您选择了离线测试选项，则在执行评估时，您可能会看到测试状态。

![A440212_1_En_7_Fig17_HTML.jpg](A440212_1_En_7_Fig17_HTML.jpg)

图 7-17。

The acoustic model creation with accuracy testing on

一旦创建，您将在声学模型细节中获得额外的精度测试结果，如图 [7-18](#Fig18) 所示。

![A440212_1_En_7_Fig18_HTML.jpg](A440212_1_En_7_Fig18_HTML.jpg)

图 7-18。

Additonal details

如上图，单词错误率为 30.77%。单击详细信息链接将显示关于准确性测试的更多详细信息。您还可以查看所有准确性测试，或者通过直接从自定义语音页面左上角的自定义语音菜单中选择准确性测试来创建新的准确性测试。

### 自定义语言模型

如前所述，创建语言模型对于理解单词的正确上下文非常重要。例如，想想像“迟钝的孩子；这可以用多种方式表达，有不同的含义:

```py
Slow, Children.
Slow, Children?
Slow Children!
Slow Children?

```

根据正确的环境和上下文，上述单词可以有不同的含义。定制语言模型遵循比创建定制声学模型更简单的过程。您不再需要创建音频文件；相反，您只处理一个抄本文件。以下是步骤:

*   准备语言数据。
*   将语言数据上传到 CSS 门户，使语言数据集完整。
*   使用语言数据集创建自定义语言模型。

创建语言数据文件需要遵循一组规则:

*   您的语言模型数据应该在一个纯文本文件中。
*   如果您使用 en-US 语言环境，您的语言模型支持 US-ASCII 或 UTF-8。如果您使用的是 zn-CN，您需要拥有 UTF-8 格式的文本文件。
*   文件中的每一行应该只有一句话。我称之为话语而不是句子的原因是，它应该准确地代表实际的口语输入，而不是语法正确。
*   每一行的默认权重为 1。你可以选择给任何线更多的权重。权重是大于 1 的整数。加权数相当于重复该行的次数。
*   为了添加任何权重，只需添加由制表符(\t)分隔的数字，如下所示。以下示例将导致成功接受语言数据文件:这是我的第一个音频文件\ t5I 我很高兴使用 CRIS API 此处大量使用了此文本
*   文件的最大大小不应超过 200MB。

一旦你的语言数据准备好了，就该上传了。与添加声学数据集类似，单击自定义语音，然后选择适应数据。提供适当的名称和描述来标识语言数据。选择正确的语言数据文件，如图 [7-19](#Fig19) 所示，点击【导入】，导入您的数据集。

![A440212_1_En_7_Fig19_HTML.jpg](A440212_1_En_7_Fig19_HTML.jpg)

图 7-19。

Importing language data

您将被重定向回语言数据页面，现在您将看到一条记录，如图 [7-20](#Fig20) 所示。您的语言数据已被分配一个唯一的 ID，并具有等待、处理或完成的状态。图 [7-20](#Fig20) 显示完成状态，这意味着您的语言数据文件已经成功通过数据验证检查，现在可以创建自定义语言模型了。您可以单击详细信息链接来查看成功话语的列表。

![A440212_1_En_7_Fig20_HTML.jpg](A440212_1_En_7_Fig20_HTML.jpg)

图 7-20。

A subset of the Dataset page showing a language dataset

与您的自定义声学模型一样，为了创建自定义语言数据模型，您的语言数据集应该具有完整的状态。点按“自定语音”,然后选取“语言模型”来创建自定语言模型。与声学模型类似，由于这是您第一次访问此链接，您将看到两个记录:Microsoft 对话模型和 Microsoft 搜索和听写模型。这些模型作为创建新的定制模型的基础模型。你不能删除这些模型。您始终可以选择创建额外的定制语言模型，并将其用作其他定制语言模型的基础模型。单击“新建”创建新的语言模型。

与您的定制声学模型一样，您现在将看到如图 [7-21](#Fig21) 所示的屏幕，以创建一个语言模型。输入名称和描述以唯一标识您的语言模型。从两个基本语言模型中选择一个作为基本模型。选择语言数据作为先前创建的语言数据集。单击创建创建语言模型。您还可以选择在不访问自定义端点的情况下对语言模型进行准确性测试。

![A440212_1_En_7_Fig21_HTML.jpg](A440212_1_En_7_Fig21_HTML.jpg)

图 7-21。

The language test model creation page

您将被重定向到语言模型主页，并将获得附加记录。请记住，在创建语言模型时，请确保您有足够的数据，否则您的语言模型状态将被设置为失败，原因是“语言数据集没有包含足够的单词来形成语言模型”如果一切顺利，您将获得完成状态，如图 [7-22](#Fig22) 所示。

![A440212_1_En_7_Fig22_HTML.jpg](A440212_1_En_7_Fig22_HTML.jpg)

图 7-22。

The language model page after creating a custom language model

### 发音数据

创建自定义语言模型时，您还可以选择添加自定义发音数据集，最终用于理解新术语或首字母缩写词。为了创建发音数据集，您需要创建一个简单的文本文件，它应该有两列(显示形式和口语形式)，如下所示:

```py
Display form    Spoken form
Cavity          kay vity
Cephalic        see pha lic
Cholesterol     kho les trol
Generic         Jeneric
Xerosis         Zerosis

```

将上述文件或类似文件保存为. txt 文件，然后转到自定义语音➤适应数据，并选择发音数据集旁边的导入按钮。您将得到如图 [7-23](#Fig23) 所示的表单。输入名称和描述，选择先前创建的发音数据文件，然后单击导入按钮。

![A440212_1_En_7_Fig23_HTML.jpg](A440212_1_En_7_Fig23_HTML.jpg)

图 7-23。

The Import Pronunciation Data page

如果发音数据文件符合上述指南，您将被重定向到声学数据集主页，在这里您将看到发音数据集中的一个条目被标记为完整，如图 [7-24](#Fig24) 所示。

![A440212_1_En_7_Fig24_HTML.jpg](A440212_1_En_7_Fig24_HTML.jpg)

图 7-24。

The pronunciation dataset

一旦将发音数据集标记为完整，就可以通过指定它来创建语言模型。

### 自定义语音转文本端点

Microsoft Cognitive Speech API 的独特功能之一是，它允许您创建自己的自定义语音到文本端点，以满足您的特定要求。一旦您的自定义声学模型和自定义语言模型准备就绪，您就可以使用这些新创建的自定义模型的相互组合或与基础模型的组合来创建自定义语音到文本端点。要创建新的自定义语音转文本端点，请单击自定义语音菜单下的部署。单击“新建”选项创建新的自定义端点。输入名称和描述以唯一标识您的自定义端点。选择基本模型作为 Microsoft 搜索和听写模型或 Microsoft 对话模型。根据所选择的选项，您将获得一个以前创建的定制声学模型和语言模型的列表。您也可以选择使用任何基本型号，如图 [7-25](#Fig25) 所示。单击创建以创建新端点。

![A440212_1_En_7_Fig25_HTML.jpg](A440212_1_En_7_Fig25_HTML.jpg)

图 7-25。

Creating a custom endpoint for deployment

您将被重定向到部署页面。与您的定制模型一样，您现在将在表中拥有一条唯一标识您的部署的记录。创建部署端点需要时间。一旦您的端点准备就绪，您的部署状态应该被标记为完成，如图 [7-26](#Fig26) 所示。

![A440212_1_En_7_Fig26_HTML.jpg](A440212_1_En_7_Fig26_HTML.jpg)

图 7-26。

List of deployment endpoints

单击 Details 链接会将您重定向到一个新页面，在这里您可以看到自定义端点的 URL，如图 [7-27](#Fig27) 所示。你最终会看到三个网址。一个 URL 是 HTTP URL，它应该用于 HTTP 请求。另外两个 URL 用于短阶段模式和长阶段模式的 web 套接字。这些 URL 主要用于创建使用 Microsoft 认知客户端服务库的自定义应用程序，该库在内部使用 web 套接字。

![A440212_1_En_7_Fig27_HTML.jpg](A440212_1_En_7_Fig27_HTML.jpg)

图 7-27。

The deployment links of the endpoint

在撰写本书时，CSS 中有一个限制，即在这些自定义端点上只支持两个并发请求。如果您向下滚动页面，您将看到一个测试您的端点的选项，如图 [7-28](#Fig28) 所示。您可以选择上传遵循音频文件自定义规则的音频文件，如粗体所示。选择适当的音频文件并单击“测试”后，自定义语音引擎将使用您新创建的自定义语音来测试端点，以识别上传的音频。如果自定义语音引擎识别了它，您会在识别文本框中看到它被转换成文本，如图 [7-28](#Fig28) 所示。

![A440212_1_En_7_Fig28_HTML.jpg](A440212_1_En_7_Fig28_HTML.jpg)

图 7-28。

The results of testing an audio file from an endpoint

## 说话人识别

现在您了解了语音识别及其使用方法。说话人识别 API 有助于根据用户和说话人的声音来识别他们。请注意，语音识别和说话人识别之间有明显的区别。语音识别是“什么”词性，而说话人识别是“谁”词性。简单地说，语音识别是处理已经说过的话的过程。说话人识别是一种识别谁在说话的方法。它也被称为基于语音的生物识别技术。

任何企业应用程序都需要验证用户。如果你熟悉传统的 ASP。基于. NET 的模型，你知道像身份验证和授权这样的术语。认证是对用户进行认证的过程。用户可以提出一个或多个声明来宣布他自己是正确的用户。几乎所有应用程序都要求用户传递用户名和密码作为身份验证媒介之一。尽管这种身份验证继续充当基本的强身份验证模型，但是已经添加了各种附加的身份验证模型来将安全性提升到下一个级别。微软认知模型提供的一种认证方式是通过说话者识别。这种安全性可以作为补充基本身份验证的第二种身份验证形式。

说话人识别并不是一个新概念。其实已经有 40 多年了。历史上，各种算法已经发展了一段时间，以在说话人识别中提供准确的结果。这一切都始于频率估计和基于离散矢量量化的系统。在过去的十年中，说话人识别系统适应高斯混合模型(GMM)。微软使用的说话人识别是基于 I 向量的说话人识别系统。

### 说话人验证与说话人识别

说话人识别 API 有助于根据用户和说话人的声音来识别他们。使用说话人识别 API，您不仅可以验证用户，还可以帮助识别用户，从而极大地增强客户体验。我们来区分一下验证和识别。验证和识别现在可以互换使用，但它们是不同的。说话人验证，也称为说话人认证，是通过存储库中的一个模式或记录来验证声明的过程。在说话者验证中，将输入语音和短语与注册语音和短语进行匹配，以证明它们是同一个人还是不同的个人。说话人确认基于 1:1 映射。在真实场景中，出示护照或任何数字身份被称为认证。

另一方面，说话人识别通过存储库中所有可能的记录来验证声明，主要用于从一系列个人中获取未知人的身份。说话人识别基于 1:N 映射。在说话人识别 API 被用于验证或识别之前，它经历了一个称为注册的过程。由于说话人识别是 1:N 映射，需要更多的时间，所以说话人确认比说话人识别更快。

### 注册-验证

每种声音都有独特的特征，可以用来帮助识别个人。在登记期间，说话者的声音被记录，并且通常提取许多特征以形成声纹，例如音调、音高、语速等。这些特征的组合一起形成了唯一识别个人声音的声纹或声音模型。如图 [7-29](#Fig29) 所示，注册过程需要一个迭代的方法，在这个过程中，每个人都要说三遍或更多的特定短语。每当用户说出特定短语时，就会提取特征。提取的特征和短语组合在一起形成了该个体的独特声纹模型。然后，该模型被存储在声纹模型库中，用于进一步的说话人验证或识别。

![A440212_1_En_7_Fig29_HTML.jpg](A440212_1_En_7_Fig29_HTML.jpg)

图 7-29。

The steps done during the enrollment process

注册验证包括三个步骤:

*   创建验证配置文件
*   提示用户说出验证短语之一并存储音频
*   通过传递配置文件 id(在步骤 1 中创建)和音频语音流(在步骤 2 中创建)来调用注册 API

创建验证配置文件是一个简单直接的过程。您需要使用 HTTP POST API，可从以下网址获得

```py
https://westus.api.cognitive.microsoft.com/spid/v1.0/verificationProfiles

  by passing in the right locale and subscription key, like so:

POST https://westus.api.cognitive.microsoft.com/spid/v1.0/verificationProfiles HTTP/1.1
Content-Type: application/json
Host: westus.api.cognitive.microsoft.com
Ocp-Apim-Subscription-Key:••••••••••••••••••••••••••••••••

{
  "locale":"en-us",
}

If you call the API with an invalid subscription key, you will end up with a 401 access denied error with the message “Access denied due to invalid subscription key. Make sure you are subscribed to an API you are trying to call and provide the right key."  While writing this book, the only locale that was supported was en-us. We do believe this list of supported locales will improve over time. If you call the API with the right subscription key, you will return a GUID called verificationProfileID, as shown:

Pragma: no-cache
apim-request-id: c245b8ec-2bfb-48a3-b562-cafb4b01789f
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
x-content-type-options: nosniff
Cache-Control: no-cache
Date: Sun, 14 May 2017 08:23:31 GMT
X-AspNet-Version: 4.0.30319
X-Powered-By: ASP.NET
Content-Length: 71
Content-Type: application/json; charset=utf-8
Expires: -1

{
  "verificationProfileId": "05822be0-5c41-4055-9890-6fa9219baaa6"
}

```

此 id 最终将用于创建注册 id。在编写本书时，个人订阅最多可以创建 1000 个验证配置文件。

一旦创建了`verificationProfileID`，下一步就是提示用户说出其中一个验证短语并存储音频。注册验证是一个依赖于文本的过程。为了说出短语，您首先需要获得所有支持的验证短语的列表。这可以通过调用如下所示的 API，传递正确的区域设置和订阅密钥来实现:

```py
GET https://westus.api.cognitive.microsoft.com/spid/v1.0/verificationPhrases?locale=en-US HTTP/1.1
Host: westus.api.cognitive.microsoft.com
Ocp-Apim-Subscription-Key: ••••••••••••••••••••••••••••••••

Calling with the right subscription key and locale will return all of the phrases. As of writing, the following 10 phrases were returned by the API:

{ "phrase": "i am going to make him an offer he cannot refuse" },
{ "phrase": "houston we have had a problem" },
{ "phrase": "my voice is my passport verify me" },
{ "phrase": "apple juice tastes funny after toothpaste" },
{ "phrase": "you can get in without your password" },
{ "phrase": "you can activate security system now" },
{ "phrase": "my voice is stronger than passwords" },
{ "phrase": "my password is not your business" },
{ "phrase": "my name is unknown to you" },
{ "phrase": "be yourself everyone else is already taken" }

```

你可以从上面的短语中选择一个，至少录三遍，保存在。WAV 文件。我们称之为注册音频文件，以供将来参考。每段录音应在 1-15 秒之间，并应以 16KHz 速率和单声道进行 PCM 编码。一旦您按照上面的规则进行了记录，您就可以通过传递记录的语音和在步骤 1 中创建的`verificationProfileID`来调用验证注册 API，从而注册用户。用户注册可以通过调用 API 来完成，如下所示:

```py
POST https://westus.api.cognitive.microsoft.com/spid/v1.0/verificationProfiles/05822be0-5c41-4055-9890-6fa9219baaa6/enroll HTTP/1.1
Content-Type: multipart/form-data
Host: westus.api.cognitive.microsoft.com

[BinaryData]

```

如上所示，前面创建的`verificationProfileID`作为查询参数传递，而注册音频文件在请求体中传递。

*   用无效的`verificationProfileID`调用它将返回 401 访问被拒绝。
*   使用有效的`verificationProfileID`和错误的音频文件调用它将返回错误 400 Bad request，并显示消息“无效的音频格式:不是 WAVE 文件-没有 RIFF 头。”
*   使用有效的`verificationProfileID`和适当的音频文件调用将返回一个 JSON 响应，其中包含以下字段:
    *   `enrollmentStatus`:指定注册状态。可以是报名、培训、已报名。
    *   `EnrollmentCounts`:指定说话人确认注册计数
    *   `remainingEnrollments`:指定所需注册的数量
    *   `Phrase`:识别注册音频文件中使用的短语

如果通过调用该注册 API 至少三次成功注册了发言人，则`enrollmentStatus`将被设置为已注册。一旦`enrollmentStatus`被设置为已注册，它就可以在验证场景中使用。如果说话者刚刚呼叫了一次，则状态返回将是正在登记，`EnrollmentCounts`将被设置为 1，`remainingEnrollments`将被设置为 2。

### 说话人验证

说话人确认是一个依赖文本的过程。这意味着相同的短语用于注册和验证。验证过程类似于注册过程。您应该使用您为注册记录的相同验证短语。像注册过程一样，您应该使用录制的语音和`verificationProfileID`调用 API，如下所示:

```py
POST https://westus.api.cognitive.microsoft.com/spid/v1.0/verify?verificationProfileId={verificationProfileId} HTTP/1.1
Content-Type: application/octet-stream
Host: westus.api.cognitive.microsoft.com

[BinaryData]

```

它将返回一个包含以下详细信息的 JSON 响应:

*   `Result`:指定是接受验证还是拒绝验证
*   `Confidence`:分享结果的置信度。它可以是低、正常或高。

```py
{
  "result" : "Accept",
  "confidence" : "Normal",
  "phrase": "my voice is my passport verify me"
}

```

结果有两个选项:接受或拒绝。接受意味着验证已被接受。拒绝意味着验证被拒绝。置信度指定验证的置信度；它可以是低、正常或高。依靠结果和信心分数的组合是非常重要的。如果某个结果被标记为“接受”且置信度较低，您可能希望对其进行更多的训练。置信度得分至少应该是正常或高。在上面的例子中，结果以正常的置信度得分被接受，可以继续进行。

### 注册-识别

与注册-验证的第一步类似，用户首先需要创建一个身份配置文件。用于调用注册标识的 URL 不同于注册验证。创建身份配置文件是一个简单明了的过程。您需要通过传递正确的区域设置和订阅密钥，在`https:// westus.api.cognitive.microsoft.com/spid/v1.0/verificationProfiles`获得可用的 HTTP POST API。在撰写本文时，身份识别配置文件支持中文普通话(zn-CN)的附加区域设置。

```py
POST https://westus.api.cognitive.microsoft.com/spid/v1.0/identificationProfiles HTTP/1.1
Content-Type: application/json
Host: westus.api.cognitive.microsoft.com
Ocp-Apim-Subscription-Key:••••••••••••••••••••••••••••••••

{
  "locale":"en-us",
}

```

如果使用正确的订阅密钥和区域设置调用 API，API 将返回一个名为`IdentificationProfileID`的 GUID。该 ID 唯一地标识讲话者，并且将在以后用于创建登记标识 ID，然后从讲话者组中标识讲话者。

说话人识别是一个与文本无关的系统。这意味着用户可以在注册和验证中使用任何文本作为语音样本。这就是为什么与注册-验证不同，注册-识别是一个两步过程。对用户在音频中说什么没有限制，因此不需要获取所有短语的步骤。

如下所示，`IdentificationProfileID`作为查询参数传递，注册音频文件在请求体中传递。每段录音的长度应该在 5 秒到 5 分钟之间，并且应该以 16KHz 的速率进行 PCM 编码。去除静音后，注册的最小推荐存储语音约为 30 秒。您可以选择将`ShortAudio`设置为真/假，这取决于您想要提交的音频的长度。它指示服务放弃注册所需的音频限制的推荐量，并且相应地配置文件的注册状态从注册更改为已注册。这样做时，您可以发送 1 秒钟长但不超过 5 分钟的音频文件。可以通过调用 API 来注册用户的身份

*   用无效的`IdentificationProfileID`调用它将返回 401 访问被拒绝。
*   使用有效的`IdentificationProfileID`和错误的音频文件调用它将返回错误 400 Bad request，并显示消息“无效的音频格式:不是 WAVE 文件-没有 RIFF 头。”
*   如果注册中出现任何错误，您将收到错误 500，并显示“SpeakerInvalid”消息
*   使用有效的`IdentificationProfileID`和适当的音频文件调用将返回一个 JSON 响应，其中包含以下字段:
    *   `Accepted`:服务已成功接受请求。
    *   `Operation URL`:用于以后通过操作 API 检索结果状态。

```py
https:// westus.api.cognitive.microsoft.com/spid/v1.0/identificationProfiles/{identificationProfileId}/enroll[?shortAudio]
Content-Type: multipart/form-data
Host: westus.api.cognitive.microsoft.com

[BinaryData]

```

### 说话人识别

如前所述，说话人识别帮助你从一群说话人中识别出一个人。一旦您注册了所有的说话者，并且对所有的说话者都有了可接受的响应，您就可以通过调用说话者识别 API 来从一组说话者中识别出一个说话者，如下所示:

```py
https:// westus.api.cognitive.microsoft.com/spid/v1.0/identify?identificationProfileIds={identificationProfileIds}[&shortAudio]
Content-Type: multipart/form-data
Host: westus.api.cognitive.microsoft.com

[BinaryData]

```

如上所示，您需要将所有标识概要文件(之前创建的)作为逗号分隔的值进行传递。与 enrollment-identification 类似，您需要在主体中传递音频，并根据音频长度将可选的`shortAudio`字段设置为 true/false。您将获得用`operationalURL`返回的结果，您可以用它来跟踪操作状态。

### 操作状态

现在您了解了如何创建标识配置文件。有时，您会想要检查操作的确切状态。例如，您可能希望稍后检查扬声器识别配置文件的操作状态或注册状态。您可能需要检查注册是否成功或是否仍处于培训模式。微软说话人识别使得处理任何操作任务都非常容易；所有这些都可以通过调用操作 URL 来完成:

```py
GET https://westus.api.cognitive.microsoft.com/spid/v1.0/operations/{operationID} HTTP/1.1
Host: westus.api.cognitive.microsoft.com

```

用正确的`operationID`和订阅密钥调用上面的 API 将返回一个 JSON 响应。这里提到的`operationID`既可以通过调用为识别配置文件创建注册来获得，也可以通过说话者识别-识别来获得，我们将在后面介绍。以下代码显示了注册状态为成功时的 JSON 响应:

```py
{
"status": "succeeded",// [notstarted|running|failed|succeeded]
  "createdDateTime":  "2017-01-13T01:28:23Z",
  "lastActionDateTime": "2017-01-15T01:37:23Z",
  "processingResult":
  {
    "enrollmentStatus" : "Enrolled", // [Enrolled|Enrolling|Training]
    "remainingEnrollmentSpeechTime" : 0.0,
    "speechTime" : 0.0,
    "enrollmentSpeechTime":0.0
  }
}

```

一旦得到 JSON 响应，首先需要检查状态。它可以是下列之一:

*   未开始
*   运转
*   不成功的
*   成功

如果状态为 Succeeded，查看如上图的处理结果 JSON 对象。如果状态不是成功，您可能需要检查其他字段选项。例如，如果状态为“失败”,请检查消息字段以查看失败的原因。您还可以完成其他操作任务；参见表 [7-2](#Tab2) 了解一些信息。

表 7-2。

Other Operational Tasks

<colgroup><col> <col></colgroup> 
| API 名称 | 描述 |
| --- | --- |
| 删除个人资料 | `DELETE` `https://westus.api.cognitive.microsoft.com/spid/v1.0/identificationProfiles/{identificationProfileId}`从服务中永久删除所有说话人识别简档以及所有相关注册。 |
| 获取所有配置文件 | `GET` `https://westus.api.cognitive.microsoft.com/spid/v1.0/identificationProfiles`获取一个订阅内的所有发言者标识配置文件。它将返回 JSON response 订阅中的所有标识配置文件，以及其他详细信息，如注册状态、地区、用于标识的秒数、成功注册的剩余秒数等。 |
| 获取特定配置文件 | `GET` `https:// westus.api.cognitive.microsoft.com/spid/v1.0/identificationProfiles/{identificationProfileId}`传递带有特定标识配置文件 id 的 URL，以给出与该 id 相关联的说话人标识配置文件。您可以调用这个 API 来获取标识概要文件的注册状态。 |
| 删除所有注册 | `POST` `https:// westus.api.cognitive.microsoft.com/spid/v1.0/identificationProfiles/{identificationProfileId}/reset`从服务中永久删除与给定说话人识别简档相关联的所有注册。 |

## 摘要

在本章中，您学习了如何使用 Bing 语音 API 获得语音到文本和文本到语音的功能。本章还详细介绍了定制语音服务(以前称为 CRIS)的需求。在本章的最后，您还探索了说话人识别 API，并学习了如何使用它来进行说话人识别、验证和注册。下一章将带你踏上使用 Bing 搜索和其他各种搜索的奇妙旅程。