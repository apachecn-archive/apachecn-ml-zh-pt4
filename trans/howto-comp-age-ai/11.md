# 11.将所有这些放在一起:走向人机合作生态系统

英国植物学家阿瑟·坦斯利在 20 世纪 30 年代提出了生态系统的概念，其灵感来自于一个局部的生物群落如何相互作用，并利用环境来维持、繁荣，甚至适应环境的变化。

1993 年，詹姆斯·马龙将这一概念扩展到现代商业，在现代商业中，公司共同发展能力，合作和竞争，以支持新产品和满足客户需求，并最终推动下一轮创新。

之前，我们讨论了经济如何从大型“独立”公司转向基于网络的公司，这主要是由数字技术、丰富的数据、大规模增加的连接和云计算实现的。苹果公司明确将其产品和服务设计为一个生态系统，为客户提供无缝体验；易贝认识到，它必须把重点放在有意建立自己的“共享生态系统”上。价值创造(财富创造)具有完全不同的含义；更密集、更丰富的网络，生产者和消费者通过合作来扩大双方的收益。例如，谁会想到帮助我们连接到自己部落的平台(脸书、Twitter 和 WhatsApp)会产生巨大的商业价值，毕竟，这些只是人与人系统的连接。

我们的需求总是驱动手段。我们关心自己的健康，这驱使我们去寻找医疗实践、医生、医院、药房等方式。同样，我们想找到人类肌肉和受过训练的动物之外的能源，所以我们想出了一种从化石燃料、电力、煤炭和核聚变中获取能源的方法。核能、飞机、汽车、个人电脑、收音机、互联网等创新。破坏了成熟的商业生态系统和社会运作方式，每一项创新都可以被称为*黑天鹅*。 <sup>[1](#Fn1)</sup> 这些突破在他们发现的时候还不是突破。它们在实验室里花了数年时间，只有在变得足够便宜以供大众消费后才成为主流。人工智能策略本身可能会误导他们。我们认为人工智能技术是另一只即将到来的黑天鹅，因此，如果要真正利用人工智能的变革力量，我们没有其他选择，只能建立和利用一个基于人机共生的新生态系统。

## 人机共生

2011 年，IBM Watson 赢得了 *Jeopardy* 游戏节目。这是一个分水岭时刻，不是因为机器在人类自己的游戏中击败了人类，而是因为可能性打开了我们的眼睛。随之而来的是人工智能领域的一系列惊人突破——图像识别、语音识别等等——所有这些都可能通过一种被称为*深度学习*的技术实现。

另一方面，机器变得比人更聪明也产生了相当大的不确定性和对我们与机器未来关系的猜测。埃隆·马斯克(Elon Musk)将人工智能描述为“我们最大的生存威胁”。斯蒂芬·霍金警告说，“全人工智能的发展可能意味着人类的终结”。哲学家尼克·博斯特罗姆在他的书《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》、《超级智慧》，这本书》中提到了技术上的“奇点”，在

所有这些辩论和猜测都基于一个默认的假设，即由于机器可以看、听、行动，并且不断学习，它们将在各种任务中超越人类(尽管目前只是在狭窄的领域中)，并且根据同样的论点，它们很快将能够更普遍地“思维超越”我们。

重要的问题是，除了相互竞争，我们是否有办法编排一种机制，让机器智能和人类智能相互补充？最终目标将是建造能像人类一样思考的机器，并设计能帮助人类更好思考的机器。

就像任何新技术进化一样，人工智能也经历了相当多的进化阶段。理解这一切是如何开始的，以及最初的意图是如何被多次重新定义的，这一点很重要。

1955 年夏天，约翰·麦卡锡在达特茅斯大学召集了一次会议。人工智能先驱名人录参加了会议。被普遍称为“达特茅斯会议宣言”的原始提案称:“学习的每一个方面或智力的任何其他特征原则上都可以如此精确地描述，以至于可以制造一台机器来模拟它。” <sup>[5](#Fn5)</sup>

一般智力(也称为 g 因子)是指存在一种广泛的心智能力，使个体能够执行不同的认知任务，例如，一个人在一种类型的认知任务(例如，绘制自然和风景的能力)上的表现往往与同一个人在其他类型的认知任务(例如，绘制建筑图的能力)上的表现相当。这种特殊的一般智能能力我们在今天的人工智能应用中是看不到的。一个设计用来下棋的算法，如果被要求给出产品推荐，会完全不知所措。简而言之，今天的人工智能代理的 g 因子被限制在一个狭窄的智能类型中。

此外，像“神经网络”和“深度学习”这样的术语正在证伪这样的说法，即我们即将创造出“像人类一样思考”的机器。虽然神经网络受人脑的启发，但实际上它被视为统计回归模型的推广。类似地，“深度”不是指心理深度，而是指增加结构(“隐藏层”)，使模型能够捕捉复杂的非线性模式。“学习”是指对回归模型中的大量模型参数进行数值估计。

简而言之，迄今为止，人工智能的惊人成功是建立在统计推断的基础上，而不是建立在我们认为的人类智能的近似或模拟基础上。

## Licklider 的增长

达特茅斯会议五年后，心理学家和计算机科学家 J. C. R. Licklider 阐明了人类和计算机智能之间的共生关系。他提出了一种互补的关系:人类将定义问题陈述，设定目标，并扮演验证学习的关键角色。机器将做筛选大量数据的艰苦工作，以产生洞察力和预测。这种互补的能力和协调的任务执行将比人类单独执行任务有效得多。

这种人机共生已经悄悄进入我们的日常生活。常见的例子包括:

*   使用 Waze 等 GPS 应用程序在繁忙的十字路口和复杂的道路迷宫中导航

*   使用个性化推荐菜单在大量书籍或电影中进行搜索

*   对着机器说话，发出命令，然后机器用事实、数字、天气模式、附近的餐馆等等来回应

在每种情况下，人类都指定了目标和标准(例如“带我去市区，但不要去繁忙的街道”或“给我一些适度搞笑、不可救药的浪漫、亚洲主题的电影选项”或“在步行距离内给我找一家评级高、价格适中的意大利餐馆”)。它是如何工作的？一种人工智能算法筛选海量数据，以得出预测。然后人类评估机器的输出来做出决定。每当人类选择或忽略某个特定的预测时，人工智能算法都会记录下来，并将这些信息反馈到学习过程中，从而不断改进和学习。在任何情况下，人类的智慧都不会被遮蔽，反而会被增强。

事实证明，人类的思维不像最初意识到的那样可预测地不理性，人工智能也不像最初希望的那样像人。

## 琳达实验:快速思考和慢速思考

人工智能先驱希尔伯特·西蒙在人机合作的背景下展示了人工智能的另一个方面。他认为，我们人类必须满足于“增加”而不是优化的解决方案，因为我们长时间工作、记忆大量事物和合理推理的能力是有限的。相比之下，机器不会表现出工作疲劳，总是做出一致的决定，并且可以用最小的努力处理大量的输入。此外，它们可以比人类更准确地评估数百万个因素。

丹尼尔·卡内曼在他的书*思考的快与慢、* <sup>[6](#Fn6)</sup> 中强调了人类决策过程中有趣而非理性的方面。

每当我们面前有众多的选择时，我们就会进入一个决策过程，很自然地，我们会评估大量的数据，运用一定的理性思维，回忆我们过去的经历，与他人商议，然后做出决定。这就是卡尼曼所说的“系统 2”思维，或者“缓慢思维”

事实上，我们并不总是求助于这种漫长而缓慢的评估过程来做出决定。相反，我们通常依靠我们自己的部落知识，包括各种各样的心理经验法则(启发法)来做出决定。我们自己的决策过程在叙述上看似合理，但在逻辑上却常常是可疑的。卡尼曼称之为“系统 1”思维或“快速思维”，琳达的实验对此进行了著名的阐释。

在一项针对顶尖大学学生的实验中，卡尼曼和特沃斯基描述了一个名叫琳达的虚构人物。她很聪明，大学主修哲学，参加过女权运动和反核示威。根据这些关于琳达大学时代的细节，哪一个更有可能是今天琳达的情况？

“琳达是银行出纳员。或者琳达是一名活跃在女权运动中的银行出纳。”

卡尼曼和特沃斯基报告说，87%的学生认为第二种情况更有可能，尽管片刻思考表明这不可能是事实。女权银行出纳员是所有银行出纳员的一个子集。但是，添加琳达仍然活跃在女权运动中的额外暗示影响了叙事的连贯性，并导致学生们选择(不太可能)第二种情景。

从本质上说，人类的思维混淆了容易想象的和高度可能的，让情绪影响判断，在随机噪音中假设模式，将因果关系与虚假的相关性联系起来，以及从个人经验中过度概括。我们用来做出判断的许多启发法和智慧被证明是系统性的偏见。如果我们说，我们的大脑需要算法来消除我们的判断偏差，我们的眼睛当然需要人工晶状体来过滤掉虚假的相关性，这或许并不具有高度挑衅性。

另一方面，如果有一种方法可以对常规任务背后的处理逻辑进行编码，那么可以肯定的是，在相同的任务上，算法将优于人类。然而，这种算法将缺乏评估新的或新颖情况所需的概念理解和常识推理。为了说明这种谨慎，让我们以 IBM Watson 赢得 *Jeopardy* 为例。在竞赛中，在“美国城市”类别下提出了一个问题:“它最大的机场是以一位二战英雄命名的；它是第二大的，用于第二次世界大战沃森回答“多伦多。”类别“美国城市”，但回答“多伦多”！(对了，正确答案是芝加哥。)

这个特殊的例子表明，人类智能的某些优势，如常识推理，可以抵消蛮力机器学习的基本限制。

人工智能令人着迷的一点是，很难预测哪些部分容易，哪些部分难。起初，像下棋这种对人类来说具有精神挑战性的活动对机器来说是最难的，但事实证明这很容易。另一方面，像识别物体或捡起它们这样对人类来说相当简单的任务，对机器来说就要困难得多。

总之，人类需要机器来避免“系统 1”决策陷阱，机器需要人类来避免“系统 2”决策陷阱。总之，它们意味着人机共生的理由比以往任何时候都更加充分。

我们需要的是人机一体的策略。

## 人机一体化战略(HMIS)

虽然人工智能的增强使人类的生活一天比一天轻松，但人们一直担心基于人工智能的系统会对人类构成威胁。人工智能社区的人们对于人工智能模仿人类行为的利弊有着不同的看法。

例如，一个被训练来检测 MNIST 数据集中数字的神经网络在被输入图像的阴性测试样本(即，将黑色转换为白色，反之亦然)时悲惨地失败了，这是人类不会有问题的事情。算法只有在用于训练它们的数据的完整性范围内才是可靠的。和往常一样，垃圾入意味着垃圾出。

此外，人工智能系统有一种从收集的数据中继承的偏见(这是人类有意识地试图避免的)。例如，一个餐馆评论系统给墨西哥餐馆的评分很低，因为墨西哥这个词与其他非法活动联系在一起。

与其担心人工智能的进步，不如我们可以提出一种人机集成的策略，包括人类和机器，共同生活在一个复杂的适应性生态系统中，怎么样？

我们姑且称这样的生态系统为人机一体化战略。它主要包括人类认知和智能机器，其中人类和机器在解决问题的活动中互相帮助。未来的人机整合战略不是人工智能机器取代人类，而是机器和人类共生共存。利用人工智能最有效的方式是用它来增强人类的能力。机器在特定任务上做得更好，而人类在一般任务上做得更好。因此，人类和机器在相互推动或委托任务的同时进行互动的社交设置，如果他们不擅长，是一种合适的前进方式。

人机集成架构中的人类角色是一个关键的区分因素，不仅可以提高信任度、互惠性和可爱度，还可以减轻与人工智能系统激增相关的恐惧和担忧。

智能机器模糊了计算方面与人类输入和输出交互之间的界限。他们借用并扩展了一些已建立的计算原则，如群体计算智慧、集体智能计算、社会网络等。虽然群体计算的智慧侧重于将几个专家带到一个中央平台，但智能机器通过提供计算智能来增强这些专家的决策。这种混合系统允许人类和计算机和谐地工作，以实现共同的目标。

有几项现实生活中的任务是机器无法完全完成的。在这类任务中建设性地使用人类认知有助于让机器更容易解决问题。研究人员长期以来一直试图让人类和机器之间的交互看起来无缝而自然。

人类的基本要素是食物、水、能量和安全。虽然这些基本目标对人类来说似乎微不足道，但人们可能会想它们对机器来说意味着什么。在机器代理的情况下，这些需求可以是电力、网络、存储、维护等。就像人类世界一样，没有什么是免费的。机器代理使用的每一项资源都必须有一个付费的概念。可以把它想象成一种虚拟货币，它为机器代理人采取的每一个行动带来了奖励和惩罚的概念。

### 应用场景

基于 HMIS 的应用程序主要有以下不同的场景，这取决于人工代理和机器代理的组成，以及谁负责所采取的操作。

#### 作为人类助手的机器代理

人类在实时机器代理的帮助下与不同的多文化代理(有时讲不同的语言)合作。机器代理向人类提供必要的信息和建议，但最终的决定是由人类做出的。

例如，考虑一个机器代理帮助一个法官处理类似案件的事实和对以前裁决的分析，或者一个机器代理帮助一个招聘人员根据候选人的文化适合度做出雇用决定。

#### 完全自主的机器代理

自主机器代理与其他人类和机器代理合作。在这个场景中，没有人为机器代理的行为负责。因此，这种情况仅限于机器代理的操作风险非常低的应用。

例如，将自主代理视为在危险的物理环境中执行任务的虚拟人物，或者在人类低接触环境中输入-活动-输出被明确定义的虚拟人物，或者模拟自主机器代理在质量控制过程中观察、记录和发出警报。

#### 一个与人类互动的机器代理

在这种情况下，机器代理被训练成能够理解并根据其人类对手的偏好和目标行事。一个机器智能体有能力为它的人类谈判和做决定。它的工作方式就像一个自主的机器代理，但是当它有疑问的时候，它会向它的人类伙伴求助。因此，由机器代理采取的行动的责任在于它的人类对应物。

例如，考虑呼叫中心的机器代理回答人们的询问，对他们的文化背景敏感，或者机器代理在世界各地执行随机 A/B 测试，考虑人们的文化背景。

#### 所有机器代理交互

就代理如何被训练以及谁承担机器代理所采取的动作的责任而言，这类似于前面的场景。不同之处在于没有人类参与互动。就代理之间如何进行交互而言，它提出了不同的挑战。

例如，考虑决策者的机器代理在招聘过程中就候选人排名达成共识，或者团队选择委员会成员的机器代理合作选择团队。

#### 一个与机器代理交互的人

这个场景指向一个典型的设置，其中一个人走进他们家或办公室的一个特殊的房间，在那里她可以沉浸在她想要与之合作的另一组人(位于远处)的虚拟环境中。远程组的全部或部分可以由他们的机器代理来代表。

例如，考虑一个在虚拟环境中与朋友的机器代理人交谈以获得乐趣的人，或者一个进行拍卖的人，在拍卖中人们派他们的机器代理人去投标。

## 治理框架

根据艾萨克·阿西莫夫著名的机器人三定律，HMIS 需要一个治理框架来管理和监控人类和机器代理如何以协作的方式执行各自的任务。这些定律可以和机器人三定律一起使用:

*   机器代理永远不会收集肤色、身高、体重等身体特征。作为视觉输入，用于学习或识别他们所迎合的个人的文化背景。输入将始终需要通过正式的输入渠道提供。这是为了使系统不受任何与身体特征相关的刻板印象的影响。

*   对文化不敏感的评论、句子、词汇或俚语对所有文化都是不敏感的，除非它的另一个积极方面对特定文化有明确的表述。

*   人类代理将需要一个主动学习系统，能够在交互后整合反馈，以持续验证其行为。

需要为人类和机器代理确定明确的“责任分离”和事件响应矩阵。然后，人类和机器代理可以使用强制的或自动化的活动来管理相互依赖和交互。这些相互依赖和相互作用要么是事先设计好的，要么可以在运行时根据它们所暴露的场景来执行。

效率和特异性至关重要，因此应通过适当的交互管理清楚地记录协调规则。HMIS 应该能够处理多智能体复杂适应性生态系统中的并发行为，提供安全性、可靠性和容错能力。协调过程应该准确地反映出其形成的语义。协调和责任的效果应该精确地保持在代理交互空间上。安全是人机交互中的另一个因素，治理框架不应该忽视它。由于人类在社会中遵守法律，所以对于机器代理人来说，遵守一些防止他们采取可能导致破坏性行动的激烈措施的法律也同样重要。

就像在社会中，我们有律师和警察来执行法律，在 HMIS 的生态系统中，我们也需要看门狗来维护法律。

例如，在一家公司通过面试过程招聘员工的背景下，最终决定由机器代理人做出，HMIS 看门狗将跟踪机会均等的基本价值观是否在整个过程中得到维护。例如，如果它在不知情的情况下招募了引起性别偏见或种族、肤色或性取向偏见的候选人，HMIS 看门狗将不会批准这样的招募，并将建议机器代理重新评估或改变他们的决定。其他例子可以包括 HMIS 看门狗监控通信文本，以检测不可接受的或文化不敏感的词语或概念。此外，HMIS 看门狗还可以提供分布式自主平台来验证来自机器的通信的真实性。在 HMIS，法律的治理很重要，但同样重要的是道德，道德应该灌输给代理人。

在商业环境中，违反行为准则应属于道德范畴。如上所述，任何导致种族主义滥用行为甚至错误使用数据的行为都属于违反道德的范畴，在这种情况下，惩罚应该是严厉的。比道德规范低一步的是治理，在治理中，代理在其领域或任务中受上级代理的治理。代理人之间的治理结构应该是分等级的，在这种结构中，治理官员之间有问责制和自主权，可以根据达成的共识采取行动。HMIS 看门狗有望在整个治理过程中变得更加聪明。这可以通过利用对立的机器学习过程来实现。由于代理需要发展自己的角色和职责，因此在代理的学习过程中进行足够的垃圾邮件过滤非常重要。只有当 HMIS 看门狗变得聪明时，才能通过改进智能体通信模式中的过滤机制来做到这一点，否则智能体可能会破坏智能体的健康学习过程，进而破坏整个生态系统。

我们还必须讨论在实现一个稳定的 HMIS 生态系统的道路上可能遇到的挑战，这将在下面讨论。

### 机器的可信度和可爱度

完成两个人之间的互动的首要要求是接受这样的互动。这种接受度是个体之间相互信任程度的函数，而这种接受度的范围/程度与个体之间的喜爱程度直接相关。

如果环境中的一个个体是一台机器，这就变成了一件棘手的事情。虽然从人类的角度来看，信任和可爱并不构成障碍，因为它总是可以通过编程来改变，但反之亦然的关系是一场艰苦的战斗。随着每一篇关于机器能力增强或强调自主机器所犯错误的文章或新闻的出现，对机器的信任受到了打击。HMIS 生态系统要求人类和机器之间进行无缝、开放的信息交流，这需要消除人类对机器的任何怀疑。智力优势并不一定会让机器变得可爱，因此这是一个比理论上看起来复杂得多的问题。例如，与总是正确的机器相比，犯错误的机器似乎让人类更舒服。

### 人机关系

人机关系是一种共生关系，两个实体相互依赖。虽然机器在执行明确定义的任务方面优于人类，但人类在不受完全控制和受不确定因素影响的动态任务方面更胜一筹。由于现实生活中的情况可能包含这两种类型的任务，因此需要在人类和机器之间划分自由度。

HMIS 生态系统由机器网络和人类网络混合而成。随着机器变得越来越自主，保持对它们之间关系的本质和依赖性范例的变化的检查是很重要的。

定义关系是至关重要的；如果自主代理犯了错误或违反了法律，这种关系对于确定承担责任的实体至关重要。像自主性、任务所有权和工作分配这样的挑战必须得到解决。

### 所有的人都是不同的

作为人类，一个人与不同的人有不同的互动。人们与家人互动的方式不同于他们与同事互动的方式，因为有一个高维度的背景来设定互动的基调和水平。上下文并不局限于交互各方之间共享的关系，还延伸到所涉及各方的个人特征，如年龄、性别等。自然，在让机器变得越来越像人类的道路上，识别最大上下文是至关重要的，因此成为我们的下一个挑战。

虽然在识别人类的身体特征(例如性别和年龄)方面已经做了大量的研究，但是在识别动态特征(例如情绪、情感等)方面也做了相当多的工作。从语气、表情、肢体语言。使用这种确定的上下文的一个有趣的方面和增加的复杂性是它是特定于文化的。即使在机器和人类之间的互动方面，文化也在不同文化背景的个人对机器的不同看法中发挥着重要作用。虽然机器需要理解某些手势在不同文化中的含义，但它们也需要理解它们的反应在相同的上下文中会被如何接收。

人和技术必须扮演他们的角色，如图 [11-1](#Fig1) 所示，人类必须不断进化机器的设计，提供治理框架，监控能力，并接受集成人机策略不会消除他们而是会增加他们的努力的概念。

![../images/467925_1_En_11_Chapter/467925_1_En_11_Fig1_HTML.png](../images/467925_1_En_11_Chapter/467925_1_En_11_Fig1_HTML.png)

图 11-1

人机一体化战略地图

## 结论

数据的增长速度远远超过了我们从中获取商业价值的能力。大多数原始数据到洞察解决方案都是为批量设计的。很少是为复杂性而设计的。更少的公司在数量和复杂程度上都表现出色。因此，挑战在于如何从大量复杂的数据中提取商业价值。

与此同时，也有一些公司在大胆尝试，比如“思维即服务”。Nectome <sup>[7](#Fn7)</sup> 正在研究开发一种复杂的大脑银行技术，将临床保存的大脑数字化，并使用这些信息重建人的思维。利用大脑中神经元之间的突触并对大脑进行逆向工程，有可能保持其所有记忆的完整性。结果呢？从童年到一个人生活中的所有高潮和低谷，与这些事件相关的经历，教训和智慧，都可以在以后作为一种服务提供。然而，我们离这种可能性有多近，只有时间能告诉我们。

对许多人来说，“算法”这个词描绘了一套复杂的难以理解的数学公式。然而，现实是，算法就在我们身边，大多数情况下以一种不可见的模式工作，但却带来了重大的商业成果，并提高了我们的生活质量。毫无疑问，在商业环境中使用算法有巨大的优势；然而，也有一些挑战需要解决。

*情境化*:算法呈现非常客观的观点，导致最终的预测。例如，他们可以准确预测客户对某项优惠的反应，但他们不能准确指出客户为什么会有这样的行为！因此，将原因与相关性联系起来，语境化变得极其重要，这是算法改进并更接近现实的唯一方式。将决策完全建立在算法预测的基础上会将你的策略引入歧途。

*判断技巧*:算法擅长分析百万数据点，精准推荐；然而，它们缺乏人类所具备的判断能力。例如，对于您的采购分析，该算法可以考虑各种因素，如供应商绩效、经济状况、当地原材料折扣率、与供应商的关系强度等。并得出最佳选择。然而，它不能像一个有经验的人那样与供应商谈判以获得最好的价格。

在我们与企业主、技术从业者和首席执行官的无数次交谈中，一个问题经常被重复:“你认为算法会影响我们的组织和角色吗？”为了回答这个问题，我们借用了 Gartner 报告中的一些数据，该报告估计，到 2018 年，20%的商业内容将来自机器；而到 2020 年，自主软件代理将参与所有经济交易的 5%。

总而言之，鉴于丰富的数据源(得益于数字化成为一种生活和商业方式)和快节奏的技术演进(物联网、云、自动化、区块链和 ML/DL)，算法业务将成为常态，而不是异常。虽然预测的成本会越来越低，但对判断技能的需求将会上升，而且不能完全排除人为因素。组织需要重新思考他们的战略，关注产品、服务、客户、市场和人力资源。他们还需要在他们的企业游戏计划中增加一个维度——人机集成的人工智能策略。

## 参考

1.  [T2`http://www.digitalistmag.com/future-of-work/2017/07/19/automation-will-lead-to-collaboration-between-man-machine-05217208`](http://www.digitalistmag.com/future-of-work/2017/07/19/automation-will-lead-to-collaboration-between-man-machine-05217208)

2.  [T2`http://www.worktechacademy.com/changes-everything-working-human-robotic-ecosystem/`](http://www.worktechacademy.com/changes-everything-working-human-robotic-ecosystem/)

3.  [T2`https://www2.deloitte.com/insights/us/en/deloitte-review/issue-20/augmented-intelligence-human-computer-collaboration.html`](https://www2.deloitte.com/insights/us/en/deloitte-review/issue-20/augmented-intelligence-human-computer-collaboration.html)

4.  [T2`https://blog.dominodatalab.com/ai-enterprise/`](https://blog.dominodatalab.com/ai-enterprise/)

5.  [T2`https://dataorigami.net/blogs/napkin-folding/17543555-datas-use-in-the-21st-century`](https://dataorigami.net/blogs/napkin-folding/17543555-datas-use-in-the-21st-century)

6.  [T2`https://towardsdatascience.com/the-pac-framework-how-non-technical-executives-should-think-about-artificial-intelligence-b2d733036a52`](https://towardsdatascience.com/the-pac-framework-how-non-technical-executives-should-think-about-artificial-intelligence-b2d733036a52)

7.  [T2`https://www.linkedin.com/pulse/cognitive-collaboration-why-humans-computers-think-better-lewis/`](https://www.linkedin.com/pulse/cognitive-collaboration-why-humans-computers-think-better-lewis/)

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

纳西姆·尼古拉斯·塔勒布，《黑天鹅》(兰登书屋，2007 年)

  [2](#Fn2_source)

埃隆·马斯克，2014 年 10 月在麻省理工学院航空航天系百年研讨会上的演讲。

  [3](#Fn3_source)

[T2`http://www.bbc.com/news/technology-30290540`](http://www.bbc.com/news/technology-30290540)

  [4](#Fn4_source)

牛津大学出版社，2014。

  [5](#Fn5_source)

[T2`http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html`](http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html)

  [6](#Fn6_source)

法勒、斯特劳斯和吉鲁，2011 年。

  [7](#Fn7_source)

[T2`https://nectome.com/`](https://nectome.com/)

  [8](#Fn8_source)

[T2`https://www.gartner.com/smarterwithgartner/gartner-predicts-our-digital-future/`](https://www.gartner.com/smarterwithgartner/gartner-predicts-our-digital-future/)

 </aside>