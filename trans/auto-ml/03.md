

# 二、自动化机器学习、算法和技术

"机器智能是人类需要创造的最后一项发明."

尼克·博斯特罗姆

"人工智能的关键一直是表现."

杰夫·霍金斯

“到目前为止，人工智能最大的危险是，人们过早地断定他们理解它。”

-埃利泽·尤德考斯基

自动化听起来像是那些美妙的禅宗元思想之一，但是学会学习并非没有挑战。在上一章中，我们介绍了**机器学习** ( **ML** )开发生命周期，并定义了自动化 ML，简要概述了它的工作原理。

在这一章中，我们将探索使自动化 ML 成为可能的底层技术、技巧和工具。在这里，你将看到 **AutoML** 实际上是如何工作的，自动特征工程、自动模型和超参数转向以及自动深度学习的算法和技术。您将了解元学习以及最先进的技术，包括贝叶斯优化、强化学习、进化算法和基于梯度的方法。

在本章中，我们将讨论以下主题:

*   自动化 ML–打开发动机罩
*   自动化特征工程
*   超参数优化
*   神经结构搜索

# 自动化 ML–打开发动机罩

简单来说，典型的 ML 管道包括数据清理、特征选择、预处理、模型开发、部署和消费步骤，如以下工作流程所示:

![Figure 2.1 – The ML life cycle
](img/Figure_2.1_B16890.jpg)

图 2.1–ML 生命周期

自动化的 ML 的目标是简化和民主化这个管道的步骤，以便公民数据科学家可以访问它。最初，自动化 ML 社区的关键焦点是模型选择和超参数调整，也就是说，为工作找到性能最佳的模型和最适合问题的相应参数。然而，近年来，它已被转移到包括整个管道，如下图所示:

![Figure 2.2 – A simplified AutoML pipeline by Waring et al.
](img/Figure_2.2_B16890.jpg)

图 2.2-Waring 等人的简化 AutoML 管道。

元学习的概念，即学会学习，是自动化 ML 领域的一个重要主题。元学习技术用于通过观察学习算法、类似任务和来自先前模型的那些来学习最优超参数和架构。使用诸如学习任务相似性、主动测试、代理模型转移、贝叶斯优化和堆叠等技术来学习这些元特征，以改进基于相似任务的自动化 ML 流水线；本质上，是一个温暖的开始。自动化 ML 管道功能并不真正在部署时结束——需要一个迭代反馈回路来监控漂移和一致性预测。这个反馈循环确保预测的结果分布与业务指标相匹配，并且在硬件资源消耗方面存在异常。从操作的角度来看，错误和警告日志(包括自定义错误日志)是以自动化的方式进行审核和监控的。所有这些最佳实践也适用于训练周期，其中概念漂移、模型漂移或数据漂移会对您的预测造成严重破坏；注意“买者自负”的警告。

现在，让我们探索一些你将在这一章和以后的章节中看到的关键自动化 ML 术语。

## 自动 ML 术语的分类

对于自动化 ML 的新手来说，最大的挑战之一是熟悉行业术语——大量新的或重叠的术语会淹没和阻碍那些探索自动化 ML 前景的人。因此，在本书中，我们尽量保持简单，尽可能概括，而不失去任何深度。你会在这本书以及其他自动 ML 文献中反复看到，重点放在三个关键领域——即，自动特征工程、自动超参数调整和自动神经架构搜索方法。

自动化特征工程进一步分为特征提取、选择和生成或构建。自动超参数调整，或者学习特定模型的超参数，有时会与学习模型本身捆绑在一起，因此成为更大的神经架构搜索区域的一部分。这种方法被称为 **全模型选择** ( **FMS** )或**组合算法选择和超参数** ( **CASH** )优化问题。神经架构搜索也被称为**自动化深度** **学习**(缩写为 **AutoDL** )，或者简称为架构搜索。下图概述了**数据准备**、**特征工程**、**模型生成**和**评估**及其子类别如何成为更大的 ML 管道的一部分:

![Figure 2.3 – Automated ML pipeline via state-of-the-art AutoML survey, He, et al., 2019
](img/Figure_2.3_B16890.jpg)

图 2.3–通过最先进的自动化物流调查实现自动化物流管道，何等，2019 年

用于执行自动化 ML 的这三个关键原则的技术有一些共同点。贝叶斯优化、强化学习、进化算法、无梯度和基于梯度的方法几乎用于所有这些不同的领域，其变化如下图所示:

![Figure 2.4 – Automated ML techniques
](img/Figure_2.4_B16890.jpg)

图 2.4–自动化 ML 技术

因此，如果你提到在自动特征工程中使用遗传编程，你可能会感到困惑，而有人认为进化分层 ML 系统是一种超参数优化算法。这是因为你可以将同一类技术，如强化学习、进化算法、梯度下降或随机搜索，应用于自动化 ML 管道的不同部分，而且效果很好。

我们希望在*图 2.2* 和*图 2.4* 之间提供的信息能够帮助您理解 ML 管道、自动化 ML 显著特征和用于实现这三个关键特征的技术/算法之间的关系。你将在本章中建立的心智模型将会有很长的路要走，尤其是当你遇到市场营销杜撰的荒谬术语时(是的，托德，我说的就是你！)，比如基于深度学习的超参数优化产品与比特币和超账本。

下一站是自动化特征工程，自动化 ML 管道的第一个支柱。

# 自动化特征工程

特征工程是从数据集中提取和选择正确属性的艺术和科学。它是一门艺术，因为它不仅需要专业知识，还需要领域知识以及对道德和社会问题的理解。从科学的角度来看，特性的重要性与其对结果的影响高度相关。预测建模中的特征重要性衡量一个特征对目标的影响程度，因此在回顾时更容易对影响最大的属性进行排序。下图解释了自动特征生成的迭代过程是如何工作的，即生成候选特征，对它们进行排序，然后选择特定的特征作为最终特征集的一部分:

![Figure 2.5 – Iterative feature generation process by Zoller et al. Benchmark and survey of automated ML frameworks, 2020
](img/Figure_2.5_B16890.jpg)

图 2.5-佐勒等人的迭代特征生成过程。自动化 ML 框架的基准和调查，2020 年

从数据集中提取要素需要基于具有多个可能值的列生成分类二元要素、缩放要素、消除高度相关的要素、添加要素交互、替换循环要素以及处理数据/时间场景。例如，日期字段会产生几个特征，如年、月、日、季、周末/工作日、假日和注册期。提取后，从数据集中选择一个特征需要移除稀疏和低方差的特征，并应用维度缩减技术，如**主成分分析** ( **PCA** ，以使特征的数量易于管理。我们现在将研究超参数优化，它曾经是自动化 ML 的同义词，现在仍然是该领域的基本实体。

# 超参数优化

由于超参数优化的普遍性和框架的简易性，它有时被认为是自动化 ML 的同义词。根据搜索空间，如果包括特征，超参数优化，也被称为超参数调整和超参数学习，被称为自动化管道学习。对于像为模型找到正确的参数这样简单的事情来说，所有这些术语可能有点令人生畏，但毕业的学生必须发表论文，我跑题了。

当我们进一步研究这些构造时，有几个关于超参数的关键点值得注意。众所周知，默认参数并未优化。Olson 等人在他们的 NIH 论文中展示了默认参数几乎总是一个坏主意。Olson 提到"*根据算法的不同，调优通常会提高算法的准确性 3–5%。在某些情况下，参数调整导致 CV 精度提高了 50%* 。这是在奥尔森等人的*交叉验证准确性改进——将 ML 应用于生物信息学问题的数据驱动建议*中观察到的:[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/)。

第二个重要的点是，对这些模型的比较分析会带来更高的准确性；正如您将在接下来的章节中看到的，整个流程(模型、自动化特征、超参数)都是获得最佳精度平衡的关键。奥尔森等人([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/))在*将 ML 应用于生物信息学问题*的数据驱动建议中的*算法比较分析热图*部分显示了奥尔森等人进行的实验，其中 165 个数据集被用于多个不同的算法以确定最佳精度，根据性能从上到下排序。从这个实验中得出的结论是，没有一个算法可以被认为是在所有数据集上表现最好的。因此，在解决这些数据科学问题时，有必要考虑不同的 ML 算法。

让我们快速回顾一下什么是超参数。每个模型都有其内部和外部参数。内部参数或模型参数是模型固有的，例如权重或预测矩阵，而外部参数也称为超参数，在模型“外部”；例如学习速率和迭代次数。例如，在 k-means 中，k 代表所需的聚类数，而历元用于指定在训练数据上完成的遍数。这两个都是超参数的例子，即模型本身没有的参数。类似地，用于训练神经网络的学习速率、**支持向量机** ( **支持向量机**)、 *k* 树的叶子数量或深度、矩阵分解中的潜在因子、深度神经网络中的隐藏层数等等都是超参数的例子。

要找到正确的超参数，有多种方法，但首先让我们看看有哪些不同类型的超参数。超参数可以是连续的，例如:

*   模型的学习率
*   隐藏层的数量
*   迭代的次数
*   批量

超参数也可以是分类的，例如，操作符的类型、激活函数或算法的选择。它们也可以是有条件的，例如，如果使用卷积层，则选择卷积核大小，或者如果在 SVM 中选择了**R**径向基函数 ( **RBF** )核，则选择核宽度。由于有多种类型的超参数，因此也有多种超参数优化技术。网格、随机搜索、贝叶斯优化、进化技术、多臂 bandit 方法和基于梯度下降的技术都用于超参数优化:

![Figure 2.6 – Grid and random search layout. Bergstra and Bengio – JMLR 2012
](img/Figure_2.6_B16890.jpg)

图 2.6-网格和随机搜索布局。伯格斯特拉和本吉奥——JMLR 2012

最简单的超参数调整技术是手动、网格和随机搜索。手动转向，顾名思义，是基于直觉和基于以往经验的猜测。网格搜索和随机搜索略有不同，因为您要么为每个组合(网格)选择一组超参数，要么随机迭代以保留性能最佳的超参数。然而，正如您可以想象的，随着搜索空间变大，这可能会在计算上很快失控。

另一个突出的技术是贝叶斯优化，从随机组合超参数开始，用它来构建一个代理模型。然后，您使用这个代理模型来预测超参数的其他组合将如何工作。作为一般原则，贝叶斯优化建立了一个概率模型来最小化目标函数，使用过去的性能来选择未来的值，这正是贝叶斯关于它的内容。众所周知，在贝叶斯宇宙中，你的观察没有你先前的信念重要。

贝叶斯优化的贪婪本质由探索和开发权衡(预期的改进)、分配固定时间评估、设置阈值等控制。存在这些代理模型的变体，例如随机森林代理和梯度增强代理，它们使用上述技术来最小化代理的功能:

![Figure 2.7 – A taxonomy of hyperparameter optimization techniques, Elshawi et al., 2019
](img/Figure_2.7_B16890.jpg)

图 2.7–超参数优化技术的分类，Elshawi 等人，2019 年

基于群体的方法(也称为元启发式技术或样本优化方法)也广泛用于执行超参数调整，其中遗传编程(进化算法)最受欢迎，其中超参数被添加、变异、选择、交叉和调整。当配置空间在每次迭代中更新时，粒子群向最佳个体配置移动。另一方面，进化算法通过维护一个配置空间来工作，并通过进行较小的改变和组合个体解决方案来改进它，以建立新一代的超参数配置。现在让我们探索自动化 ML 难题的最后一块——神经结构搜索。

# 神经架构搜索

选择模型可能具有挑战性。在回归的情况下，也就是说，预测一个数值，您可以选择线性回归、决策树、随机森林、套索对岭回归、k 均值弹性网、梯度推进方法，包括 **XGBoost** 和支持向量机等等。

对于分类，换句话说，通过类来分离事物，你有**逻辑回归**、**随机森林**、 **AdaBoost** 、**梯度推进**和**基于 SVM 的分类器**供你使用。

神经架构有搜索空间的概念，它定义了原则上可以使用哪些架构。然后，必须定义一个搜索策略，概述如何使用勘探-开采权衡进行勘探。最后，必须有一个业绩评估策略，评估候选人的表现。这包括架构的培训和验证。

有几种技术用于执行搜索空间的探索。最常见的包括链式结构神经网络、多分支网络、基于单元的搜索以及使用现有架构的优化方法。搜索策略包括随机搜索、进化方法、贝叶斯优化、强化学习以及无梯度与基于梯度的优化方法，如**可区分架构搜索** ( **飞镖**)。使用蒙特卡罗树搜索或爬山来分级探索架构搜索空间的搜索策略是流行的，因为它通过快速接近性能更好的架构来帮助发现高质量的架构。这些是梯度“自由”方法。在基于梯度的方法中，连续搜索空间的基本假设有利于 DARTS，与传统的强化学习或进化搜索方法不同，DARTS 使用梯度下降来探索搜索空间。神经架构搜索的可视化分类可以在下图中看到:

![Figure 2.8 – A taxonomy of neural architecture search techniques, Elshawi et al., 2019
](img/Figure_2.8_B16890.jpg)

图 2.8–神经架构搜索技术的分类，Elshawi 等人，2019 年

为了评估哪种方法最适合特定数据集，性能评估策略有一系列从简单到更复杂(尽管是优化的)的方法。最简单的评估策略是训练候选架构，并评估其在测试数据上的性能——如果它成功了，那就太好了。否则，扔掉它，尝试不同的架构组合。随着候选架构数量的增长，这种方法可能很快变得极其资源密集；因此，引入了低保真度策略，如更短的训练时间、子集训练和每层更少的滤波器，但这些策略并不详尽。早期停止，换句话说，通过推断架构的学习曲线来评估架构的性能，对于这种近似也是一种有益的优化。变形训练过的神经架构，以及将所有架构视为超级图的子图的单短搜索，也是关于单次架构搜索的有效方法。

已经进行了几项与自动化 ML 相关的调查,对这些技术进行了深入的概述。具体的技术也有自己的出版物，有清晰的基准数据、挑战和成功——所有这些都超出了本文的范围。然而，在下一章中，我们将使用利用这些技术的库，所以你将更好地接触到它们的可用性。

# 总结

今天，ML 在企业中的成功很大程度上取决于人类 ML 专家，他们能够构建特定于业务的特性和工作流。自动化 ML 旨在改变这种情况，因为它旨在自动化 ML，以便提供无需专业知识即可使用的现成 ML 方法。为了理解自动化 ML 是如何工作的，我们需要回顾自动化 ML 的四个子领域或支柱:超参数优化；自动化特征工程；神经架构搜索；和元学习。

在这一章中，我们解释了使自动化 ML 成为可能的技术、技巧和工具。我们希望本章已经向您介绍了自动化 ML 技术，并且您现在已经准备好更深入地研究实现阶段。

在下一章中，我们将回顾实现这些算法的开源工具和库，以获得如何在实践中使用这些概念的实际概述，请继续关注。

# 延伸阅读

有关以下主题的更多信息，请参考建议的资源和链接:

*   *自动化 ML:方法、系统、挑战* : Frank Hutter(编辑)，Lars Kotthoff(编辑)，Joaquin Vanschoren(编辑)。关于 ML 挑战的 Springer 系列
*   自动化 ML 实践:使用 AutoML 和 Python 构建自动化 ML 系统的初学者指南，作者:西班詹·达斯和于米特·梅尔特·卡马克
*   *具有强化学习的神经架构搜索*:【https://arxiv.org/pdf/1611.01578.pdf】T2
*   *学习可扩展图像识别的可转移架构*:【https://arxiv.org/pdf/1707.07012.pdf 
*   *渐进式神经架构搜索*:https://arxiv.org/pdf/1712.00559.pdf[T3](https://arxiv.org/pdf/1712.00559.pdf)
*   *通过参数共享的高效神经架构搜索*:【https://arxiv.org/pdf/1802.03268.pdf】T2
*   *通过网络改造进行高效架构搜索*:【https://arxiv.org/pdf/1707.04873.pdf 
*   *网络态射*:【https://arxiv.org/pdf/1603.01670.pdf】T2
*   *通过拉马克进化的高效多目标神经架构搜索*:【https://arxiv.org/pdf/1804.09081.pdf】T2
*   *Auto-Keras* : *一个高效的神经架构搜索系统*:[https://arxiv.org/pdf/1806.10282.pdf](https://arxiv.org/pdf/1806.10282.pdf)
*   *卷积神经网络*:【https://arxiv.org/pdf/1606.02492.pdf 
*   *飞镖* : *差异化建筑搜索*:【https://arxiv.org/pdf/1806.09055.pdf】T4
*   *神经架构优化*:【https://arxiv.org/pdf/1808.07233.pdf 
*   *粉碎*:通过超级网络的一次性模型架构搜索:【https://arxiv.org/pdf/1708.05344.pdf】T2
*   *py torch 里的飞镖*:【https://github.com/quark0/darts 
*   使用模拟退火的超参数调整:[https://santhoshhari . github . io/2018/05/18/hyperparameter-Tuning-Using-Simulated-Annealing . html](https://santhoshhari.github.io/2018/05/18/hyperparameter-tuning-using-simulated-annealing.html)
*   贝叶斯优化:[http://krasserm.github.io/2018/03/21/bayesian-optimization/](http://krasserm.github.io/2018/03/21/bayesian-optimization/)
*   神经架构搜索:一项调查:[https://www.jmlr.org/papers/volume20/18-598/18-598.pdf](https://www.jmlr.org/papers/volume20/18-598/18-598.pdf)
*   将 ML 应用于生物信息学问题的数据驱动建议:[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/)