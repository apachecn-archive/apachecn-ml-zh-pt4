# 5.集合模型的可解释性

集合模型是一组模型，其中使用某种度量来聚集预测，以便生成最终预测。例如，可以开发一组基于树的模型来预测实值输出，如回归模型。对所有树的预测进行平均，并作为最终输出。以类似的方式，分类模型生成单独的类预测，然后采用投票规则。编号最高的类被作为最终输出。集合模型不仅难以解释，而且难以向最终用户解释。考虑一个场景，四棵树预测是，六棵树预测否；基于 6/4 的多数原则，我们将考虑否定作为最终答案。这里很难向最终用户解释为什么一些模型预测是。因此，解释集合模型是非常重要的。在这一章，你将主要使用 SHAP 来解释集合模型预测。

## 集合模型

集合模型是需要详细解释的最复杂的模型集，因为输出是多个预测的组合结果。*合奏*简单来说就是*分组*。在集合模型的情况下，重要的是如何解释预测，哪个模型变量实际上产生了预测，以及如何读取特征在最终预测过程中的边际贡献。

决策树模型的优势在于它考虑了数据集中存在的潜在非线性。在生成模型预测时，变量交互开始起作用。然而，决策树模型的局限性在于它容易产生偏差，因为强大或较强的特征参与树的构建过程，而弱特征由于缺乏预测能力而无法进入树的分支过程。因此，模型变得偏向于数据集的一些选择性更强的要素。这有时也会导致模型过度拟合。为了平衡强特征的影响，规范特征进入基于树的模型是很重要的。如果您将强特征排除在模型创建步骤之外，而仅将弱特征包括在树创建中，您仍将能够生成预测，但这将再次成为有偏差的模型。因此，只有在树构建过程中使用强特征和弱特征的组合时，才能处理模型偏差并同时控制过度拟合。组合可以完全基于自举方法来完成，并且树的数量可以增加到足以平均预测。

### 集合模型的类型

集合模型有三种类型:bagging 模型(也称为 bootstrap 聚合模型)、boosting 模型和 stacking 模型(图 [5-1](#Fig1) )。叠加模型有两种类型:同组叠加和不同变体模型叠加。相同的组堆叠涉及同质模型类型，例如仅包括基于树的模型，并且将每个模型结果与其他模型进行比较。异构堆叠模型意味着将基于树的模型和非基于树的模型放在一起，并组合它们的预测。叠加模型的模型可解释性一点也不困难，因为我们可以识别特定的模型并解释预测和参数；然而，解释装袋和助推模型有点棘手。随机森林模型是 bagging 模型的一个示例，在该模型中，种植了许多树木，最后将预测结果组合在一起以得出最终结果。推进模型采用基础模型，从基础模型的结果中学习，并尝试以迭代的方式改进模型。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig1_HTML.png](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig1_HTML.png)

图 5-1

三种类型的集合模型

## 为什么是集合模型？

当数据集中的特征总数增加时，比方说超过 50 个，由于以下原因，它不能容纳在单个决策树中:

*   强有力的特征将占据决策树构造中的分支创建步骤；因此，弱特征将不参与树的形成。

*   单一决策树场景中的模型变得更有偏差，因为一些强大的功能控制着预测。

*   由于一些强大的功能，超参数的选择是有限的。

*   当你想到在 100 多个特性中，有 20%的强大特性驱动着预测时，这个问题就变得更大了。剩余的 80%的弱特征未被利用。

*   由于单一的树，不是所有的特征在预测中都有公平的机会。

*   单决策树场景中的偏差缓解主要是通过修剪来完成的。

为了解决单个决策树模型的上述限制，需要有一组树，因此命名为*树集合*。装袋、推进和堆叠模型可以应用于基于回归的问题以及基于分类的问题。图 [5-2](#Fig2) 提供了区分三种集合模型的示意图。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig2_HTML.png](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig2_HTML.png)

图 5-2

装袋模型

Bagging(也称为 bootstrap aggregation)从训练数据集中获取不同的特征和记录样本，并尝试训练决策树。在图 [5-2](#Fig2) 中，你可以开发 n 棵树，其中 n 可以是 100、500 或 1000。随着树数量的增加，训练数据集中的要素越多，模型的准确性也会提高。如果你有更少的特征和样本，并且如果你增加树的数量，那么你将能够获得更高的精度，但是在这个过程中你将会产生大量的重复树。它们是重复的树，因为相同的特征集出现在每个树中。Bagging 本质上是并行的，因为您可以为基于分类和回归的任务并行训练多个树。因此，按照行业惯例，当训练数据集中有超过 50 个特征和超过 50，000 条记录时，建议使用集成。

另一方面，提升是一个连续的过程，其中训练基本分类器来分类或预测目标列。然后，在分类场景中分离正确预测的情况，并且在基于回归的场景中拾取误差分布，以便在相同的配置或设置上重新训练模型。这个过程重复多次，直到不可能进一步提高精度。这在本质上是连续的，因为第二个模型对第一个模型的结果起作用。这是获得集合模型的一种强有力的技术。该过程如图 [5-3](#Fig3) 所示。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig3_HTML.png](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig3_HTML.png)

图 5-3

助推模型训练过程

系综模型的第三个变体是叠加。有两种叠加模型预测的方法:同质组叠加和异质组叠加。见表 [5-1](#Tab1) 。

表 5-1

回归问题同质群堆叠

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"></colgroup> 
|   | 

模型

 |   |
| --- | --- | --- |
| 

**测试数据**

 | 

暗行扫描(Dark Trace)

 | 

无线电频率(radio frequency)

 | 

自适应增压

 | 

梯度推进

 | 

XGBoost

 | 

预报

 |
| --- | --- | --- | --- | --- | --- | --- |
| **1** | Twenty-five point three | Twenty-five point two | Twenty-four point three | Twenty-six point one | Twenty-four point five | Twenty-five point zero eight |
| **2** | Twelve point five | Thirteen point two one | Fourteen point one | Eleven point nine | Twelve point two | Twelve point seven eight two |
| **3** | Seventeen point eight | Fifteen point three | Sixteen point three | Sixteen point seven | Seventeen point one | Sixteen point six four |

最后一列“预测”是不同模型预测值的平均值。您只考虑了测试数据集中的三条记录。现在参见表 [5-2](#Tab2) 。

表 5-2

分类问题同质群组堆叠

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"></colgroup> 
|   | 

模型

 |   |
| --- | --- | --- |
| 

**数据**

 | 

暗行扫描(Dark Trace)

 | 

无线电频率(radio frequency)

 | 

自适应增压

 | 

梯度推进

 | 

XGBoost

 | 

预报

 |
| --- | --- | --- | --- | --- | --- | --- |
| **1** | 是 | 不 | 是 | 是 | 是 | 是 |
| **2** | 不 | 不 | 不 | 是 | 不 | 不 |
| **3** | 是 | 是 | 不 | 不 | 不 | 不 |

在第一行有四个是和一个不是，因此最后的预测是肯定的。同样，在第二行和第三行，最终预测由多数投票决定。在异构堆叠场景中，不仅基于树的模型可以参与，其他模型也可以参与，例如逻辑回归模型、支持向量机模型等。在表 [5-3](#Tab3) 中，可以看到回归模型异质组叠加，在表 [5-4](#Tab4) 中，可以看到异质组叠加的分类模型。

表 5-4

分类问题异质堆积

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"> <col class="tcol8 align-left"> <col class="tcol9 align-left"></colgroup> 
|   | 

模型

 |   |   |   |
| --- | --- | --- | --- | --- |
| 

**数据**

 | 

暗行扫描(Dark Trace)

 | 

无线电频率(radio frequency)

 | 

自适应增压

 | 

梯度推进

 | 

XGBoost

 | 

支撑向量机

 | 

实验室反应堆

 | 

预报

 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **1** | 是 | 不 | 是 | 是 | 是 | 不 | 不 | 是 |
| **2** | 不 | 不 | 不 | 是 | 不 | 是 | 是 | 不 |
| **3** | 是 | 是 | 不 | 不 | 不 | 不 | 是 | 不 |

表 5-3

回归问题异质群堆叠

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"> <col class="tcol8 align-left"> <col class="tcol9 align-left"></colgroup> 
|   | 

模型

 |   |   |   |
| --- | --- | --- | --- | --- |
| 

**数据**

 | 

暗行扫描(Dark Trace)

 | 

无线电频率(radio frequency)

 | 

自适应增压

 | 

梯度推进

 | 

XGBoost

 | 

支撑向量机

 | 

实验室反应堆

 | 

预报

 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **1** | Twenty-five point three | Twenty-five point two | Twenty-four point three | Twenty-six point one | Twenty-four point five | Twenty-four point nine | Twenty-four point six | 24.98571429 |
| **2** | Twelve point five | Thirteen point two one | Fourteen point one | Eleven point nine | Twelve point two | Twelve point six | Eleven point nine | Twelve point six three |
| **3** | Seventeen point eight | Fifteen point three | Sixteen point three | Sixteen point seven | Seventeen point one | Sixteen point eight | Seventeen point one | 16.72857143 |

## 将 SHAP 用于集合模式

您将考虑两个不同的数据集。您将使用流行的波士顿房价数据集来解释回归用例场景中的模型预测，并使用成人数据集来解释分类场景。以下是波士顿房价数据集中的变量:

*   `CRIM`:城镇人均犯罪率

*   `ZN`:面积超过 25，000 平方英尺的住宅用地比例

*   `INDUS`:每镇非零售营业亩数比例

*   `CHAS`:查尔斯河虚拟变量(1 如果区域边界为河流；否则为 0)

*   `NOX`:一氧化氮浓度(百万分之一)

*   `RM`:每套住宅的平均房间数

*   `AGE`:1940 年以前建造的业主自用单元的比例

*   `DIS`:到五个波士顿就业中心的加权距离

*   `RAD`:放射状公路可达性指标

*   `TAX`:每万美元的全值财产税税率

*   `PTRATIO`:各城镇学生与教师的比例

*   `B` : 1000(Bk - 0.63)^2 其中 Bk 是各城镇黑人的比例

*   `LSTAT` : %较低的人口地位

*   `MEDV`:以千美元为单位的自有住房中值

```
import pandas as pd
import shap
import sklearn

# boston Housing price prediction
X,y = shap.datasets.boston()
X100 = shap.utils.sample(X, 1000) # 1000 instances for use as the background distribution

# a simple linear model
model = sklearn.linear_model.LinearRegression()
model.fit(X, y)

```

波士顿房价数据集现在是 SHAP 图书馆的一部分。基本模型计算使用线性回归模型进行，因此您可以对该数据集执行集成模型并比较结果。

```
print("Model coefficients:\n")
for i in range(X.shape[1]):
    print(X.columns[i], "=", model.coef_[i].round(4))

```

表 5-5

线性回归模型的模型系数

<colgroup><col class="tcol1 align-left"></colgroup> 
| 模型系数: |
| 卷曲= -0.108 |
| 锌= 0.0464 |
| 印度河= 0.0206 |
| 查斯= 2.6867 |
| NOX = -17.7666 |
| RM = 3.8099 |
| 年龄= 0.0007 |
| DIS = -1.4756 |
| 拉德= 0.306 |
| 税= -0.0123 |
| PTRATIO = -0.9527 |
| B = 0.0093 |
| 国家= 0.5248 |

基础模型的系数是起点(表 [5-5](#Tab5) )。随后，你将会看到复杂的集合模型，并将系数与基本的线性模型进行比较。还有，可以对比一下解释。预测越好，解释能力就越强。

```
shap.plots.partial_dependence(
    "RM", model.predict, X100, ice=False,
    model_expected_value=True, feature_expected_value=True
)

```

图 [5-4](#Fig4) 所示的水平虚线 E【f(x)】不过是房价平均值的预测中值。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig4_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig4_HTML.jpg)

图 5-4

使用 X100 子集样本的 RM 和预测房价的 PDP

如果取预测结果的平均值，就是 22.84。RM 特征与模型预测结果之间存在线性关系，如图 [5-4](#Fig4) 所示。

```
# compute the SHAP values for the linear model
explainer = shap.Explainer(model.predict, X100)
shap_values = explainer(X)

# make a standard partial dependence plot
sample_ind = 18
shap.partial_dependence_plot(
    "RM", model.predict, X100, model_expected_value=True,
    feature_expected_value=True, ice=False,
    shap_values=shap_values[sample_ind:sample_ind+1,:]
)

```

图 [5-5](#Fig5) 显示 RM 特征对目标列预测值的边际贡献是线性关系，如向上上升的直线所示。红线表示预测值和平均预测值之间的差异。SHAP 值相当于预测值。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig5_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig5_HTML.jpg)

图 5-5

数据集的第 18 行叠加在 PDP 图上

```
X100 = shap.utils.sample(X,100)
model.predict(X100).mean()
model.predict(X100).min()
model.predict(X100).max()
shap_values[18:19,:]
X[18:19]
model.predict(X[18:19])
shap_values[18:19,:].values.sum() + shap_values[18:19,:].base_values

```

因此，第 18 条记录的预测结果为 16.178，不同要素的 SHAP 值加上基础值(即平均预测值)之和等于预测值 16.178。

```
shap.plots.scatter(shap_values[:,"RM"])

```

在图 [5-6](#Fig6) 中，关系是线性的，因为 SHAP 值是使用线性模型作为解释器生成的。如果你切换到一个非线性模型，你可以预期线是非线性的。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig7_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig7_HTML.jpg)

图 5-7

预测结果与 SHAP 值之间的关系

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig6_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig6_HTML.jpg)

图 5-6

RM 和 RM 的 SHAP 值之间的线性关系

```
# the waterfall_plot shows how we get from shap_values.base_values to model.predict(X)[sample_ind]
shap.plots.waterfall(shap_values[sample_ind], max_display=14)

```

在图 [5-7](#Fig7) 中，横轴显示预测结果平均值，为 22.841，纵轴显示不同特征的 SHAP 值。数据集中每个要素的假设值显示为灰色，负的 SHAP 值显示为蓝色，正的 SHAP 值显示为红色。纵轴还显示第 18 条记录的预测结果，即 16.178。

## 使用解释助推模型

在本节中，您将使用广义可加模型(GAM)来预测房价。你可以用 SHAP 图书馆来解释这个模型。可以使用解释 Python 库来训练广义加法模型，然后可以将训练的模型对象传递给 SHAP 模型，以生成对增强模型的解释。

解释器库可以通过三种方式安装:

```
!pip install interpret-core

```

这与使用 pip 安装过程没有任何依赖关系。

```
conda install -c interpretml interpret-core

```

这是使用 anaconda 发行版。您可以使用 conda 环境中的终端进行安装。

```
git clone https://github.com/interpretml/interpret.git && cd interpret/scripts && make install-core

```

这是直接来自使用 GitHub 的源代码。

解释 Python 库支持两种算法:

*   **Glassbox 模型**:这些模型被设计成更易解释，使用 scikit-learn 框架，保持与最先进的 sklearn 库相同的准确性。它们支持四种不同的模型:线性模型、决策树、决策规则和基于 boosting 的模型。

*   **黑盒解释器**:这些被设计来提供关于模型行为和模型预测的近似解释。当机器学习模型的组件都不可解释时，这些算法的变体将是有用的。它们支持 Shapely 解释、LIME 解释、部分依赖图和 Morris 敏感度分析。

```
# fit a GAM model to the data
import interpret.glassbox
model_ebm = interpret.glassbox.ExplainableBoostingRegressor()
model_ebm.fit(X, y)

```

首先，您必须从 interpret 导入 glassbox 模块，初始化可解释的 boosting 回归器，并拟合模型。模型对象是`model_ebm`。

```
# explain the GAM model with SHAP
explainer_ebm = shap.Explainer(model_ebm.predict, X100)
shap_values_ebm = explainer_ebm(X)

```

您将对训练数据集进行采样，并获取 100 个样本来创建一个使用 SHAP 库生成解释的背景。在 SHAP 解释器中，你使用`model_ebm.predict`并抽取 100 个样本来生成解释。

```
# make a standard partial dependence plot with a single SHAP value overlaid
fig,ax = shap.partial_dependence_plot(
    "RM", model_ebm.predict, X, model_expected_value=True,
    feature_expected_value=True, show=False, ice=False,
    shap_values=shap_values_ebm[sample_ind:sample_ind+1,:]
)

```

图 [5-8](#Fig8) 显示了基于增压的模型。你可以看到一个分段的非线性曲线，以及 RM 值和预测目标列(房价的平均值)之间的非线性关系。你在这里再次解释同样的第 18 条记录，如红色直线所示。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig8_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig8_HTML.jpg)

图 5-8

RM 特性的升压模型 PDP 图

```
shap.plots.scatter(shap_values_ebm[:,"RM"])

```

在图 [5-9](#Fig9) 中，关系显示为非线性曲线。在初始阶段，随着 RM 的增加，预测值不会增加太多，但是在某个阶段之后，随着 RM 值的增加，RM 的 SHAP 值也呈指数增加。另见图 [5-10](#Fig10) 。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig10_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig10_HTML.jpg)

图 5-10

同一组变量的瀑布图

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig9_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig9_HTML.jpg)

图 5-9

RM 和 RM 的 SHAP

```
# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]
shap.plots.waterfall(shap_values_ebm[sample_ind], max_display=14)

```

```
# the waterfall_plot shows how we get from explainer.expected_value to model.predict(X)[sample_ind]
shap.plots.beeswarm(shap_values_ebm, max_display=14)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig11_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig11_HTML.jpg)

图 5-11

特征值与 SHAP 值

图 [5-11](#Fig11) 是 SHAP 值和特征值的另一个可视化。

```
# train XGBoost model
import xgboost
model_xgb = xgboost.XGBRegressor(n_estimators=100, max_depth=2).fit(X, y)

# explain the GAM model with SHAP
explainer_xgb = shap.Explainer(model_xgb, X100)
shap_values_xgb = explainer_xgb(X)

# make a standard partial dependence plot with a single SHAP value overlaid
fig,ax = shap.partial_dependence_plot(
    "RM", model_xgb.predict, X, model_expected_value=True,
    feature_expected_value=True, show=False, ice=False,
    shap_values=shap_values_ebm[sample_ind:sample_ind+1,:]
)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig12_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig12_HTML.jpg)

图 5-12

提取 RM 和 SHAP 值的 PDP 图

在图 [5-12](#Fig12) 中，极端梯度推进回归模型用于解释集合模型，这里你也可以看到存在非线性关系。

```
shap.plots.scatter(shap_values_xgb[:,"RM"])

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig13_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig13_HTML.jpg)

图 5-13

RM 散点图的 RM 和 SHAP 值

在图 [5-13](#Fig13) 中，有一条曲线显示了 RM 和 RM 的 SHAP 值之间的非线性关系。

```
shap.plots.scatter(shap_values_xgb[:,"RM"], color=shap_values)

```

图 [5-14](#Fig14) 显示了相同的非线性关系和额外的税收特征叠加，显示 RM 值越高，税收成分越高，反之亦然。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig14_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig14_HTML.jpg)

图 5-14

RM 和 RM 的 SHAP 可以用第三维度来表现，那就是颜色

## 集合分类模型:SHAP

对于回归模型，您已经看到了 SHAP 特征和特征重要性，两者都包括一些特征的部分相关性图。现在让我们看看常见和流行的收入普查分类数据集(俗称成人数据集)。它之所以受欢迎，是因为它易于理解和理解，并且出现在几乎所有的机器学习示例代码中。

```
# a classic adult census dataset price dataset
X_adult,y_adult = shap.datasets.adult()

# a simple linear logistic model
model_adult = sklearn.linear_model.LogisticRegression(max_iter=10000)
model_adult.fit(X_adult, y_adult)

```

在上面的脚本中，加载了人口普查收入数据集，它已经是 SHAP 图书馆的一部分。训练逻辑回归模型，因为这是一个二元分类问题。最后，模型拟合过程创建训练模型。

```
def model_adult_proba(x):
    return model_adult.predict_proba(x)[:,1]
def model_adult_log_odds(x):
    p = model_adult.predict_log_proba(x)
    return p[:,1] - p[:,0]

```

对于使用人口普查收入数据集的分类示例，目标列有两个标签:年收入超过 5 万美元的人和年收入低于 5 万美元的人。这是一个二元分类模型，作为基线，您可以使用逻辑回归模型将 PDP 显示为 SHAP 值的线性函数。然后，您可以使用集合模型来比较 PDP，并显示函数如何变得非线性。非线性和复杂的关系被集合模型捕获。名为`model proba`和`model log odds`的两个函数分别提供两个类别的概率值和对数比值比。

```
# make a standard

partial dependence plot
sample_ind = 18
fig,ax = shap.partial_dependence_plot(
    "Capital Gain", model_adult_proba, X_adult, model_expected_value=True,
    feature_expected_value=True, show=False, ice=False
)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig15_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig15_HTML.jpg)

图 5-15

资本收益和赚少于 5 万美元的概率之间的 PDP

图 [5-15](#Fig15) 中的 x 轴显示资本收益变量，y 轴显示少于 50K 美元的预测概率。概率值的范围从 0.05 到 0.99，在此之后，它变得接近 100%，但不完全是 100%。这里的解释是，由于资本收益金额超过 20K 美元，因此对预测的概率值没有影响。高达 20K 美元，随着资本收益的增加，预测的概率也增加。

```
# compute the SHAP values for the linear model
background_adult = shap.maskers.Independent(X_adult, max_samples=100)
explainer = shap.Explainer(model_adult_proba, background_adult)
shap_values_adult = explainer(X_adult[:1000])

shap.plots.scatter(shap_values_adult[:,"Age"])

```

图 [5-16](#Fig16) 显示了线性关系，但是数据点的分散性存在变化，这表明可能存在非线性，并且无法使用线性模型正确捕捉。因此，需要集合模型。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig16_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig16_HTML.jpg)

图 5-16

线性模型的年龄与年龄的 SHAP 值之间的散点图

```
# compute the SHAP values for the linear model
explainer_log_odds = shap.Explainer(model_adult_log_odds, background_adult)
shap_values_adult_log_odds = explainer_log_odds(X_adult[:1000])

shap.plots.scatter(shap_values_adult_log_odds[:,"Age"])

# make a standard partial dependence plot
sample_ind = 18
fig,ax = shap.partial_dependence_plot(
    "Age", model_adult_log_odds, X_adult, model_expected_value=True,
    feature_expected_value=True, show=False, ice=False
)

# train XGBoost model
model = xgboost.XGBClassifier(n_estimators=100, max_depth=2).fit(X_adult, y_adult)

# compute SHAP values
explainer = shap.Explainer(model, background_adult)
shap_values = explainer(X_adult)

# set a display version of the data to use for plotting (has string values)
shap_values.display_data = shap.datasets.adult(display=True)[0].values

```

现在让我们看看极端梯度提升分类器模型。作为增强模型，它提高了分类器的准确性，同时提供了对预测结果的解释。

```
shap.plots.bar(shap_values)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig17_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig17_HTML.jpg)

图 5-17

对要素进行排序的平均绝对 SHAP 值

在图 [5-17](#Fig17) 中，关系变量显示了最高的平均绝对 SHAP 值，这反映了该变量在预测目标列的类别中的重要性。第二个最重要的变量是年龄，然后是资本收益，等等。

```
shap.plots.bar(shap_values.abs.max(0))

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig18_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig18_HTML.jpg)

图 5-18

不同特征的 SHAP 绝对值的最大值

在图 [5-18](#Fig18) 中，资本收益特征具有最高的 SHAP 值，因此这可能是异常值之一。同样，资本损失也可能是一个离群值。这将通过图 [5-19](#Fig19) 得到澄清。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig19_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig19_HTML.jpg)

图 5-19

相对于特征值绘制的 SHAP 值

在图 [5-19](#Fig19) 中，SHAP 值显示了特征值如何影响模型输出。蓝色点表示低特征值，红色点表示高特征值。

```
shap.plots.beeswarm(shap_values)

```

```
shap.plots.beeswarm(shap_values.abs, color="shap_red")

```

在图 [5-20](#Fig20) 中，你看不到负的 SHAP 值，资本收益和资本损失这两个特征表明 SHAP 值有很多变化，这意味着这两个特征的实际值的变化可能会产生不一致的预测。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig20_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig20_HTML.jpg)

图 5-20

SHAP 的绝对值

在图 [5-21](#Fig21) 中，让我们考虑一下关系特征。直到第 430 个<sup>第</sup>个实例，它都有很高的 SHAP 分数。此后，SHAP 值变为负值，直到第 580 <sup>个</sup>实例，并持续到第 610 <sup>个</sup>实例。之后，关系要素的所有实例都会产生负 SHAP 值。该图显示了训练数据集中不同实例的 SHAP 值的变化。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig21_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig21_HTML.jpg)

图 5-21

按实例和 SHAP 值的特征值变化

```
shap.plots.heatmap(shap_values[:1000])

```

图 [5-22](#Fig22) 解释了年龄变量和年龄变量的 SHAP 分数之间的严格非线性关系。这种非线性是由极端梯度增强分类器模型产生的。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig22_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig22_HTML.jpg)

图 5-22

年龄和年龄的 SHAP 值

```
shap.plots.scatter(shap_values[:,"Age"])

```

```
shap.plots.scatter(shap_values[:,"Age"], color=shap_values)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig23_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig23_HTML.jpg)

图 5-23

年龄和 SHAP 值年龄与教育数字叠加

在图 [5-23](#Fig23) 中，年龄和教育数字遵循与 SHAP 年龄值相似的模式。

```
shap.plots.scatter(shap_values[:,"Age"], color=shap_values[:,"Capital Gain"])

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig24_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig24_HTML.jpg)

图 5-24

与每周小时数的关系和 SHAP 值

在图 [5-24](#Fig24) 中，丈夫、妻子和其余的关系状态值有明显的不同。丈夫和妻子的 SHAP 价值观是积极的；然而，对于其他四种关系，SHAP 值为负。

## 用 SHAP 解释分类推进模型

当分类模型的特性包含分类变量时，我们通常对特性进行一次热编码，以便识别每个特性对目标列的影响。在本例中，您将使用 CatBoost Python 库和 SHAP 来生成解释。

要安装 CatBoost 库，可以使用 pip 安装命令。

```
!pip install catboost
shap.initjs()
import catboost
from catboost.datasets import *
import shap

```

这个人口普查收入数据集具有诸如关系、性别、职业和婚姻状况等分类特征。这些分类特征不能使用 sklearn 或任何其他管道(如 Keras、TensorFlow 等)直接传递给机器学习训练算法。这些分类特征首先需要被转换成标签编码器，然后被转换成一个热编码器。如果你有超过一百个特征是绝对的，这就变得非常麻烦了。因此你需要一个可以自动处理这个过程的库。CatBoost 库满足了这一需求。

Initjs()函数将 JavaScript 可视化代码加载到 Jupyter 笔记本中。因此，我们使用上面的语法来加载 JS 功能。

CatBoost 回归模型或分类模型在梯度增强模型之上工作，因此被称为 CatBoost 模型。

```
X,y = shap.datasets.boston()

model = CatBoostRegressor(iterations=300, learning_rate=0.1, random_seed=123)
model.fit(X, y, verbose=False, plot=False)

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# visualize the first prediction's explanation
shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig25_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig25_HTML.jpg)

图 5-25

用于记录预测的力图

在图 [5-25](#Fig25) 中，你试图解释一个局部预测。对于定型数据集中的记录 0，您正在显示预测解释。例如，您获取了波士顿房价数据集，使用默认参数训练了一个 CatBoost 模型，并保存了该模型。下一步，使用树形解释器生成解释器对象，并且使用训练数据集 X 作为输入，再次使用解释器对象来生成 SHAP 值。最后，使用训练数据集中输入的第一条记录的预测值和 SHAP 值来创建力图可视化。第一次记录，预测房价是 25.35，也就是 25350 美元。预测房价的平均值是 22.53，也就是 22530 美元。年龄、PTRATIO 和 LSTAT 等特征将预测值推得更高(红色箭头指向右侧)。RAD、CRIM 和 RM 等特性将预测向下推，这意味着向左推预测，如向后显示的蓝色箭头所示。以灰色显示在水平轴上的值是所有样品的预测值。

```
# visualize the training set predictions
shap.force_plot(explainer.expected_value, shap_values, X)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig26_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig26_HTML.jpg)

图 5-26

按样本相似度排序的模型解释器

在图 [5-26](#Fig26) 中，样本按样本相似度排序。横轴显示样本，纵轴显示预测输出值。这是一个有趣的可视化；只需悬停鼠标或光标，您就可以看到要素的 SHAP 值、准确的预测输出值以及选择作为解释器输入的相应样本。该图表详细展示了对预测结果和特性贡献的理解。

为了解释如何应用 CatBoost 分类器，让我们使用 Amazon 数据集。表 [5-6](#Tab6) 列出了数据集中的特征。

表 5-6

亚马逊数据集数据描述

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

功能名称

 | 

描述

 |
| --- | --- |
| `ACTION` | 如果资源被批准，操作为 1；如果资源未被批准，操作为 0 |
| `RESOURCE` | 每个资源的 ID |
| `MGR_ID` | 当前员工 ID 记录的经理的员工 ID；一个员工一次只能有一个经理 |
| `ROLE_ROLLUP_1` | 公司角色分组类别 id 1(例如，美国工程) |
| `ROLE_ROLLUP_2` | 公司角色分组类别 id 2(例如，美国零售) |
| `ROLE_DEPTNAME` | 公司角色部门描述(如零售) |
| `ROLE_TITLE` | 公司角色职务描述(例如，高级工程零售经理) |
| `ROLE_FAMILY_DESC` | 公司角色系列扩展描述(例如，零售经理、软件工程) |
| `ROLE_FAMILY` | 公司角色系列描述(例如，零售经理) |
| `ROLE_CODE` | 公司角色代码；该代码对每个角色(如经理)都是唯一的 |

当员工开始在软件开发部门工作时，他们需要特定的访问和权限来满足他们的工作要求。有一个授予员工访问权限的手动过程，但员工可能会在该过程中遇到障碍。没有授予对必要软件系统的访问权的自动化机制。此用例的目标是对历史数据进行建模，以确定员工的访问需求。目标列为`ACTION`；如果资源被批准，则为 1；如果资源访问被拒绝，则为 0。其余的特征用于预测目标特征。

```
train_df, test_df = catboost.datasets.amazon()
train_df

set(train_df.ACTION)

y_train = train_df.ACTION
X_train = train_df.drop('ACTION',axis=1)
cat_features = list(range(0,X_train.shape[1]))

from catboost import Pool, CatBoostClassifier, cv
# Initialize CatBoostClassifier
model = CatBoostClassifier(iterations=300,
                           learning_rate=0.1,
                           random_seed=12)

# Fit model

model.fit(X_train,y_train,cat_features=cat_features,verbose=False,plot=False)

```

使用 CatBoost 分类器的模型训练过程有些复杂，这意味着复杂的关系不能由单个函数来近似。如果要素具有线性关系，您可以使用直线来近似函数。然而，如果特征是圆形或锯齿形图案，那么它被称为复杂图案，因为数学上没有函数可以近似圆形或锯齿形图案。在这些情况下，要么需要一个以上的函数，要么需要数据规范化来匹配任何数学函数。

在当前情况下，由于要素数据集的复杂性，SHAP 值是近似计算的。

```
eval_dataset = [X_train.iloc[0:1]]
eval_dataset

# Get predicted classes
preds_class = model.predict(X_train)

# Get predicted probabilities for each class
preds_proba = model.predict_proba(X_train)

# Get predicted RawFormulaVal
preds_raw = model.predict(X_train,
                          prediction_type='RawFormulaVal')

preds_class

preds_proba

preds_raw

```

经过训练的模型存储在模型对象中。您可以以两个样本为例，得出该实例的概率和一个原始公式值。样本可以是训练数据集中的任何数据点。在脚本中，我们以第一和第 91 记录为例。

```
import numpy as np
np.log(0.9964/(1-0.9964))
np.exp(5.62)/(1+np.exp(5.62))
cat_features

```

第一个记录显示了朝向类别 1 的 99.64%的预测概率。对数比值为 5.61。Log odds 表示 log(P/1-P ),这里 P 是类 1 的概率。如果想从原始预测值中得到概率，可以使用公式 exp(原始预测)/ (1+ exp(原始预测))。例如，对于第一条记录，exp(5.61) / (1+ exp(5.61)) = 0.9964。以类似的方式，您可以解释第 91 条记录的原始预测值-3.4734。CatBoost 模型提供了预测不同预测类型的选项；参见表 [5-7](#Tab7) 。

表 5-7

由 CatBoost 模型对象生成的预测类型

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

预测类型

 | 

描述

 |
| --- | --- |
| **概率** | 对应于每个类别的概率值 |
| **类** | 将生成类别标签 |
| **RawFormula Value** | 对数概率值 |
| **指数** | 预测概率的指数值 |
| **对数概率** | 概率的对数 |

CatBoost 模型中不同类型的预测有助于更好地理解预测值以及模型如何生成预测。所有类型之间的联系提供了更好的理解。您可以转换原始预测值，以获得第 1 类的概率。

```
from catboost.datasets import *
train_df, test_df = catboost.datasets.amazon()
y = train_df.ACTION
X = train_df.drop('ACTION', axis=1)
cat_features = list(range(0, X.shape[1]))

model = CatBoostClassifier(iterations=300, learning_rate=0.1, random_seed=12)
model.fit(X, y, cat_features=cat_features, verbose=False, plot=False)

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(Pool(X, y, cat_features=cat_features))

test_objects = [X.iloc[7:9], X.iloc[56:58]]

for obj in test_objects:
    print('Probability of class 1 = {:.4f}'.format(model.predict_proba(obj)[0][1]))
    print('Formula raw prediction = {:.4f}'.format(model.predict(obj, prediction_type='RawFormulaVal')[0]))
    print('\n')

shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Figa_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Figa_HTML.jpg)

```
shap.force_plot(explainer.expected_value, shap_values[91,:], X.iloc[91,:])

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig27_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig27_HTML.jpg)

图 5-27

来自训练集的记录 1 的力图

在图 [5-27](#Fig27) 中，力图显示了如何根据拉(蓝色，下图)和推(红色，下图)特征值来确定原始预测值。从 1.124 到 6.124 的标度上反映的值是原始预测值，其是相对于类别 1 的预测概率的对数优势比。平均对数比值比为 3.624。拉动值使用诸如资源、角色头衔、滚动、上滚 2 等特征来降低预测的比值比。推送功能(角色部门名称、经理 id 等。)会把预测的赔率提高到更高的水平。最终值为 5.61，这是最终预测的原始概率值，也称为比值比。

```
shap.summary_plot(shap_values, X)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig28_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig28_HTML.jpg)

图 5-28

显示对模型输出影响的 SHAP 值汇总

图 [5-28](#Fig28) 显示了影响模型输出的特征及其重要性。资源是最具影响力的功能，然后是 MGR ID，依此类推，如图所示。

## 使用 SHAP 多类分类推进模型

分类增强分类器模型也可以用于训练多类分类模型。经过训练的模型对象可以通过 SHAP 树解释器来生成 SHAP 值。参见图 [5-29](#Fig29) 。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig29_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig29_HTML.jpg)

图 5-29

特征值和 SHAP 值表示

```
model = CatBoostClassifier(loss_function = 'MultiClass', iterations=300, learning_rate=0.1, random_seed=123)
model.fit(X, y, cat_features=cat_features, verbose=False, plot=False)

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(Pool(X, y, cat_features=cat_features))

shap.summary_plot(shap_values[0], X)

```

CatBoost 分类器或回归有许多超参数。表 [5-8](#Tab8) 显示了一些重要参数以及它们如何影响模型结果。

表 5-8

CatBoost 分类器的超级参数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

参数

 | 

说明

 |
| --- | --- |
| `cat_features` | 定型数据集中存在的分类列索引 |
| `text_features` | 文本列索引(指定为整数)或名称(指定为字符串) |
| `learning_rate` | 对于损失函数优化，用于减少梯度步骤 |
| `Iterations` | 解决机器学习问题时可以构建的最大树数 |
| `Max_depth` | 树的深度 |
| `leaf_estimation_method` | 用于计算树叶值的方法可能的值:牛顿梯度确切的取决于模式和选定的损失函数:具有分位数或 MAE 损失函数的回归-一次精确迭代使用除分位数或 MAE 之外的任何损失函数进行回归——一次梯度迭代分类模式–十次牛顿迭代多分类模式–一次牛顿迭代 |
| `boosting_type` | 升压方案可能的值:有序-通常在小数据集上提供更好的质量，但可能比普通方案慢普通-经典的渐变增强方案 |

为了获得更好的结果，上述参数可以改变或修改，并可以使用 SHAP 库进行解释。

## 用 SHAP 解释轻型 GBM 模型

正如在 CatBoost 分类器一节中所讨论的，以类似的方式，您也可以使用一个光梯度增强模型。轻型 gbm 模型和 CatBoost 模型之间的区别在于，轻型 GBM 在混合数据场景中运行速度更快，在混合数据场景中，训练数据集中有一组混合的要素。当数据集中有大量分类要素时，CatBoost 分类器非常有用。在 light GBM 中，您需要对分类特性进行编码，但是在 CatBoost 模型中，模型有一个内部机制来处理分类特性。

要安装轻型 gbm 型号:

```
conda install -c conda-forge lightgbm

from sklearn.model_selection import train_test_split
import lightgbm as lgb
import shap

X,y = shap.datasets.adult()
X_display,y_display = shap.datasets.adult(display=True)

# create a train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)
d_train = lgb.Dataset(X_train, label=y_train)
d_test = lgb.Dataset(X_test, label=y_test)

params = {
    "max_bin": 512,
    "learning_rate": 0.05,
    "boosting_type": "gbdt",
    "objective": "binary",
    "metric": "binary_logloss",
    "num_leaves": 10,
    "verbose": -1,
    "min_data": 100,
    "boost_from_average": True
}

model = lgb.train(params, d_train, 10000, valid_sets=[d_test], early_stopping_rounds=50, verbose_eval=1000)

```

您可以使用相同的人口普查收入分类数据集来训练使用轻型 GBM 模型的分类器，并生成对预测结果的解释。有许多 hyper 参数可以通过浏览 light gbm 模型的官方文档页面来了解。表 [5-9](#Tab9) 显示了以下代码行中使用的超级参数。

表 5-9

LGB 模型的超参数

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

因素

 | 

说明

 |
| --- | --- |
| `Max_bin` | 将存储要素值的最大箱数少量的箱可能会降低训练精度，但可能会增加总功率(处理过拟合) |
| `Learning_rate` | 收缩率 |
| `Boosting_type` | gbdt，传统梯度推进决策树，别名:gbrt射频，随机森林，别名:随机森林镖，辍学符合多重加法回归树goss，基于梯度的单侧采样 |
| `Objective` | 应用类型，如二进制分类、多类分类等。 |
| `Metric` | 要在评估集上评估的度量 |
| `Num_leaves` | 一棵树的最大叶子数 |
| `Min_data` | 每个分类组的最小数据量 |

训练完轻型 GBM 模型后，您可以去 SHAP 树解释器解释预测结果，也可以生成 SHAP 值的力图(图 [5-30](#Fig30) )。

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig30_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig30_HTML.jpg)

图 5-30

用 SHAP 特征值预测对数几率的力图

```
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_display.iloc[0,:])

```

在图 [5-30](#Fig30) 中，教育程度和年龄将预测优势比推高，资本收益和关系将预测优势比拉低。因此，来自数据集的记录 1 的最终预测比值比产生-8.43，这远低于由基值-2.43 所示的平均比值比。

```
shap.force_plot(explainer.expected_value[1], shap_values[1][:1000,:], X_display.iloc[:1000,:])

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig31_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig31_HTML.jpg)

图 5-31

从 LGBM 模型生成输出值时的样本顺序相似性

在图 [5-31](#Fig31) 中，比值比是输出，从比值比中你可以得到目标类的概率值，如本章所示。此图显示了样本的相似性，然后进行了相应的排序。产生较高比值比的样本被置于 0-50 之间。之后，其余的样本产生了+3 到-9 的比值比，直到第 900 个记录。之后，赔率下降到一个非常负的范围。红线表示有助于提高预测优势比的特征值，蓝线表示有助于降低预测优势比的特征值。平均线对应-2.43 的值。如果你取整个数据集的比值比，然后计算平均值，得到的值是-2.43。

```
shap.summary_plot(shap_values, X)

```

![../images/506619_1_En_5_Chapter/506619_1_En_5_Fig32_HTML.jpg](../images/506619_1_En_5_Chapter/506619_1_En_5_Fig32_HTML.jpg)

图 5-32

汇总图

图 [5-32](#Fig32) 显示了汇总图。这是一个非常重要的问题。它通过对应于目标列的每个类的不同特征来提供平均 SHAP 值。此图中有趣的是，平均值平均分布在两个类别之间，与特征值无关。

## 结论

您已经介绍了几个用于分类场景和回归场景的基于 boosting 的模型。以类似的方式，可以训练 bagging 分类器，并且可以产生类似的图形作为输出。图形不会改变，但是当您从一个模型切换到另一个模型时，对值的解释会改变。以下是你应该注意的五件重要事情:

*   在回归场景中，您可以预测目标列，并且根据特性贡献，您可以看到预测结果是如何上下移动的。因此，如果您更改一个输入特性，您可以很容易地理解对目标输出的影响。

*   在分类方案中，您将对数比值比预测为一个连续变量，并根据要素输入值预测该值。您知道哪些要素是重要的，因此您可以根据输入要素的变化准确了解预测结果。

*   无论是装袋、推进还是堆叠，都可以生成类似的 PDP 框架、力图和汇总图，以理解模型的决策制定。但是，作为一个例子，PDP 图中显示的模式将会改变。

*   图形模式的变化是因为模型试图捕捉数据中的复杂模式(如果有的话)。随着数据复杂性的增加，模型变得更加复杂，从视觉上看可能难以解释。然而，从这些值得出的推论将保持不变。

*   因此，基于这一章的分析，你覆盖多少集合模型并不重要。更重要的是建立一个其他集合模型可以遵循的框架。