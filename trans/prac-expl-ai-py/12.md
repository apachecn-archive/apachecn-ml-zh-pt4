# 十四、通过识别预测不变性的模型不可知的解释

取自数学领域的不变术语解释说，如果我们干预解释变量，机器学习模型生成的预测保持不变，因为该模型是通过正式的因果关系生成的。因果机器学习模型和非因果机器学习模型是有区别的。因果模型显示了响应变量和解释变量之间的真实因果关系。非因果模态模型显示一种偶然关系；预测确实会改变很多次。理解因果模型和非因果模型之间的区别的唯一方法是通过解释。在本章中，您将使用一个名为 Alibi 的 Python 库来生成与模型无关的解释。

## 什么是模型不可知的？

术语*模型不可知*意味着模型独立。与模型类型无关的有效的机器学习模型的解释被称为模型不可知的。当我们解释机器学习模型的固有行为时，我们通常不假设机器学习模型本身所显示的行为的任何底层结构。在这一章中，你将使用模型不可知的解释来描述预测不变性。从人类的角度理解机器学习模型如何工作一直是一个挑战。人们总是对机器学习模型的行为感到好奇并提出问题。为了理解模型不可知的行为，人类总是认为他们可以对模型行为做出预测。

## 什么是主播？

复杂模型通常被认为是黑盒模型，因为很难理解为什么要做出预测。主播的概念无非是找出规则。这里的规则是指解释模型行为的高精度规则，考虑了局部和全局因素。换句话说，锚是锚定预测而不考虑其他特征值的 if/then/else 条件。

锚算法建立在与模型无关的解释之上。它有三个核心功能:

*   **覆盖率**:这意味着一个人可以多久改变一次框架来预测一个机器学习模型的行为。

*   精度:这意味着人类预测模型行为的准确性。

*   **工作**:这意味着解释模型行为或理解模型行为所产生的预测所需的前期工作。这两种情况都很难解释。

## 利用不在场证明进行解释

为了生成黑盒模型的锚点解释，您将使用 Python 库 Alibi。以下脚本显示了解释该概念所需的导入语句。您将使用一个随机森林分类器，它也是一个黑盒模型，因为有许多生成预测的树，这些预测被组合起来以产生最终输出。

```py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from alibi.explainers import AnchorTabular
from alibi.datasets import fetch_adult

```

Alibi 库有几个数据集，以生成可用于解释概念的示例。这里您将使用`adult`数据集；它也被称为人口普查收入数据集，可从 UCI 机器学习库中获得。这是一个二元分类模型。有几个特征可用于预测收入类别以及收入是否超过 5 万美元或更少。

```py
adult = fetch_adult()
adult.keys()

```

上面的脚本将数据带到 Jupyter 环境中。按键是`dict_keys(['data', 'target', 'feature_names', 'target_names', 'category_map'])`。

```py
data = adult.data
target = adult.target
feature_names = adult.feature_names
category_map = adult.category_map

```

角色类别`map`的功能是映射数据中出现的所有分类列或字符串列。

```py
from alibi.utils.data import gen_category_map

```

分类映射只是一个虚拟变量映射，因为不能使用分类变量或字符串变量进行计算。

```py
np.random.seed(0)
data_perm = np.random.permutation(np.c_[data, target])
data = data_perm[:,:-1]
target = data_perm[:,-1]

```

在训练模型之前，将数据集中可用的数据随机化，并将 33，000 条记录用于模型训练，2，560 条记录用于模型测试或验证。

```py
idx = 30000
X_train,Y_train = data[:idx,:], target[:idx]
X_test, Y_test = data[idx+1:,:], target[idx+1:]

X_train.shape,Y_train.shape
X_test.shape, Y_test.shape

ordinal_features = [x for x in range(len(feature_names)) if x not in list(category_map.keys())]
ordinal_features

```

选择序号特征后，可以应用从管道触发的序号特征转换器，如下所示:

```py
ordinal_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                                      ('scaler', StandardScaler())])
categorical_features = list(category_map.keys())
categorical_features

```

顺序特征是特征号 0、8、9 和 10，分类特征是 1、2、3、4、5、6、7 和 11。对于序数特征，应用标准的定标器。如果有缺失值，您将根据中间值进行估算。

```py
categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])
categorical_transformer
preprocessor = ColumnTransformer(transformers=[('num', ordinal_transformer, ordinal_features),
                                               ('cat', categorical_transformer, categorical_features)])

```

对于分类特征，在缺失值的情况下，应用中值插补技术，并为分类变量转换执行一次热编码。如果分类变量值未知，则忽略该值。

```py
Preprocessor
ColumnTransformer(transformers=[('num',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('scaler', StandardScaler())]),
                                 [0, 8, 9, 10]),
                                ('cat',
                                 Pipeline(steps=[('imputer',
                                                  SimpleImputer(strategy='median')),
                                                 ('onehot',
                                                  OneHotEncoder(handle_unknown='ignore'))]),
                                 [1, 2, 3, 4, 5, 6, 7, 11])])

```

预处理器模块现在可以进行模型训练了。预处理器拟合函数为模型训练转换数据集。用于分类的随机森林模型以 50 棵树作为估计器的数量来初始化。初始化后的对象存储在`clf`中。`clf fit`函数训练模型。

```py
preprocessor.fit(X_train)
np.random.seed(0)
clf = RandomForestClassifier(n_estimators=50)
clf.fit(preprocessor.transform(X_train), Y_train)

predict_fn = lambda x: clf.predict(preprocessor.transform(x))
print('Train accuracy: ', accuracy_score(Y_train, predict_fn(X_train)))
Train accuracy:  0.9655333333333334

print('Test accuracy: ', accuracy_score(Y_test, predict_fn(X_test)))
Test accuracy:  0.855859375

```

下一步，您将使用或使锚点解释器适合表格数据。板状锚的参数要求如表 [12-1](#Tab1) 所示。

表 12-1

锚表的参数说明

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

因素

 | 

说明

 |
| --- | --- |
| `Predictor` | 具有预测功能的模型对象，接受输入数据并生成输出数据 |
| `Feature_names` | 功能列表 |
| `Categorical_names` | 一种字典，其中键是功能列，值是功能的类别 |
| `OHE` | 无论分类变量是一个热编码(OHE)与否。如果不是 OHE，他们被认为有序数编码。 |
| `Seed` | 为了再现性 |

```py
explainer = AnchorTabular(predict_fn, feature_names, categorical_names=category_map, seed=12345)
explainer

explainer.explanations
explainer.fit(X_train, disc_perc=[25, 50, 75])
AnchorTabular(meta={
  'name': 'AnchorTabular',
  'type': ['blackbox'],
  'explanations': ['local'],
  'params': {'seed': 1, 'disc_perc': [25, 50, 75]}}
)
idx = 0
class_names = adult.target_names
print('Prediction: ', class_names[explainer.predictor(X_test[idx].reshape(1, -1))[0]])
Prediction:  <=50K

explanation = explainer.explain(X_test[idx], threshold=0.95)
explanation.name
print('Anchor: %s' % (' AND '.join(explanation.anchor)))
print('Precision: %.2f' % explanation.precision)
print('Coverage: %.2f' % explanation.coverage)
idx = 6
class_names = adult.target_names
print('Prediction: ', class_names[explainer.predictor(X_test[idx].reshape(1, -1))[0]])
Prediction:  >50K

```

`explainer.explain`对象解释了分类器对观察所做的预测。`explain`功能具有如下参数，如表 [12-2](#Tab2) 所示。

表 12-2

解释函数参数值

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

参数

 | 

说明

 |
| --- | --- |
| `Xtest` | 需要解释的新数据 |
| `Threshold` | 最小精度阈值 |
| `batch_size` | 用于取样的批量 |
| `coverage_samples` | 用于估计搜索结果覆盖率的样本数 |
| `beam_size` | 在新锚构造的每个步骤中延伸的锚的数量 |
| `stop_on_first` | 如果为真，波束搜索算法将返回满足概率约束的第一个锚点。 |
| `max_anchor_size` | 结果中的最大要素数 |

```py
explanation = explainer.explain(X_test[idx], threshold=0.95)
print('Anchor: %s' % (' AND '.join(explanation.anchor)))

Output : [Anchor: Capital Loss > 0.00 AND Relationship = Husband AND Marital Status = Married AND Age > 37.00 AND Race = White AND Sex = Male AND Country = United-States
]

print('Precision: %.2f' % explanation.precision)
print('Coverage: %.2f' % explanation.coverage)

```

在上面的例子中，模型解释的精度只有 72%，覆盖率只有 2%。可以得出结论，由于不平衡的数据集，覆盖率和精度较低。收入低于 5 万美元的人比收入高于 5 万美元的人多。您可以在文本分类数据集上应用类似的锚点概念。在本例中，您将使用情感分析数据集。

### 用于文本分类的锚文本

anchor 的`text`函数将帮助您获得文本分类数据集的 anchor 解释。它可以是垃圾邮件识别，也可以是情感分析。生成锚文本可以微调的参数如表 [12-3](#Tab3) 所示。

表 12-3

锚文本参数解释

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

参数

 | 

说明

 |
| --- | --- |
| `Predictor` | 通常是可以生成预测的经过训练的模型对象 |
| `sampling_strategy` | 扰动分布法，其中`unknown`用 UNKs 代替单词；`similarity`根据相似性得分对样本进行语料库嵌入；和`language_model`根据语言模型的输出分布进行采样 |
| `NLP` | 采样方法为`unknown`或`similarity`时的空间对象 |
| `language_model` | 变形金刚屏蔽语言模型 |

您将使用的采样策略是`unknown`和`similarity`，因为变压器方法超出了本书的范围。

```py
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
#import spacy
from alibi.explainers import AnchorText
from alibi.datasets import fetch_movie_sentiment
#from alibi.utils.download import spacy_model

```

上面的脚本从库中导入必要的库和函数。

```py
movies = fetch_movie_sentiment()
movies.keys()

```

上面的脚本加载带标签的数据用于情感分类。

```py
data = movies.data
labels = movies.target
target_names = movies.target_names

```

这包括电影数据、目标列(被标记为正面和负面)和特征名称(来自语料库的一组唯一的标记)。

```py
train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)
train, test, train_labels, test_labels

```

上面的脚本将数据分为训练集和测试集，下面的作为训练和验证数据集。

```py
train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)
train, val, train_labels, val_labels

train_labels = np.array(train_labels)
train_labels

test_labels = np.array(test_labels)
test_labels

val_labels = np.array(val_labels)
val_labels

vectorizer = CountVectorizer(min_df=1)
vectorizer.fit(train)

```

计数矢量器函数创建文档术语矩阵，其中文档表示为行，标记表示为特征。

```py
np.random.seed(0)
clf = LogisticRegression(solver='liblinear')
clf.fit(vectorizer.transform(train), train_labels)

```

使用逻辑回归是因为它是一个二元分类问题，求解器为 lib 线性。下面的`predict`函数获取经过训练的模型对象，并在给定测试数据集的情况下进行预测:

```py
predict_fn = lambda x: clf.predict(vectorizer.transform(x))

preds_train = predict_fn(train)
preds_train

preds_val = predict_fn(val)
preds_val

preds_test = predict_fn(test)
preds_test

print('Train accuracy', accuracy_score(train_labels, preds_train))

print('Validation accuracy', accuracy_score(val_labels, preds_val))

print('Test accuracy', accuracy_score(test_labels, preds_test))

```

训练、测试和验证准确性是一致的。

要生成锚点解释，您需要安装 spacy。您还需要安装 en_core_web_md 模块，这可以通过从命令行使用以下脚本来完成:

```py
python -m spacy download en_core_web_md

import spacy
model = 'en_core_web_md'
#spacy_model(model=model)
nlp = spacy.load(model)

```

`AnchorText`函数需要 spacy 模型对象，因此从 spacy 初始化 nlp 对象。

```py
class_names = movies.target_names

# select instance to be explained
text = data[40]
print("* Text: %s" % text)

# compute class prediction
pred = class_names[predict_fn([text])[0]]
alternative =  class_names[1 - predict_fn([text])[0]]
print("* Prediction: %s" % pred)

explainer = AnchorText(
    predictor=predict_fn,
    nlp=nlp
)

explanation = explainer.explain(text, threshold=0.95)

explainer

explanation

print('Anchor: %s' % (' AND '.join(explanation.anchor)))
print('Precision: %.2f' % explanation.precision)

Anchor: watchable AND bleak AND honest
Precision: 0.99

print('\nExamples where anchor applies and model predicts %s:' % pred)
print('\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))

print('\nExamples where anchor applies and model predicts %s:' % alternative)
print('\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))

```

锚应用和模型预测为负的例子:

努力的结果是 UNK UNK UNK 诚实，但 UNK UNK 如此惨淡 UNK UNK UNK 几乎看不下去。

## 用于图像分类的锚图像

与锚文本类似，影像分类数据集也有锚影像模型解释。在这里，您将使用时尚 MNIST 数据集。表 [12-4](#Tab4) 显示了需要说明的锚图像参数。

表 12-4

锚图像参数说明

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

参数

 | 

说明

 |
| --- | --- |
| `Predictor` | 图像分类模型对象 |
| `image_shape` | 要解释的图像的形状 |
| `segmentation_fn` | 任何内置分段函数字符串:“felzenszwalb”、“slic”、“quickshift”或自定义分段函数(可调用)，该函数返回带有每个超像素标签的图像蒙版 |
| `segmentation_kwargs` | 内置分段函数的关键字参数 |
| `images_background` | 要覆盖超像素的图像 |
| `Seed` | 为了再现结果 |

让我们看看下面生成锚点图像的脚本。

```py
import matplotlib
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, Input
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from alibi.explainers import AnchorImage

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape)

```

训练数据集包含 60，000 个样本图像，测试数据集包含 10，000 个样本图像。你将开发一个卷积神经网络模型(CNN)来预测图像类并解释所涉及的锚。目标类有 9 个标签；见表 [12-5](#Tab5) 。

表 12-5

目标类别描述

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

班级

 | 

描述

 |
| --- | --- |
| Zero | t 恤/上衣 |
| one | 裤子 |
| Two | 套衫 |
| three | 连衣裙 |
| four | 外套 |
| five | 凉鞋 |
| six | 衬衫 |
| seven | 运动鞋 |
| eight | 袋 |
| nine | 踝靴 |

```py
idx = 57
plt.imshow(x_train[idx]);

x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

```

通过将每个像素值除以最高像素值来完成特征归一化(255)。

```py
x_train = np.reshape(x_train, x_train.shape + (1,))
x_test = np.reshape(x_test, x_test.shape + (1,))
print('x_train shape:', x_train.shape, 'x_test shape:', x_test.shape)

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
print('y_train shape:', y_train.shape, 'y_test shape:', y_test.shape)

```

CNN 模型通常用于预测图像类别。它有一个卷积二维过滤器，以更好地识别像素值，然后是一个最大池层(有时是平均池，有时是最大池)，以便准备抽象层。丢弃层被设计成控制模型预测中可能出现的任何过拟合问题。

```py
def model():
    x_in = Input(shape=(28, 28, 1))
    x = Conv2D(filters=64, kernel_size=2, padding='same', activation='relu')(x_in)
    x = MaxPooling2D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    x = Conv2D(filters=32, kernel_size=2, padding='same', activation='relu')(x)
    x = MaxPooling2D(pool_size=2)(x)
    x = Dropout(0.3)(x)

    x = Flatten()(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    x_out = Dense(10, activation='softmax')(x)

    cnn = Model(inputs=x_in, outputs=x_out)
    cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return cnn

cnn = model()
cnn.summary()

cnn.fit(x_train, y_train, batch_size=64, epochs=3)

# Evaluate the model on test set
score = cnn.evaluate(x_test, y_test, verbose=0)
print('Test accuracy: ', score[1])

```

超像素在图像分类示例中提供了锚点解释。因此，为了识别核心特征，可以使用锚点图像。

```py
def superpixel(image, size=(4, 7)):
    segments = np.zeros([image.shape[0], image.shape[1]])
    row_idx, col_idx = np.where(segments == 0)
    for i, j in zip(row_idx, col_idx):
        segments[i, j] = int((image.shape[1]/size[1]) * (i//size[0]) + j//size[1])
    return segments

segments = superpixel(x_train[idx])
plt.imshow(segments);
predict_fn = lambda x: cnn.predict(x)
image_shape = x_train[idx].shape
explainer = AnchorImage(predict_fn, image_shape, segmentation_fn=superpixel)

i = 11
image = x_test[i]
plt.imshow(image[:,:,0]);

cnn.predict(image.reshape(1, 28, 28, 1)).argmax()

explanation = explainer.explain(image, threshold=.95, p_sample=.8, seed=0)

plt.imshow(explanation.anchor[:,:,0]);

explanation.meta

explanation.params

{'custom_segmentation': True,
 'segmentation_kwargs': None,
 'p_sample': 0.8,
 'seed': None,
 'image_shape': (28, 28, 1),
 'segmentation_fn': 'custom',
 'threshold': 0.95,
 'delta': 0.1,
 'tau': 0.15,
 'batch_size': 100,
 'coverage_samples': 10000,
 'beam_size': 1,
 'stop_on_first': False,
 'max_anchor_size': None,
 'min_samples_start': 100,
 'n_covered_ex': 10,
 'binary_cache_size': 10000,
 'cache_margin': 1000,
 'verbose': False,
 'verbose_every': 1,
 'kwargs': {'seed': 0}}

```

## 结论

在本章中，您查看了使用成人数据集的表格数据示例、用于情感分析的 IMDB 电影评论数据集以及用于图像分类示例的时尚 MNIST 数据集。对于表格数据、文本数据和图像数据，anchors 函数有很大的不同。在本章中，您使用 Alibi 中的解释器作为新库，学习了解释图像分类和预测、文本分类和预测以及图像分类的示例。