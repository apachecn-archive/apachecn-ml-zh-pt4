# 1.模型可解释性和可解释性

在本书中，我们将首先介绍模型可解释性和可解释性的基础知识，人工智能应用中的伦理考虑，以及人工智能模型生成的预测中的偏差。我们将涵盖人工智能模型在不同用例中生成预测的可靠性。然后我们将介绍解释人工智能中使用的线性模型的方法和系统，例如非线性模型和时间序列模型。接下来，我们将使用诸如 Lime、SHAP、Skater、ELI5 等框架探索最复杂的集合模型、可解释性和可解释性。然后我们将讨论非结构化数据和自然语言处理相关任务的模型可解释性。

## 建立框架

过去几年，机器学习和深度学习领域取得了巨大进展，在不同领域创建了人工智能(AI)解决方案，涵盖零售、银行、金融服务、保险、医疗保健、制造和基于物联网的行业。随着各种业务功能的快速数字化，人工智能是许多产品和解决方案的核心。人工智能是这些产品和解决方案的根本原因是，智能机器现在由学习、推理和适应能力驱动。经验匮乏。如果我们能够利用聪明人获得的丰富经验，并通过机器使用学习和推理层来反映这些经验，这将大大增加学习元素的倍数。凭借这些能力，今天的机器学习和深度学习模型能够在解决复杂的业务问题方面实现前所未有的性能水平，从而推动业务成果。

如果我们看看最近两年，有大量的 AutoML(自动机器学习)工具、框架、低代码和无代码工具(最少的人工干预)，这是人工智能系统已经达到的另一个复杂水平。这是解决方案的设计、交付和部署所需的几乎为零的人工干预的缩影。当决策完全由机器做出时，因为人类总是处于接收端，所以迫切需要了解机器是如何做出这些决策的。驱动人工智能系统的模型通常被称为黑盒模型。因此，为了解释人工智能模型做出的预测，需要模型的可解释性和可解释性。

## 人工智能

人工智能是指将一个系统设计为一个计算机程序，它可以在没有明确编程的情况下，代表人类就某项任务自动做出决策。图 [1-1](#Fig1) 解释了机器学习、深度学习、人工智能之间的关系。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig1_HTML.png](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig1_HTML.png)

图 1-1

ML、DL 和 AI 之间的关系

人工智能是一种使用计算机程序设计的系统，其中可以根据问题陈述得出智能推论。在获取智能的过程中，我们可能会使用机器学习算法或深度学习算法。机器学习算法是在使用输入和输出数据组合的优化过程中使用的简单数学函数。此外，这些函数可用于使用新输入来预测未知输出。对于结构化数据，我们可以使用机器学习算法，但当数据维度和数量增加时，如图像数据、音频数据、文本和视频数据，机器学习模型无法更好地执行，因此需要深度学习模型。专家系统被设计成基于规则的系统，有助于获得推论。当我们没有足够的训练数据来训练机器学习模型或深度学习模型时，这是必需的。总的来说，人工智能系统的构建需要结合专家系统、机器学习算法和深度学习算法来生成推理。

机器学习可以定义为一个系统，其中算法从与之前定义的一些任务相关的示例中学习，随着我们向系统提供越来越多的数据，学习性能会提高。任务可以被定义为受监督的，其中输出/结果是预先知道的；无人监管，事先不知道产出/结果；和强化，其中行动/结果总是由反馈层驱动，反馈可以是奖励或惩罚。就学习算法而言，它们可以分为线性算法、确定性算法、加法和乘法算法、基于树的算法、集成算法和基于图的算法。可以根据算法的选择来定义性能标准。解释人工智能模型的决定被称为可解释的人工智能(XAI)。

### 对 XAI 的需求

让我们来看看称人工智能模型为黑盒模型的原因。图 [1-2](#Fig2) 解释了经典建模场景，其中独立变量集通过函数传递，该函数是预先确定的，以产生输出。将产生的输出与真实输出进行比较，以评估该函数是否符合数据。如果函数不太适合，那么我们必须转换数据或者考虑使用另一个函数，以便它可以适合数据，但是实验是相当手动的，并且每次有数据刷新时，需要统计学家或建模者重新校准模型，并且再次查看数据是否适合模型。这就是为什么用于推理处理的预测模型生成的经典方式具有人类依赖性，并且总是服从多于一种的解释。有时，利益相关者很难信任该模型，因为许多专家提出的所有变体在某种意义上可能听起来不错，但没有普遍性。因此，如图 [1-2](#Fig2) 所示的模型开发的经典系统很难在人工智能系统的世界中实现，在那里每时每刻数据都在运行。它一直在变化。人类对校准的依赖是一个瓶颈；因此，需要一种使用动态算法动态生成推理的现代系统。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig2_HTML.jpg](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig2_HTML.jpg)

图 1-2

经典的模特训练系统

图 [1-2](#Fig2) 显示了经典的模型开发场景，其中模型可以通过一个方程来捕获，并且该方程易于解释，并且向任何人解释都很简单，但是基于公式的解释的存在在人工智能世界中正在发生变化。图 [1-3](#Fig3) 显示了探索利用输入产生输出的最佳可能功能的结构。这里没有将模型限制为特定函数(如线性或非线性)的约束。在这种结构中，学习通过多次迭代进行，并使用交叉验证方法来识别最佳模型。人工智能模型的挑战在于可解释性和可解释性，因为许多算法都很复杂，所以向每个人解释预测并不容易。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig3_HTML.jpg](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig3_HTML.jpg)

图 1-3

人工智能模型训练过程

随着计算机程序和算法的进步，开发者很难搜索不同种类的函数，例如线性和非线性函数，并且对这些函数的评估也变得极其困难。ML 或 DL 模型负责搜索适合训练数据的适当函数。图 [1-3](#Fig3) 解释了机器识别最终模型，该模型不仅在准确性方面，而且在生成预测的稳定性和可靠性方面提供了更好的性能。当输入和输出之间的函数关系被明确定义时，模糊性就会减少，预测也就透明了。然而，当我们的人工智能模型选择一个复杂的功能关系时，作为最终用户很难理解。因此，人工智能模型被认为是黑箱。在本书中，我们希望让黑盒模型变得可解释，这样人工智能解决方案将变得越来越可部署和可适应。

人工智能决策模型的日常执行需要透明、公正和道德。有许多目前缺乏解释力的场景:

*   有人申请信用卡，AI 模型拒绝了信用卡申请。传达申请被拒绝的原因以及申请人可以采取哪些纠正措施来改变他们的行为是很重要的。

*   在基于生活方式和生命参数的医疗诊断中，人工智能模型预测这个人是否会患糖尿病。在这里，如果人工智能模型预测这个人可能会患糖尿病，那么它还必须传达为什么以及未来患糖尿病的驱动因素是什么。

*   自动驾驶汽车识别道路上发现的物体，并做出明确的决定。他们还需要一个清晰的解释，来解释他们为什么做出这些决定。

还有许多其他的用例，其中支持模型输出的解释是至关重要的。人类倾向于不采纳他们无法解释或理解的东西。从而降低了他们对人工智能模型预测的信任度。我们使用人工智能模型是为了消除人类决策中的偏见；然而，如果结果不合理、不合法、不透明，决策将是危险的。另一方面，有人可能会说，如果我们不能解释和说明人工智能模型的决定，那么为什么要使用它们。原因是模型精度和性能。在模型的性能和模型的可解释性之间总会有一个权衡。图 [1-4](#Fig4) 解释了两者之间的权衡。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig4_HTML.jpg](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig4_HTML.jpg)

图 1-4

模型可解释性和性能(准确性)之间的权衡

在图 [1-4](#Fig4) 中，横轴表示模型的性能或精度，纵轴表示模型解释和说明。基于规则的系统被放置在性能不是最佳的位置；不过可解释性还是不错的。相比之下，基于深度学习的模型提供了卓越的性能和良好的准确性，但代价是可解释性和可解释性较差。

### 可解释性与可解释性

模型解释和可解释性是有区别的。解释是关于预言的意义。可解释性是为什么模型预测了一些事情，为什么有人应该相信这个模型。为了更好地理解这种差异，让我们看一个销售预测的真实例子，其中有助于预测的因素是广告费用、产品质量、制作广告的来源、广告的大小等。回归建模完成后，每个因素都有一个系数。这些系数可以解释为由于一个因素(如广告费用)的增量变化而导致的销售增量变化。但是，如果您预测下个月的销售额将是 20，000 美元，而历史上平均月销售额一直小于或等于 15，000 美元，这就需要一个解释。作为模型可解释性的一部分，我们需要以下内容:

*   模型的可解释性确保决策是自然的，并且在预测中没有偏见。

*   区分假因果关系和真因果关系，这有助于使预测变得透明

*   需要产生可解释的模型，而不损害学习经验和迭代的高性能

*   让决策者信任人工智能模型。

> ***“XAI 应用和产品将是 2021 年的新趋势，因为人工智能对可解释性、信任和道德的需求是响亮而明确的”——*****Pradeepta Mishra*****(来源:各种研究报告)***

XAI 一直在进行研究，试图向最终用户解释人工智能模型及其行为，以增加人工智能模型的采用。现在的问题是，谁是 XAI 的最终用户？

*   评估贷款申请、信贷请求等的信贷官员。如果他们理解这些决定，那么他们可以帮助教育客户纠正他们的行为。

*   评估自己的解决方案并确保模型得到改进的数据科学家。这是使用当前数据集可以制作的最佳模型吗？

*   需要在高层次上满足法规和合规性需求的高级经理

*   需要信任人工智能黑箱决策并寻找任何可以依赖的历史证据的业务主管

*   需要回答投诉和解释决策的客户支持主管

*   内部审计师和监管者，他们必须确保建立透明的数据驱动流程

XAI 的目标是实现以下目标:

*   **信任**:预测精度是数据质量、真实因果关系和选择适当算法的明确函数。然而，模型在预测过程中容易产生假阳性。如果模型产生大量的误报，那么最终用户将会对模型失去信任。因此，向最终用户传达对模型的信心是很重要的。

*   **关联**:ML 或 DL 模型学习基于各种特征之间的关联进行预测。这些关联可以是相关性或者仅仅是关联。无法解释的相关性是虚假的相关性，使得模型无法解释。因此，捕捉真实的相关性是很重要的。

*   **可靠性**:对模型的信心，模型在预测中的稳定性，模型的稳健性也很重要。这是必要的，以便在人工智能模型中有更多的可信度，并确保最终用户在模型预测中有足够的信心。如果这不可用，那么没有用户会信任模型。

*   **公平**:人工智能模型应该是公平的，符合伦理的。在进行预测时，他们不应该区分宗教、性别、阶级和种族。

*   **身份**:人工智能模型应该能够保持隐私考虑的完整性，而不暴露个人的身份。生成 XAI 时的隐私和身份管理非常重要。

### 可解释类型

机器学习的可解释性是模型可解释性的一个组成部分。模型解释有多种分类:

*   **内在解释**:简单模型属于这一类，如简单线性回归模型和基于决策树的模型，其中简单的 if/else 条件可以解释预测，这意味着 XAI 是模型本身固有的，不需要执行任何类型的后期分析。

*   **事后解释**:复杂模型，如非线性模型、基于系综树的模型、随机梯度增强的基于树的模型和堆叠模型，其中需要给予更大的关注以创建可解释性

*   **模型特定**:从一个特定类型的模型中可以得出一套解释，仅此而已。例如，线性回归模型不提供特征重要性。然而，有人可以用线性回归模型的系数作为代理。

*   **模型不可知**:这些解释有些可以通过查看一对训练输入数据和训练输出数据组合来解释。在本书中，我们将在后面的章节中探讨模型不可知的解释。

*   **局部解释**:这给出了一个关于单个预测的想法，这是对单个数据点的解释。举个例子，如果模型预测借款人可能违约，为什么会这样？这是当地的解释。

*   **全局解释**:这给出了对所有数据点的预测、整体模型行为等的全局理解。

*   **局部解释**:这解释了一组数据点的局部解释，而不是所有数据点。这与当地的解释不同。

*   **文本解释**:基于文本的解释包括数字部分以及传达模型中某个参数含义的语言。

*   视觉解释:视觉解释很好，但有时它们不够直观，不足以解释预测，因此非常需要伴随文字解释的视觉。

### 模型可解释性工具

有各种工具和框架可以用来从 ML 和 DL 模型中生成可解释性。开源 Python 库有一些优点和缺点。在本书的所有例子中，我们将混合使用 Python 库作为开源库和来自各种网站的公开可用数据集。这里是需要安装的工具和需要设置的环境。

### 世鹏科技电子

SHAP(SHapley Additive explaints)库是一种基于 Python 的统一方法，用于解释任何机器学习模型的输出。SHAP Python 库是基于带有本地解释的博弈论。博弈论方法是一种预测一个因素存在与否的方法。如果预期结果发生重大变化，那么该因素对目标变量非常重要。这种方法结合了以前的几种方法来解释机器学习模型生成的输出。SHAP 框架可用于不同类型的模型，基于时间序列的模型除外。参见图 [1-5](#Fig5) 。SHAP 图书馆可以用来理解这些模型。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig5_HTML.png](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig5_HTML.png)

图 1-5

Shapely 值和解释示例图像

为了安装 SHAP，可以使用以下方法:

*   `!Pip install shap`(摘自朱庇特笔记本)

*   `conda install -c conda-forge shap`(使用终端)

```py
!pip3 install shap

```

### 石灰

LIME 代表局部可解释的模型不可知解释。局部是指围绕模型预测的类的局部性的解释。分类器在局部周围的行为给出了关于预测的很好的理解。可解释意味着如果预测不能被人类解释，那就没有意义。因此，类预测需要是可解释的。模型不可知意味着系统和方法应该能够生成解释，而不是理解特定的模型类型。

诸如情感分析或任何其他文本分类的文本分类问题是，输入是作为文档的句子，输出是类。参见图 [1-6](#Fig6) 。当模型预测一个句子的积极情绪时，我们需要知道哪些单词使模型预测该类是积极的。这些词向量有时非常简单，比如单个词。有时它们很复杂，比如单词嵌入，在这种情况下，我们需要知道模型如何解释单词嵌入，以及它如何影响分类。在这些场景中，LIME 在理解机器学习和深度学习模型方面非常有用。LIME 是一个基于 Python 的库，可以用来展示它是如何工作的。可以按照以下步骤安装库:

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig6_HTML.png](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig6_HTML.png)

图 1-6

蘑菇分类的一个例子

```py
!pip install lime

```

### ELI5

ELI5 是一个基于 Python 的库，旨在用于可解释的人工智能管道，它允许我们使用统一的 API 来可视化和调试各种机器学习模型。它内置了对几种 ML 框架的支持，并提供了一种解释黑盒模型的方法。该库的目的是使各种黑盒模型的解释变得容易。参见图 [1-7](#Fig7) 。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig7_HTML.jpg](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig7_HTML.jpg)

图 1-7

ELI5 的示例图像

ELI5 中的图 [1-7](#Fig7) 显示了在收入分类用例中预测收入等级的因素的重要性，我们将在后续章节中研究这些因素。可以使用以下语法完成 ELI5 的 Python 安装:

```py
!pip install eli5

```

这需要更新许多基于 Python 的库，您可能需要等待一段时间。

### 滑冰者

Skater 是一个开源的统一框架，能够对所有形式的模型进行模型解释，以帮助我们建立一个可解释的机器学习系统，这通常是真实世界用例所需要的。Skater 支持算法在全局(基于完整数据集的推理)和局部(关于单个预测的推理)解开黑盒模型的已知结构。

Skater 通过提供根据需要推断和调试模型决策策略的能力来实现这一愿景，将“人带入循环”参见图 [1-8](#Fig8) 。要安装 Skater 库，我们可以使用以下命令:

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig8_HTML.png](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig8_HTML.png)

图 1-8

显示使用 SHAP 和溜冰者的 PDP 的示例图像

```py
!pip install skater

```

### Skope _ 规则

Skope-rules 旨在学习用于“界定”目标类的逻辑的、可解释的规则(即，以高精度检测该类的实例)。Skope-rules 是决策树的可解释性和随机森林的建模能力之间的折衷。参见图 [1-9](#Fig9) 。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig9_HTML.jpg](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig9_HTML.jpg)

图 1-9

skope-规则示例图像

上面提到的基于 Python 的库大多是开源的，可以在任何软件应用程序中免费使用和集成。然而，有许多基于企业的工具和框架可用，如 H2O.ai 的领域相对较新，因为在使模型解释变得容易方面的研究仍在进行中。工具和框架正走向前台，使行业可以在各种用例中应用该流程。

### ML 的 XAI 方法

机器学习模型中的透明度水平可以分为三组:算法的透明度，参数和超参数的分解，以及在类似情况下相同结果的可再现性。有些设计的 ML 模型是可解释的，有些需要一套其他程序来解释。图 [1-10](#Fig10) 解释了模型可解释性的方法。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig10_HTML.png](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig10_HTML.png)

图 1-10

XAI 的方法

模型可解释性有三种方法:

*   文本解释需要阐述数学公式的含义、模型参数的结果或模型定义的度量。可以基于预定义的模板来设计解释，其中故事线必须预先准备好，并且仅需要在模板中填充参数。有两种不同的方法来实现这一点:使用自然语言生成(NLG)方法，这需要收集文本并创建一个描述对象的句子，以及使用摘要生成进行解释。

*   可以使用自定义的图形和图表来提供直观的解释。对于最终用户来说，基于树的图表非常简单，一目了然。每个基于树的方法都有一组规则支持。如果这些规则能够以简单的 if/else 语句的形式展示给用户，那么它将更加强大。

*   基于实例的方法确保了我们可以通过绘制平行线来用常见的日常实例来解释模型。此外，一个常见的业务场景可以用来解释模型。

### XAI 兼容型号

让我们看看模型的当前状态，它们的内在本质，它们对 XAI 的兼容性如何，以及这些模型是否需要额外的框架来解释。

*   **线性模型**:线性回归模型或 logistic 回归模型，通过分析它们的系数值，很容易解释，它是一个数字。这些值非常容易解释。然而，如果我们将此扩展到正则化回归族，就变得很难解释了。通常我们更重视个人特征。我们不包含交互功能。如果我们包括相互作用，例如加法相互作用、乘法相互作用、基于二次多项式的相互作用或基于三次多项式的相互作用，那么模型的复杂性就会增加。数学结果需要一种更简单的解释方法。因此，在这些复杂的场景中有更多的东西需要解释。

*   **时间序列预测模型**:这些也是非常简单的模型，遵循一种可以通过参数方法很容易解释的回归场景。

*   **基于树的模型**:基于树的模型分析起来更简单，对人类来说解释起来也非常直观。然而，这些模型通常不能提供更好的准确性和性能。它们也缺乏鲁棒性，并且存在固有的偏性和过拟合问题。由于有如此多的缺点，这种解释对最终用户来说毫无意义。

*   **基于集合的模型**:有三种不同类型的集合模型:打包、推进和堆叠。这三种类型都缺乏可解释性。需要一个简化的描述来传达模型结果。特征重要性也需要简化。

*   **数学模型**:基于向量数学的支持向量机用于基于回归的任务和基于分类的任务。这些模型很难解释，因此模型简化是非常必要的。

*   **深度学习** **模型**:深度神经网络(DNN)模型通常有三个以上的隐层，才能符合深度的定义。除了深度学习模型的层之外，还有各种模型调整参数，例如权重、正则化类型、正则化强度、不同层的激活函数的类型、模型中使用的损失函数的类型以及包括学习速率和动量参数的优化算法。这在本质上非常复杂，需要一个简化的解释框架。

*   **卷积神经网络(CNN)** :这是另一种类型的神经网络模型，通常应用于对象检测和图像分类相关的任务。这被认为是一个完整的黑盒模型。有卷积层、最大/平均池层等等。如果有人问为什么模型把猫预测成狗，我们能解释是哪里出了问题吗？目前，答案是否定的。向最终用户解释这种模式需要大量的工作。

*   **递归神经网络(RNNs)** :基于递归神经网络的模型通常适用于文本分类和文本预测。还有一些变化，如长短期记忆网络(LSTM)和双向 lstm，解释起来非常复杂。我们一直需要更好的框架和方法来解释这些模型。

*   **基于规则的模型**:这些是超级简单的模型，因为我们只需要 if/else 条件就可以实现这些模型。

### XAI 会见负责任的艾

负责任的人工智能是一个框架，其中在各种软件应用程序、数字解决方案和产品中确保可解释性、透明度、道德和责任。人工智能的发展在不同领域迅速创造了许多机会，这些技术触及了普通人的生活，因此人工智能需要负责任，决策应该可以解释。

负责任的人工智能的七个核心支柱是可解释性的关键部分(图 [1-11](#Fig11) )。让我们来看看每一个支柱。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig11_HTML.png](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig11_HTML.png)

图 1-11

责任人工智能的核心支柱

*   公平:人工智能系统生成的预测不应该导致人们在演员、信仰、宗教、性别、政治派别、种族等方面受到歧视，因此需要更大程度的公平。

*   **伦理**:在追求构建智能系统的过程中，我们不应该忘记使用数据收集情报的伦理。

*   **透明**:模型预测和预测是如何产生的应该是透明的。

*   **隐私**:在开发 AI 系统时，应该保护个性化数据(PII，个性化的可识别信息)。

*   **安全**:智能系统要安全。

*   **责任**:在错误预测的情况下，人工智能模型应该能够承担修复问题的责任。

*   **安全**:当人工智能模型就自动驾驶汽车的导航、机器人牙科手术和数据驱动的医疗诊断做出决定时，任何不正确的预测都可能导致危险的后果。

许多组织正在准备在他们的解决方案中使用人工智能的指南和标准，以避免人工智能在未来产生意想不到的负面后果。就拿组织 a 来说吧，它用 AI 来预测销量。人工智能预测销售额将比平均水平高出 30%,因此企业储备了大量产品，并动员人力来支持销售。但是，如果实际销售额真的与历史平均销售额持平，那么增加库存和人力就是一种浪费。在这里，人工智能的预测是错误的。在模型可解释性的帮助下，这种情况可以被分析，并且可能模型可以被修正。

### XAI 评价

对于基于 Python 的库在互联网上生成的不同解释的评估，没有一个通用的标准。XAI 进程在评估解释时应遵循以下步骤:

*   每个阶层都应该有一个单独的解释。如果我们有一个数据集由于容量大而无法用于模型训练，我们通常会对该数据集进行采样。如果我们采用分层抽样的方法，那么每一层都应该有一个单独的解释。

*   **时间限制**:我们知道现实生活中的数据集非常庞大。即使我们继续进行分布式计算框架，由 XAI 库生成的解释通常也不会花太多时间。

*   **实例不变性**:如果数据点基于它们的属性是相同的，它们应该是同一组的一部分，因此它们应该传达相似的解释。

在各种人工智能项目和倡议中，当我们做出预测时，经常会出现一个问题，为什么有人会相信我们的模型。在预测分析或机器学习或深度学习中，在预测的内容和为什么预测之间存在权衡。如果预测符合人类的预期，那么就是好的。如果超出了人类的预期，那么我们需要知道模型为什么会做出这样的决定。如果是客户定位、数字营销或内容推荐等低风险场景，预测和预期偏差是可以的。然而，在高风险环境中，如临床试验或药物测试框架，预测和预期中的小偏差会产生很大的差异，并且会出现许多问题，如模型为什么会做出这样的预测。作为人类，我们认为自己比所有人都优越。人们很好奇，想知道这个模型是如何做出预测的，为什么不是人类。XAI 框架是识别机器学习训练过程中固有偏差的一个很好的工具。XAI 帮助我们调查偏见的原因，并找到偏见到底出现在哪里。

因为许多人不适合解释机器学习模型的结果，他们无法推理出模型的决定，所以他们不准备使用人工智能模型(图 [1-12](#Fig12) )。这本书试图推广 XAI 框架的概念，以调试机器学习和深度学习模型，以增加人工智能在行业中的采用。在高风险环境和用例中，有法规要求和审计要求来推理出模型的决策。这本书是有组织的，为监督回归，分类，无监督学习，聚类和分段相关的任务提供 XAI 框架的实际操作。此外，某些时间序列预测模型需要 XAI 来描述预测。此外，XAI 框架也可以用于非结构化文本分类相关的问题。单一的 XAI 框架并不适合所有类型的模型，因此我们将讨论各种类型的开源 Python 库以及在生成基于 XAI 的解释中的使用。

![../images/506619_1_En_1_Chapter/506619_1_En_1_Fig12_HTML.png](../images/506619_1_En_1_Chapter/506619_1_En_1_Fig12_HTML.png)

图 1-12

人工智能如何提高采用率

检查模型的公平性还需要使用预测结果模拟假设场景。我们也会谈到这一点。然后我们将讨论人工智能模型的反事实和对比解释。我们将涵盖深度学习模型、基于规则的专家系统的模型可解释性，以及使用各种 XAI 框架对预测不变性和计算机视觉任务的模型不可知解释。

模型的可解释性和可解释性是这本书的重点。有一些数学公式和方法通常用于解释人工智能模型做出的决定。向读者提供了软件库方法、类、框架和函数，以及如何使用它们来实现模型的可解释性、透明性、可靠性、伦理、偏见和可解释性。如果人类能够理解人工智能模型做出决定背后的原因，它将赋予用户更大的权力来做出修改和建议。

## 结论

模型的可解释性和可解释性是所有使用人工智能预测事物的过程所需要的，原因是我们需要知道预测背后的原因。在本章中，您学习了以下内容:

*   模型可解释性和可解释性基础

*   人工智能应用中的伦理考虑和人工智能模型产生的预测偏差

*   人工智能模型在不同使用情况下生成预测的可靠性

*   用于解释人工智能中使用的线性模型、非线性模型和人工智能中使用的时间序列模型的方法和系统

*   最复杂的集合模型、可解释性和可解释性，使用诸如 Lime、SHAP、Skater、ELI5 等框架

*   非结构化数据和自然语言处理相关任务的模型可解释性