# 8.人工智能中的伦理

伦理和 AI 有什么关系？为什么要在技术书籍中包含这个主题？这可能是你开始阅读本章时想到的两个问题。随着计算机开始做出以前可能由人类做出的决定，可能会出现许多问题。结果是如何确定的？它是基于一个包括来自许多不同群体的代表性样本的样本集吗？关于数据集的选择，是否有伦理上的考虑？用于选择组的逻辑是什么？只是你数据的 70%的随机样本？这些数据反映了总体人口，还是包含了不反映你周围世界人口的人口统计偏差？像不注意用于训练你的解决方案的数据这样的小事会对依赖你的解决方案结果的人产生重大影响。

在本章中，我们将了解在创建、测试和评估算法时需要考虑哪些因素，以确保您不会偏向某一组而不是另一组。我们将探讨以下主题:

*   开发人工智能时要考虑的事项

*   从伦理上选择正确的测试数据

*   在开发过程中加入偏差测试

*   在模型发布后监控持续的学习

*   提出问题时解决问题

## 为什么人工智能中的伦理是一个问题？

人工智能中的伦理不仅仅是一个技术问题。人工智能的问题已经蔓延到新闻网站的头版。网飞是第一批遇到人工智能问题的公司之一。2010 年，网飞发起了一项竞赛，奖金为 100 万美元，奖励他创造了一种算法，可以根据客户的观看历史更好地推荐电影。这是一个非常成功的竞赛，因为它为提供竞赛产生了大量的免费宣传，并且开发的一些代码被用于创建 Apache Spark。网飞希望再次举办比赛，当它做到了，美国美国联邦贸易委员会参与进来。为什么？因为德克萨斯大学的两名研究人员表明，应该匿名的数据可以相对容易地去匿名化。一群网飞用户以侵犯隐私为由提起了集体诉讼。比赛停止了，网飞以 900 万美元了结诉讼。诉讼认为，这些数据应该受到网飞隐私政策的保护。

### 人工智能伦理问题

用于分析的数据会受到很多道德挑战。由于对数据进行去匿名化比以往任何时候都容易，因此出现了许多关于表面上应该可以使用的数据集的问题。即使数据在某种程度上是公开的，比如 2015 年从 OkCupid 网站收集的数据，也已经赢得了诉讼。这些数据是由一些丹麦研究人员使用屏幕抓取工具从网站上提取的。他们对关于他们使用数据的强烈抗议感到非常惊讶，因为任何拥有 OkCupid ID 的人都可以获得这些数据。人们再次确定，对数据进行解密并不难，这样就有可能确定提供数据的人的姓名。OkCupid 抱怨说，使用这些数据侵犯了他们的版权，这些数据被从互联网上删除，此前数千名研究人员已经对这些数据进行了自己的分析。

被评估的人可能会注意到偏见，正如 2019 年 11 月高盛的苹果信用卡一样。高盛被指控存在偏见，因为史蒂文·沃兹尼亚克等知名男性客户获得的信贷额度高于其女性配偶。对于那些提交联合纳税申报表并且已经结婚多年的顾客来说，这似乎是不正确的。苹果将其归咎于用于授予信用的算法。监管机构宣布他们将调查高盛。最初，高盛否认他们在申请过程中知道性别，这一声明遭到了监管机构和社交媒体账户的质疑。监管者要求知道所使用的标准。高盛解释说，他们使用个人信用评分和个人债务。当被要求解释算法时，高盛表示他们无法解释这一决定。参议员伊丽莎白·沃伦呼吁，如果算法无法解释，就应该取消这张卡。这种偏见问题不仅限于美国。

这些只是数据问题中三个比较著名的例子。诸如此类的问题引发了对数据使用和存储的立法变革，包括欧洲的通用数据保护条例(GDPR ),该条例规定了如何存储和使用数据。这一章后面会有更多的介绍。

对于用于机器学习的数据集，已经发现了对结果有不利影响的其他问题。贷款承销一直充满了基于其数据集的问题。将种族或性别作为评估信用或信用成本的贷款算法的一部分是非法的，但监管者如何确保法律得到遵守？为了规避这些法律，代理歧视被使用。代理歧视是用一种元素代替种族或性别。例如，如果一个组织希望将性别作为一个歧视因素，而不是询问性别，则可以通过查看除臭剂的选择并结合在亚马逊上购买的书籍来确定性别，以此作为实际询问客户性别的替代。如果使用代理歧视，人工智能可以评估一个人观看的电视节目的类型。从统计学上讲，您可以通过选择的通道类型来确定，例如，Lifetime 和 BET，它们用作种族或性别的替代。令人怀疑的是，在法律系统评估时，这种替代是否会被视为不分种族或性别。

### 政府管制与人工智能分析

熟悉不同行业人工智能分析的法律问题非常重要。例如，虽然使用性别来确定贷款的核保标准是非法的，但在美国的大多数州，评估性别是允许的。性别是向新司机收取费用的一个主要考虑因素，没有法律理由反对使用这一因素。美国一些州不同意这种做法，已经修改了法律。加利福尼亚州最近宣布，将性别作为确定保险费率的标准是非法的，这意味着用于保险定价的算法已经发生了变化。

AI 的法律责任也是一个问题。在某些情况下，软件开发者是有责任的；在其他情况下，雇佣他们的机构。根据 GDPR，如果使用了机器学习算法，一个人有权得到解释。这并不意味着使用 Keras 或其他深度学习工具的模型的作者必须解释算法如何工作，而是什么因素促成了决策。创建模型的人将需要提供对特征及其在决策过程中的权重的解释。从过程的角度来看，人们还需要请求人工干预的权利，这意味着人工智能过程需要能够被人审查。

GDPR 还要求必须公平处理数据，以便数据控制者可以采取措施防止对个人的歧视性影响。为了遵守法律，需要消除数据偏差。如果您的数据将在 GDPR 覆盖的国家使用，您必须能够为算法的使用提供一个人类可读的解释。出于这个原因，可以采用其他类型的人工智能，如生成式对抗网络(GANs)或联合学习。

GAN 减少了对训练数据的需求。其主要思想是在输出数据的帮助下生成输入数据。基本上，使用这种方法，我们获取输入，并试图找出输出会是什么样子。为了实现这一点，我们需要训练两个神经网络。一个是发生器，一个是鉴别器。

生成器学习如何将数据放在一起以生成类似输出的图像。鉴别器学习如何区分真实数据和生成器产生的数据。这里的问题是，GANs 仍然需要大量的数据来进行适当的训练。这种方法不能消除对训练数据的需要；它只是允许我们减少初始数据量，生成大量相似的增广数据。但如果我们使用少量的初始数据集，我们最终可能会得到一个有偏见的人工智能模型。因此，生成对立神经网络并没有完全解决这些问题，尽管它们允许我们减少初始测试数据集的数量。

联合学习是在人工智能开发中减少数据需求的另一种方法。值得注意的是，它根本不需要收集数据。在联合学习中，个人数据不会离开存储它的系统。它从未被收集或上传到人工智能的电脑上。通过联邦学习，人工智能模型在每个系统上用本地数据进行本地训练。稍后，经过训练的模型作为更新与主模型合并。但问题是，本地训练的人工智能模型是有限的，因为它是个性化的。即使没有数据离开设备，该模型仍然在很大程度上基于个人数据。不幸的是，这违背了 GDPR 的透明原则，这意味着遵循法律条文的联合学习可能不是一个好的选择。

### 人工智能中的偏差

偏见，韦氏词典将其定义为“气质或观点的倾向；特别是:在人工智能算法的创造中，可以以多种不同的方式发现“个人的、有时不合理的判断”。一个比较常见的方法是选择偏差。算法中使用的数据元素的选择或算法基于数据做出的训练决策可以包括用于强化历史或定型的元素。评估一个人在创建实验时选择什么标准可以确定算法将学习什么样的偏差。另一种可能将偏差带入算法的方式是通过观察者偏差。如果您希望获得特定的结果，您可以更改数据、算法或超参数的选择以获得该结果。另一种偏差方法可能是生存偏差。遭受生存偏差的数据是先前被过滤的数据，然后丢失了一些数据元素。

统计偏差的一个例子是一项关于猫从建筑物上摔下来的存活率的研究。1987 年，纽约市动物医疗中心的兽医对从高楼上摔下来的猫做了一项研究。这项研究的结论是，从 7-32 层楼掉下来的猫比从 2-6 层楼掉下来的猫更不容易死亡。这项研究在网上被广泛引用，从中得出了许多结论。但是在你把一只猫从高楼上扔下去之前，考虑一下这个问题:这项研究中使用的数据存在统计偏差。1997 年，一份名为《真实的毒品》的联合报纸专栏对这些数据进行了更彻底的研究。

他们指出，如果一只猫从八层楼上掉下来死了，它不会被带到兽医那里。这意味着死去的动物不属于这项研究的一部分，极少数从高处跌落的猫实际上属于这项研究的一部分。更多从较短距离摔下的猫被带到兽医那里，并被纳入这项研究，这就是为什么它们显示出较高的死亡率。必须注意确保数据的过滤方式不再代表整个集团。

### 公平与准确

将偏差引入算法选择的原因是多次添加标准可以提高准确性。当这种情况发生时，也可能引入种族或性别偏见。通过使用代理歧视，贷款承销商被发现让非裔美国人更难获得低成本贷款。考虑到通过社交媒体账户、网络搜索和信用卡购买可获得的大量数据，这为机构提供了评估许多不同数据元素以创建数字复合档案的能力。这种概况可以用来选择某一种借款人，而排除其他人的基础上的标准，对待类似情况的人不同。这可能以道德选择为代价来提高准确性。如果某人来自历史上违约率较高的群体，此人可能被确定为高风险，即使此人的个人信息不会表明这一决定。

### 倾斜样本会产生复合问题

有时，对给定算法的预测和决策会影响环境。做出的决定证实了预测，而给出的例子却很少与假设相矛盾。这种分析被证明是一个自我实现的预言的一个例子是按地区来管理。如果对有更多警察记录的人的位置进行地理分析，并且机器学习算法确定该区域有更多犯罪，那么当当地警方使用这些信息向该区域派遣更多警察时会发生什么？更多的警察意味着有更大的机会被捕，因此算法将被视为正确的。如果有一个地区有相同数量的人有警察记录，而警察没有被派往该地区，该地区的犯罪率会上升吗？不。只是和警察联系的可能性比较低。对数据做出的决定对结果的解释有多大影响？这是选择数据时需要考虑的另一个方面。

### 分析道德问题的数据

选择数据时，有许多不同的问题需要评估。邮政编码、社交媒体档案或网页浏览历史等推断数据可以创建反映历史不平等的组。需要对算法进行分析，以确保不会得出错误的结论。数据从大数据集中选取，可以根据在线活动或行为排除。根据这些项目结合邮政编码对收入进行推断，这可能不正确。这会导致基于种族、性别、年龄、能力和宗教对个人或群体的歧视性对待。哈佛大学的一名研究人员发现，这些信息被用于为黑人兄弟会制作高息信用卡和购买逮捕记录的微定向广告，而忽略了这个群体与白人群体有着相似背景的事实。

历史偏好也可能导致对当前数据的道德决策。2015 年，亚马逊基于 10 年来从员工那里收集的数据，创建了一个招聘工具。该算法应该对简历进行排序，以确定最合适的简历。他们在评估该工具时确定的是，它系统地排除了女性，因为在训练中输入算法的数据包括非常少的女性。包含“女子高尔夫球队”等内容的简历被系统地删除了。该算法选择的是延续公司性别组合的简历，而不是评估技能。该项目在 2018 年被放弃，当时亚马逊确定没有办法消除算法决策中的所有偏见。人工智能在简历选择中的使用仍在继续，因为高盛和 LinkedIn 仍在候选人选择过程中使用人工智能，但 state 人工智能并不能取代招聘和简历选择过程中的人类互动。

### 评估训练数据

查看培训数据时，您需要评估的第一个因素是数据的充分性。您是否有足够的数据来评估特定目的的任务？您的数据是否包含足够多的不同模式，以正确代表将应用机器学习模型的更大社区？这可能很难评估，因为人口的某些部分可能更容易收集数据，因此数据的混合并不能反映整个群体的人口统计数据。如果您正在评估人员，您将需要查看相关地区的人口普查或类似数据，以确保您拥有不同群体的代表性样本。

有四个因素通常被用来评估培训数据的偏差:人口均等、不同的影响、平等的机会和均等的几率。这些因素均用于评估偏倚。

### 省略可变偏差

有时不包括变量会导致有偏差的结果，最常见的是在进行回归分析时。有时变量被遗漏了，因为它很难收集或者根本没有被考虑。

1999 年发表在《自然》杂志上的一项研究确定，让两岁或更小的孩子房间里亮着灯，会使孩子更有可能近视。俄亥俄州立大学的研究人员重复了这项研究，发现灯光和近视之间没有联系。他们确实确定，更有可能在孩子的房间里不关灯的父母是那些近视的人，并且近视在父母和孩子之间有很强的遗传联系。第一项研究没有将父母的近视和错误的灯光关联作为原因，而不是遗传因素。这项关于视觉的研究说明了遗漏一个重要的数据元素可能带来的偏差。遗漏的变量偏差会导致没有因果关系的相关性。

### 相关性和因果关系

大脑经常寻找模式，不管这些模式是否存在于我们面前的样本集之外，因为这可能是一件轶事。这种模式可能比最初看起来更复杂。例如，只有当 C 发生时，A 才能导致 B。或者也许 A 和 B 是由 d 引起的，或者因果关系可能是更复杂的东西，例如 A 只在其他事件的长链反应的情况下引起 B，而你只目睹了最后一部分。为了检验因果关系，您需要运行控制其他变量的实验，然后测量差异。例如，您可以测试相反的值是否成立。例如，假设您正在关注客户流失，并注意到位置和客户转向之间的相关性。如果你能证明位置对顾客是否离开有影响，你也应该能证明位置让顾客留下来。证明双方都能确定结果是因果关系还是相关关系。

### 认知偏差

人们利用认知偏差来快速做出决定，例如，在加油站吃食物可能对你的健康有害。这种偏见可能是通过目睹许多便利店的热狗永远坐在滚轴上而建立起来的。也许你记得读过一篇文章，讲的是美国西部某处一名男子在吃了便利店的玉米片后死亡。如果你住在美国东北部，你可能会有不同的看法，因为那里有一家连锁便利店，非常好。甚至有加油站的食物出现在烹饪节目中。如果评估了所有的数据，随着样本集的扩大，也许原来的偏差会被证明是错位的。也许你还记得那篇关于一个人在吃了玉米片后死去的文章，这有助于增强你对他们食物质量的信心。如果是这样的话，你可能患有确认偏见。

有几种不同的认知偏差。除了确认偏差，还有后见之明偏差或基本归因错误。后见之明偏差让我们相信我们可以从未来预测过去的结果。这使得正确评估一个决策变得困难，因为结果才是关注的焦点，而不是达成决策的过程。当人们寻找能证明他们预期的结果的信息，而其他不能提供该结果的结果被丢弃时，就会产生确认偏差。例如，一家保险公司正试图弄清楚拥有大量保险附加条款的投保人是否对价格上涨不太敏感，因为他们不太可能离开。保险主管非常确定这个假设是正确的，并让数据科学家知道这个结果实际上是有保证的。包含表明这是弱相关性的数据和算法的初始模型随后被丢弃和调整，使得这种关系更加普遍。

## 避免数据偏差的工具

当任何人工智能模型中存在偏见时，第一步是解释算法的公平性。一个人如何确定人工智能系统有多公平？用于测试和训练算法的数据是否存在偏差？当应用于不同的群体时，预测会有很大的不同吗？

### 费尔勒恩

微软在 2019 年创建了开源工具包 Fairlearn，以评估算法的公平程度，并减轻算法中识别的因素。该分析在 Python 包中执行，可用于评估系统的公平性并缓解公平性问题。Fairlearn 通过评估分配损害和服务质量损害来确定公平性。分配伤害被定义为人工智能系统在招聘、借贷或学校招生等领域扣留机会或资源的情况。服务质量危害意味着衡量系统是否对不同的人工作良好，即使在决策过程中没有机会产生。例如，语音识别对女声有效，但对男声无效。这就要求应用组公平原则，这就要求数据科学家能够识别出可以在数据中表示的用户组。

#### Fairlearn 仪表板

Fairlearn 包含一个仪表板，其中包含不同的元素，用于确定绩效和公平性之间的权衡。可视化可以帮助使用许多不同的度量来识别模型对数据组的影响。仪表板是一个 Jupyter 笔记本小部件，旨在评估预测如何影响不同的群体。以下是使用 v.0.5.0 版创建仪表板的 Python 代码:

```py
from fairlearn.widget import FairlearnDashboard

FairlearnDashboard(sensitive_features=A_test,
                   sensitive_feature_names=['Race', 'Age'],
                   y_true=Y_test.tolist(),
                   y_pred=[y_pred.tolist()])

```

变量 A_test 用于评估数据的敏感值，敏感特征在名称中列出。Y_true 包含基本事实标注，y_pred 包含标注预测。仪表板引导用户完成评估设置，其中敏感特征将被评估，性能度量用于评估模型性能以及组之间的任何差异。例如，如果您的数据包含种族和性别元素，系统会提示您选择不同的组进行评估。接下来，系统会提示您如何衡量绩效。完成此设置后，您将看到以下屏幕，以评估算法性能的差异。图 [8-1](#Fig1) 显示了 Fairlearn 创建的仪表板，以显示数据的潜在问题。

![../images/506672_1_En_8_Chapter/506672_1_En_8_Fig1_HTML.jpg](../images/506672_1_En_8_Chapter/506672_1_En_8_Fig1_HTML.jpg)

图 8-1

Fairlearn 绩效差异仪表板

如图 [8-2](#Fig2) 所示，还有一个额外的仪表板显示不同识别组之间预测结果的差异。

![../images/506672_1_En_8_Chapter/506672_1_En_8_Fig2_HTML.jpg](../images/506672_1_En_8_Chapter/506672_1_En_8_Fig2_HTML.jpg)

图 8-2

Fairlearn 预测差异仪表板

有两种类型的缓解算法用于缓解公平性，它们被设计成与二进制分类和回归一起工作。尚不支持其他机器学习算法类型。图 [8-3](#Fig3) 所示的模型仪表板提供了比较多个模型结果的能力，以确定模型选择的最佳选择。

![../images/506672_1_En_8_Chapter/506672_1_En_8_Fig3_HTML.png](../images/506672_1_En_8_Chapter/506672_1_En_8_Fig3_HTML.png)

图 8-3

Fairlearn 模型比较可视化

#### 减轻

一旦模型被评估，Fairlearn 提供了一些不同的方法来解决二元分类和回归算法的差异。提供包装器来实现监督学习中公平分类和机会均等的归约方法。缓解算法是 Scikit-Learn 之后的模型，因为 Fairlearn 实现 fit 模型来训练模型，实现 predict 模型来进行预测。有三种算法用于减轻不公平性，指数梯度、网格搜索和阈值优化器。这些算法使用包装器方法来拟合所提供的分类器，有助于减少选择率的差异。

### 解释性语言

另一个重要的解决方案是 InterpretML，它是一个工具和一组交互式仪表板，使用各种技术在推理中提供模型解释。对于不同类型的模型，InterpretML 帮助从业者更好地理解在确定模型输出时最重要的特征，执行“假设”分析，并探索数据中的趋势。微软还宣布，它将向该工具集添加一个新的用户界面，该界面配备了一套可解释性的可视化工具，支持基于文本的分类和反事实示例分析。

InterpretML 是一个开源的 Python 工具包，用于训练可解释的玻璃盒子模型和解释黑盒系统。InterpretML 帮助您理解模型的全局行为或理解单个预测背后的原因。它包括一个内置的仪表板，提供关于数据集、模型性能和模型解释的可视化信息。

InterpretML 工具包可以帮助提供关于机器学习的答案，包括

*   *模型调试*——为什么我的模型会犯这个错误？

*   *检测公平问题*–我的模型有歧视吗？

*   *人机合作*–我如何理解和信任模型的决定？

*   *法规合规性*–我的模型是否符合法律要求？

InterpretML 使用一个可解释的增强机器(EBM)来评估模型，EBM 是微软研究院开发的一个可解释的模型。它使用现代机器学习技术，如装袋、梯度推进和自动交互检测，为传统 gam(广义加性模型)注入新的活力。这使得 EBMs 与随机森林和梯度增强树等最先进的技术一样准确。然而，与这些黑盒模型不同，EBM 产生无损解释，并且可由领域专家编辑。

InterpretML 提供了一个仪表板，帮助用户了解不同数据子集的模型性能，探索模型错误，并访问数据集统计数据和分布。它包括使用各种不同方法的摘要说明以及整体和个别说明，并能够通过假设分析来执行特征扰动，假设分析探索模型预测如何随着特征扰动而变化。图 [8-4](#Fig4) 显示了为分析模型性能而创建的仪表板示例。

![../images/506672_1_En_8_Chapter/506672_1_En_8_Fig4_HTML.jpg](../images/506672_1_En_8_Chapter/506672_1_En_8_Fig4_HTML.jpg)

图 8-4

解释仪表板

InterpretML 通过一个名为 Interpret-Community 的实验性存储库方法进一步扩展。

#### 解释-社区

积极整合创新的实验可解释性技术，并允许研究人员和数据科学家进一步扩展。

应用优化，使得在真实世界数据集上大规模运行可解释性技术成为可能。

提供改进，例如“反转特征工程管道”的能力，以根据原始特征而不是工程特征提供模型洞察力。

提供交互式和探索性可视化，使数据科学家能够获得对其数据的有意义的洞察。

#### 差异隐私

差分隐私是一类算法，有助于对敏感的个人数据进行计算和统计分析，同时确保个人隐私不受损害。微软推出了 WhiteNoise，这是一个开源算法库，可以对私人敏感数据进行机器学习。差异隐私是一种公开共享数据集信息的系统，通过描述数据集中的群体模式，同时保留数据集中的个人信息。差分隐私背后的思想是，如果在数据库中进行任意单个替换的影响足够小，则查询结果不能用于推断关于任何单个个体的更多信息，因此提供了隐私。描述差异隐私的另一种方式是对用于发布有关统计数据库的聚合信息的算法的约束，这限制了其信息在数据库中的私人记录的公开。例如，一些政府机构使用不同的私有算法来发布人口统计信息或其他统计数据，同时确保调查答复的保密性；一些公司使用不同的私有算法来收集有关用户行为的信息，同时控制甚至对内部分析师可见的内容。

差分隐私是一个正式的数学框架，用于在分析或发布统计数据时保证隐私保护。最近从理论计算机科学文献中出现，差分隐私现在处于在各种学术、工业和政府设置中实现和使用的初始阶段。使用直观的插图和有限的数学形式，本文档为非技术从业者提供了差分隐私的介绍，随着差分隐私的使用越来越广泛，他们越来越多地承担关于差分隐私的决策任务。特别是，本文中的示例说明了社会科学家可以如何概念化差异隐私提供的保证，这些保证与他们在管理研究对象的个人数据时做出的决定有关，并告知他们将获得的隐私保护。

差分隐私使用两个步骤来实现隐私优势。首先，在每个结果中加入少量的统计噪声，以掩盖单个数据点的影响。噪音足够大，足以保护个人隐私，但也足够小，不会对分析师和研究人员提取的答案的准确性产生实质性影响。接下来，计算从每个查询中泄露的信息量，并从总体隐私损失预算中扣除，这将在个人隐私可能受到损害时停止额外的查询。这可以被认为是一个内置的关闭开关，防止系统在可能开始危及某人隐私时显示数据。

#### 部署差异隐私

自最初发表以来，差分隐私一直是学术界一个充满活力的创新领域。然而，众所周知，它很难在现实世界中成功应用。在过去的几年中，差分隐私社区已经能够推动这一界限，现在在生产中有许多差分隐私的例子。

#### 差分隐私平台

该项目旨在将研究社区的理论解决方案与从现实世界部署中获得的实践经验相结合，以使差分隐私广泛可用。该系统添加噪声以掩盖任何个体数据主体的贡献，从而提供隐私。图 [8-5](#Fig5) 显示了差分隐私的不同元素。

![../images/506672_1_En_8_Chapter/506672_1_En_8_Fig5_HTML.jpg](../images/506672_1_En_8_Chapter/506672_1_En_8_Fig5_HTML.jpg)

图 8-5

差异隐私

它被设计为一个组件集合，可以灵活地配置，使开发人员能够使用适合其环境的正确组合。核心包括一个基于成熟的差分隐私研究实现的差分隐私算法和机制的可插入开源库。该库提供了一个快速、内存安全的本机运行时。此外，还有用于定义分析的 API 和用于评估该分析和组成数据集上的总隐私损失的验证器。Rust 内置了运行时和验证器，同时提供 Python 支持。

## 摘要

在这一章中，我们回顾了数据分析的几个不同问题，并超越人工智能的技术要素，探索如何使用结果。我们回顾了在纳入用户不同意分析的数据时出现的一些伦理问题。我们观察了不完整数据集导致错误结论的研究结果。在检查了一些问题之后，我们通过检查如何使用 Fairlearn 和 InterpretML 以及如何使用它们来改进分析，来查看可以用来解决机器学习中使用的数据的一些问题的技术。