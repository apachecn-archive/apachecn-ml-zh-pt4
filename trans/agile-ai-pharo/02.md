# 2.人工神经元

在前一章中，我们看到了感知器是如何工作的，以及一个简单的学习算法是如何实现的。然而，感知器有一些严重的局限性，这将促使我们制定一个更强大的人工神经元，称为 sigmoid 神经元。

本章使用 Roassal 来绘制值。如前一章所见，您可以通过在操场上执行以下脚本将 Roassal 加载到 Pharo 中:

```py
Metacello new
        baseline: 'Roassal2';
        repository: 'github://ObjectProfile/Roassal2/src';
        load.

```

关于 Roassal 的完整描述可以在*敏捷可视化* ( [`http://agilevisualization.com`](http://agilevisualization.com) )一书中找到。

## 2.1 感知器的限制

感知器作为一个独立的小机器工作得很好。我们了解到，我们可以组成一些感知器来表达复杂的行为，如数字比较器。我们还了解到，单个感知器可以学习一个简单的行为。然而，组合感知器有两个主要限制:

*   *只有* `0` *或* `1` *作为输出*:一个感知器只能有两个不同的输出值`0`或`1`的事实，严重限制了它能解决的问题种类。特别是当一些感知机被链式连接时，使用二进制值会显著减少我们生活的空间。不是所有的事情都可以简化为一组`0`和`1`而不会导致感知器的爆炸。

*   一连串的感知器无法学习:我们已经看到了如何组合感知器，以及单个感知器如何学习。但是，感知器的组合也能学习吗？答案是否定的。这是只有两个输出值的另一个结果。最常见的学习算法的基本属性是表达平滑学习曲线的能力，这不能用两个不同的值来表达。只有两个不同的输出值，我们如何判断一个感知器学习得好、差还是根本不学习？

我们已经写了`z = w.x + b`，其中`w`是权重向量，`b`是偏差向量，`x`是输入向量。我们说过如果`z > 0`感知器的输出是`1`，否则是`0`。感知器公式的一个重要问题是`z`的小变化会产生输出的大变化——输出可以从`0`到`1`，或者从`1`到`0`。

通常在神经网络中使用的算法需要一个非常重要的特性:一个`z` *的小变化必定*产生输出的小变化。感知器不能满足这一需求，因为`z`的小变化可以产生输出的大变化。

## 2.2 激活功能

在讨论改进学习的更好方法之前，重要的是要解耦感知器逻辑。让我们引入一个名为σ的函数，它将`z = w.x + b`值作为参数。感知器的行为因此可以写成σ `(z) = 1 if z > 0, else` σ `(z) = 0`。

通过添加σ函数，我们将`w.x + b`的计算从条件中分离出来。我们称σ为激活函数。它根据`z`的值描述感知器的激活(即当它触发`1`时)。

感知器使用的激活函数称为*阶跃函数*，可以用图形表示，如图 [2-1](#Fig1) 所示。

![../images/484203_1_En_2_Chapter/484203_1_En_2_Fig1_HTML.jpg](../images/484203_1_En_2_Chapter/484203_1_En_2_Fig1_HTML.jpg)

图 2-1

阶跃函数

图 [2-1](#Fig1) 是执行该脚本的结果:

```py
g := RTGrapher new.
d := RTData new.
d connectColor: Color blue.
d noDot.
d points: (-7.0 to: 7.0 by: 0.01).
d x: #yourself.
d y: [ :x | x > 0 ifTrue: [ 1 ] ifFalse: [ 0 ] ].
g add: d.
g

```

您可能认识到为`y:`指令提供的步进功能。注意，提供给`y:`的函数将输入称为`x`，而`z`提供给σ。这是一个无害的重命名。

考虑一个值`z = 0`。因此我们有σ `(z) = 0`。如果我们给`z`*加上`0.00001`，一个小值，我们得到σ `(z) = 1`。添加到`z`的小值会产生σ `(z)`的大变化，从`0`到`1`。`z`的小变化产生σ `(z)`的大变化这一事实是一个严重的问题:感知器链无法学习。*

 *阶跃函数的特征在于具有垂直阶跃，这在其曲线中产生两个角度。这些角度使得阶跃函数不可导，这是一个很大的问题，我们很快就会看到。

## 2.3 乙状结肠神经元

我们将表达一种新的人工神经元，称为*乙状结肠神经元*。这种情况下的增量是使用新的激活函数，称为 *sigmoid 函数*。考虑图 [2-2](#Fig2) 中绘制的函数![$$ \sigma (z)=\frac{1}{1+{e}^{-z}} $$](../images/484203_1_En_2_Chapter/484203_1_En_2_Chapter_TeX_IEq1.png)。

![../images/484203_1_En_2_Chapter/484203_1_En_2_Fig2_HTML.jpg](../images/484203_1_En_2_Chapter/484203_1_En_2_Fig2_HTML.jpg)

图 2-2

sigmoid 函数

图 [2-2](#Fig2) 是执行脚本的结果:

```py
g := RTGrapher new.
d := RTData new.
d connectColor: Color blue.
d noDot.
d points: (-7.0 to: 7.0 by: 0.01).
d x: #yourself.
d y: [ :x | 1 / (1 + (x negated exp)) ].
g add: d.
g

```

这个 sigmoid 函数有几个优点:

*   它在曲线上处处可微，或者换句话说，它没有垂直线，更好的是，没有角度。我们可以很容易地为任何表示σ `(z)`斜率的值`z`画一条直线。当绘制时，σ `(z)`由于没有角度而非常平滑，这是一个非常好的特性。

*   它的导数有一些有趣的性质，我们将在后面看到。

*   对于非常小和非常大的`z`值，sigmoid 函数的行为类似于阶跃函数。

*   `z`的小增量会产生σ `(z)`的小变化，正如我们之前说过的，这对学习很重要。

我们将 sigmoid 神经元定义为具有 sigmoid 功能作为其激活功能的神经元。乙状结肠神经元被广泛接受为生物神经元行为的数学表示。

正如我们将在后面看到的，训练必须稍微调整，以利用σ `(z)`是可导的这一事实。

## 2.4 实现激活功能

在前一章中，我们定义了名为`Neuron`的类。我们将改进这个类来接受一个激活函数。首先，让我们为激活函数引入一个小的类层次结构。

名为`ActivationFunction`的抽象类可以定义如下:

```py
Object subclass: #ActivationFunction
        instanceVariableNames: ''
        classVariableNames: ''
        package: 'NeuralNetwork'

```

激活函数对象负责计算两件事:(I)激活值和(ii)传递导数。这个传递导数是反向传播学习算法的重要部分。本章给出了反向传播算法的实现，而理论背景将在第 [5](05.html) 章中介绍。

我们定义以下两个抽象方法。`eval`:方法计算激活值:

```py
ActivationFunction>>eval: z
      ^ self subclassResponsibility

```

方法`derivative`:计算传递导数:

```py
ActivationFunction>>derivative: output
      ^ self subclassResponsibility

```

我们刚刚定义的两个方法是抽象方法，这意味着它们是提供这些方法的适当实现的`ActivationFunction`的子类的占位符。

我们现在可以定义两个激活函数，每个都是`ActivationFunction`的子类。sigmoid 函数可以定义如下:

```py
ActivationFunction subclass: #SigmoidAF
       instanceVariableNames: ''
       classVariableNames: ''
       package: 'NeuralNetwork'

```

我们首先实现`eval`:函数:

```py
SigmoidAF>>eval: z
        ^ 1 / (1 + z negated exp)

```

然后我们实现`derivative`:方法，它代表`eval:`的数学导数:

```py
SigmoidAF>>derivative: output
        ^ output * (1 - output)

```

不赘述，我们有σ`(z)`’`=`σ`(z)`*`(1`——σ`(z))`。我们将在第 5 章[回到这一点。](05.html)

类似地，我们可以将阶跃函数定义如下:

```py
ActivationFunction subclass: #StepAF
      instanceVariableNames: ''
      classVariableNames: ''
      package: 'NeuralNetwork'

```

我们实现`eval`如下:

```py
StepAF>>eval: z
       ^ (z > 0) ifTrue: [ 1 ] ifFalse: [ 0 ]

```

我们还需要实现`derivative`:。我们将简单地让这个方法返回下面的参数:

```py
StepAF>>derivative: output
       ^ 1

```

阶跃函数的`derivative`:的公式不符合数学真理，即`0`带有一个未定义的`z = 0`值。然而，返回`1`反而简化了修改后的`Neuron`的实现，我们将在下一节中看到。

## 2.5 用激活函数扩展神经元

我们现在可以概括人工神经元从例子中学习的方式。假设一个示例值`(x, d)`，其中`x`是示例输入，`d`是示例输出。一开始，当提供输入`x = (x`<sub>`1`</sub>`, ..., x`<sub>`i`</sub>`, ..., x`<sub>`N`</sub>`)`给一个乙状结肠神经元时，输出很可能不同于`d`，一个介于 0 和 1 之间的数字。这并不奇怪，因为权重和偏差是随机选择的。这正是我们用那个例子训练神经元的原因，如果提供了`x`，就让神经元输出`d`。

学习机制可以用以下规则来概括:

*=(*d-z*)**σ*(*z**

 **w* <sub>【我】</sub>(*+1)=*【w】**【我】**

 **b*(*t*+1)=*b*(*t*)+***

其中:

*   δ是神经元的期望输出和实际输出之差

*   `d`是示例输出，这是所需的值

*   `z`是感知器的输出

*   σ是激活函数(阶跃函数或 sigmoid 函数)

*   σ′是σ的导数函数

*   `i`是权重指数，范围从`1`到`N`，即神经元所包含的权重数

*   `w` <sub>i</sub> (t)是给定时间的重量`i``t`

*   `b(t)`是给定时间的偏差`t`

*   `x` <sub>i</sub> 对应于在索引`i`处提供的输入

*   α是学习率，一个接近`0`的小正值

在很少或没有训练的情况下，神经元将输出一个与`d`非常不同的值`z`*。因此，δ也会很大。通过足够次数的训练，δ应该接近`0`。*

 *这些方程将在第 [5](05.html) 章中解释。目前，最重要的方面是它们可以被翻译成下面的伪代码:

```py
diff = desiredOutput - realOutput
delta = diff * derivative(realOutput)
alpha = 0.1
For all N:
    weightN = weightN + (alpha * inputN * delta)
bias = bias + (alpha * diff)

```

我们假设神经元有`N`个输入，因此有`N`个权重。我们现在可以扩展我们对神经元的定义，使用一个激活函数。我们可以通过向`Neuron`类添加一个名为`activationFunction`的新实例变量来做到这一点:

```py
Object subclass: #Neuron
        instanceVariableNames: 'weights bias learningRate
            activationFunction'
        classVariableNames: ''
        package: 'NeuralNetwork'

```

必须从外部访问`learningRate`变量:

```py
Neuron>>learningRate: aLearningRateAsFloat
        "Set the learning rate of the neuron. The argument should be a
              small floating value. For example, 0.01"
        learningRate := aLearningRateAsFloat

```

```py
Neuron>> learningRate
        "Return the learning rate of the neuron"
        ^ learningRate

```

喂养必须适应:

```py
Neuron>>feed: inputs
        | z |
        z := (inputs with: weights collect: [ :x :w | x * w ]) sum + bias.
        ^ activationFunction eval: z

```

我们现在准备实施算法来训练一个乙状结肠神经元。方法如下:

```py
Neuron>>train: inputs desiredOutput: desiredOutput
        | diff output delta |
        output := self feed: inputs.
        diff := desiredOutput - output.
        delta := diff * (activationFunction derivative: output).

        inputs withIndexDo: [ :anInput :index |
              weights at: index put: ((weights at: index) + (learningRate *
                   delta * anInput)) ].

        bias := bias + (learningRate * delta)

```

`train:desiredOutput:`方法与我们看到的感知器非常相似。我们引入了一个`delta`局部变量，它代表误差乘以传递导数。我们使用转移导数来表示*梯度下降*。我们将在第五章[中详细探讨这个话题。](05.html)

我们现在需要将一个神经元初始化为一个 s 形:

```py
Neuron>>initialize
        super initialize.
        learningRate := 0.1.
        self sigmoid

```

我们还可以定义两个实用程序方法:

```py
Neuron>>sigmoid
        "Use the sigmoid activation function"
        activationFunction := SigmoidAF new

```

```py
Neuron>>step
        "Use the step activation function"
        activationFunction := StepAF new

```

## 2.6 调整现有测试

如果你运行`PerceptronTest`，你会看到几个测试方法失败了。原因是神经元是用 sigmoid 激活函数初始化的。因此，我们需要修改`PerceptronTest`类来产生具有阶跃函数的神经元。幸运的是，我们可以简单地重新定义`newNeuron`方法:

```py
PerceptronTest>>newNeuron
       "Return a new neuron with the step activation function"
       ^ Neuron new step

```

所有包含在`PerceptronTest`中的测试在运行时都是绿色的。

## 2.7 测试乙状结肠神经元

由于乙状结肠神经元的行为非常类似于感知器，我们将重复使用一些测试。如下定义`NeuronTest`类:

```py
TestCase subclass: #NeuronTest
        instanceVariableNames: ''
        classVariableNames: ''
        package: 'NeuralNetwork'

```

然后我们可以训练一个神经元来学习一些逻辑门。下面的方法与我们看到的感知器非常相似:

```py
NeuronTest>>testTrainingAND
        | p |
        p := Neuron new.
        p weights: #(-1 -1).
        p bias: 2.

        5000
                timesRepeat: [
                       p train: #(0 0) desiredOutput: 0.
                       p train: #(0 1) desiredOutput: 0.
                       p train: #(1 0) desiredOutput: 0.
                       p train: #(1 1) desiredOutput: 1 ].
        self assert: ((p feed: #(0 0)) closeTo: 0 precision: 0.1).
        self assert: ((p feed: #(0 1)) closeTo: 0 precision: 0.1).
        self assert: ((p feed: #(1 0)) closeTo: 0 precision: 0.1).
        self assert: ((p feed: #(1 1)) closeTo: 1 precision: 0.1).

```

有两个不同之处:

*   纪元的数量显著增加。原因是乙状结肠神经元比感知器学习得更慢。

*   使用`closeTo:precision:`调用来比较供给神经元的结果。由于`feed:`方法的结果现在是一个浮点值而不是一个整数，我们需要调整我们比较这些值的方式。如果您仍然不确定在浮点之间使用`==`有什么问题，请评估表达式`0.1 + 0.2 - 0.3`。它返回的是`5.551115123125783e-17`，而不是预期的`0`。浮点值的编码方式导致了这种明显怪异的行为。

类似地，我们可以训练一个乙状结肠神经元来学习 OR 行为:

```py
NeuronTest>>testTrainingOR
        | p |
        p := Neuron new.
        p weights: #(-1 -1).
        p bias: 2.

        5000
                timesRepeat: [
                       p train: #(0 0) desiredOutput: 0.
                       p train: #(0 1) desiredOutput: 1.
                       p train: #(1 0) desiredOutput: 1.
                       p train: #(1 1) desiredOutput: 1 ].

        self assert: ((p feed: #(0 0)) closeTo: 0 precision: 0.1).
        self assert: ((p feed: #(0 1)) closeTo: 1 precision: 0.1).
        self assert: ((p feed: #(1 0)) closeTo: 1 precision: 0.1).
        self assert: ((p feed: #(1 1)) closeTo: 1 precision: 0.1).

```

如你所见，使用乙状结肠神经元并没有打乱我们的测试。我们只需要(I)增加我们训练神经元的历元数，以及(ii)在比较浮点值时更加小心。

*练习:*我们为乙状结肠神经元编写了一个改编版的 OR 和 and 逻辑门。修改其他逻辑门以使用 sigmoid 神经元。

## 2.8 学习速度较慢

本章一开始就指出了感知器的一个很大的局限性。这促使我们形成了乙状结肠神经元。乙状结肠神经元有一个缺点:它的学习速度比感知器慢。我们是在打赌，是用效率换取灵活性。正如你将在下一章看到的，乙状结肠神经元可以很好地结合。

我们可以很容易地比较乙状结肠神经元和感知器的学习。考虑以下脚本:

```py
learningCurveNeuron := OrderedCollection new.
0 to: 1000 do: [ :nbOfTrained |
     p := Neuron new.
     p weights: #(-1 -1).
     p bias: 2.

     nbOfTrained timesRepeat: [
             p train: #(0 0) desiredOutput: 0.
             p train: #(0 1) desiredOutput: 0.
             p train: #(1 0) desiredOutput: 0.
             p train: #(1 1) desiredOutput: 1 ].

     res := ((p feed: #(0 0)) - 0) abs +
            ((p feed: #(0 1)) - 0) abs +
            ((p feed: #(1 0)) - 0) abs +
            ((p feed: #(1 1)) - 1) abs.
     learningCurveNeuron add: res / 4.

].

learningCurvePerceptron := OrderedCollection new.
0 to: 1000 do: [ :nbOfTrained |
      p := Neuron new.
      p step.
      p weights: #(-1 -1).
      p bias: 2.

      nbOfTrained timesRepeat: [
               p train: #(0 0) desiredOutput: 0.
               p train: #(0 1) desiredOutput: 0.
               p train: #(1 0) desiredOutput: 0.
               p train: #(1 1) desiredOutput: 1 ].

      res := ((p feed: #(0 0)) - 0) abs +
             ((p feed: #(0 1)) - 0) abs +
             ((p feed: #(1 0)) - 0) abs +
             ((p feed: #(1 1)) - 1) abs.
      learningCurvePerceptron add: res / 4.

].

g := RTGrapher new.
d := RTData new.
d label: 'Sigmoid neuron'.
d noDot.
d connectColor: Color blue.
d points: learningCurveNeuron

.
d y: #yourself.
g add: d.

d := RTData new.
d label: 'Perceptron'.
d noDot.
d connectColor: Color green.
d points: learningCurvePerceptron.
d y: #yourself.
g add: d.
g axisY title: 'Error'.
g axisX noDecimal; title: 'Epoch'.
g legend addText: 'Perceptron vs Sigmoid neuron'.
g

```

![../images/484203_1_En_2_Chapter/484203_1_En_2_Fig3_HTML.jpg](../images/484203_1_En_2_Chapter/484203_1_En_2_Fig3_HTML.jpg)

图 2-3

感知器与乙状结肠神经元

该脚本生成如图 [2-3](#Fig3) 所示的图形。无论定义的学习速率如何，感知器的学习速度确实比乙状结肠神经元快得多。

下一章揭示了乙状结肠神经元的真正力量，这将抵消它学习速度较慢的事实。

## 2.9 我们在本章中看到了什么？

本章涵盖了以下主题:

*   *简要讨论了感知器的局限性。*感知器与其他感知器结合时无法学习。尽管我们还没有进一步讨论这方面，下一章我们将对此作进一步的阐述。

*   *乙状结肠神经元的定义。*乙状结肠神经元是感知器的改进，因为它可以与其他乙状结肠神经元结合，并且这种结合可以学习。在下一章中，我们将讨论反向传播算法，这是神经网络学习的一个核心方面。

*   *激活功能。*我们已经看到了两个激活函数，阶跃函数和 sigmoid 函数。周围还有很多其他的激活功能。

下一章是关于组成乙状结肠神经元，以建立人工神经网络。****