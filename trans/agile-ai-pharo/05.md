# 5.数据分类

神经网络有非常广泛的应用。数据分类是一个突出的问题，这一章将专门讨论它。

## 5.1 培训网络

在前一章中，我们看到我们可以获得一个经过训练的神经网络来表示 XOR 逻辑门。特别是，我们看到了以下脚本:

```
n := NNetwork new.
n configure: 2 hidden: 3 nbOfOutputs: 1.

20000 timesRepeat: [
        n train: #(0 0) desiredOutputs: #(0).
        n train: #(0 1) desiredOutputs: #(1).
        n train: #(1 0) desiredOutputs: #(1).
        n train: #(1 1) desiredOutputs: #(0).
].

```

在评估这个脚本之后，表达式`n feed: #(1 0)`评估为`#(0.9530556769505442)`，这是一个具有接近`1`的预期浮点值的数组。如果我们后退一点，我们会发现这个脚本实际上非常冗长。比如为什么要手动处理重复？为什么消息`train:desiredOutputs`:发了这么多次？我们可以通过提供一些基础设施来极大地简化网络的训练方式。

考虑以下方法:

```
NNetwork>>train: train nbEpochs: nbEpochs
        "Train the network using the train dataset."
        | sumError outputs expectedOutput epochPrecision t |
        1 to: nbEpochs do: [ :epoch |
              sumError := 0.
                  epochPrecision := 0.
              train do: [ :row |
              outputs := self feed: row allButLast.
              expectedOutput := (1 to: self numberOfOutputs) collect: [ :
                   notUsed | 0 ].
              expectedOutput at: (row last) + 1 put: 1.
               (row last = (self predict: row allButLast)) ifTrue: [
                    epochPrecision := epochPrecision + 1 ].
              t := (1 to: expectedOutput size)
                        collect: [ :i | ((expectedOutput at: i) - (outputs
                            at: i)) squared ].
              sumError := sumError + t sum.
              self backwardPropagateError: expectedOutput.
              self updateWeight: row allButLast.
        ].
        errors add: sumError.
           precisions add: (epochPrecision / train size) asFloat.
]

```

可以使用`predict:`方法预测一组给定输入值的输出:

```
NNetwork>>predict: inputs
        "Make a prediction. This method assumes that the number of
              outputs is the same as the number of different values
              the network can output"
        "The index of a collection begins at 1 in Pharo"
        | outputs |
        outputs := self feed: inputs.
        ^ (outputs indexOf: (outputs max)) - 1

```

这两种方法使得网络训练明显不那么冗长。用 XOR 逻辑门训练网络的脚本现在可以写成如下:

```
n := NNetwork new.
n configure: 2 hidden: 3 nbOfOutputs: 2.

data := {#(0 0 0) .
      #(0 1 1) .
      #(1 0 1) .
      #(1 1 0) }.
n train: data nbEpochs: 20000

```

`data`变量是一个数字数组的数组。每行代表一个示例，它包含输入值和输出值。例如，行`#(0 1 1)`代表行`ntrain: #(0 1)desiredOutputs: #(1)`。注意，神经网络有两个输出神经元。

这是对输出使用一次性编码的结果。这些示例有两个不同的输出值，或者是`0`或者是`1`。如果我们使用一键编码，我们有两个输出神经元，每个神经元对应一个特定的值。在本章的后面，我们将详细介绍这种编码。

使用我们刚刚介绍的语法的另一个例子如下:

```
n := NNetwork new.
n configure: 3 hidden: 8 nbOfOutputs: 8.

data := {#(0 0 0 0).
      #(0 0 1 1).
      #(0 1 0 2).
      #(0 1 1 3).
      #(1 0 0 4).
      #(1 0 1 5).
      #(1 1 0 6).
      #(1 1 1 7) }.
n train: data nbEpochs: 1000.

```

这段代码构建了一个经过训练的神经网络，可以将二进制数转换为十进制数。二进制数是用三位编码的，所以我们需要一个有三个输入的神经网络。由于十进制值的范围从`0`到 7，我们需要网络的八个输出神经元。例如，要转换二进制数，可以在前面的脚本之后计算以下表达式:

```
...
n predict: #(0 1 1)

```

最后一个表达式返回`3`，因此表明转换良好。实现`train:nbEpochs:`和`predict:`的方式强制训练数据遵循一些规则。`data`中包含的每个元素都必须是数字的集合。除最后一个数字外，所有数字都代表输入值。一个示例的最后一个值以数组的形式表示，是预期的输出值。例如，考虑示例`#(0 1 1 3)`，当`#(0 1 1)`被提供作为输入时，值`3`是期望值。预期输出是一个正值，范围从`0`到神经网络输出数减 1。

## 5.2 作为散列表的神经网络

让我们退后一点。我们花了六章来激励、描述和逐步建立神经网络。但是我们使用神经网络的方式和使用普通散列表的方式非常相似。考虑下面的例子:

```
data := {#(0 0 0 0).
      #(0 0 1 1).
      #(0 1 0 2).
      #(0 1 1 3).
      #(1 0 0 4).
      #(1 0 1 5).
      #(1 1 0 6).
      #(1 1 1 7) }.

d := Dictionary new.
data do: [ :anExample |
      d at: anExample allButLast put: anExample last ].
d at: #(0 1 1)

```

脚本产生`3`。`d`变量是一个填充了示例数据的字典。我们用作神经网络输入的值被用作字典中的关键字。的确，在这里使用字典有很多好处:填充一个字典明显比训练一个神经网络要快(快几个数量级！)，并且获取特定键的值也比前馈网络快得多。

然而，hashmap 需要完全相同的键(或者至少充分地响应消息`=`)。神经网络不需要完全相同的输入值。考虑以下脚本:

```
n := NNetwork new.
n configure: 3 hidden: 8 nbOfOutputs: 8.

data := {#(0 0 0 0).
      #(0 0 1 1).
      #(0 1 0 2).
      #(0 1 1 3).
      #(1 0 0 4).
      #(1 0 1 5).
      #(1 1 0 6).
      #(1 1 1 7) }.
n train: data nbEpochs: 1000.
n predict: #(0.4 0.7 0.6)

```

通过返回值`3`，网络可以匹配输入值`#(0.4 0.7 0.6)`到`#(0 1 1)`。没有程序员的明确指示，hashmap 无法建立这样的连接，这就是神经网络的全部意义:在没有程序员干预的情况下，在输入数据之间建立连接，并识别最相关的数据。

## 5.3 可视化误差和拓扑

我们看到，反向传播的第一步是用提供的输入评估网络。然后将输出值与预期输出值进行比较。然后，通过将实际输出和预期输出之间的差异反向传播到网络，使用这些差异来调整权重和偏差。

`NNetwork>>train:nbEpochs:`方法包含语句`errors add: sumError`和`precisions add: (epochPrecision / train size)asFloat`。这两行代码的作用是记录`sumError`的值，指示网络在所提供的示例中表现如何，以及每个时期的精度值。对于给定的网络和样本集，这两个数字集合可以被视为描述整个学习过程的助手。

我们在`NNetwork`类上定义了`viewLearningCurve`方法:

```
NNetwork>>viewLearningCurve
        "Draw the error and precision curve"
        | b ds |
        "No need to draw anything if the network has not been run"
        errors ifEmpty: [ ^ RTView new
                          add: (RTLabel elementOn: 'Should first run
                               the network');
                          yourself ].
        b := RTDoubleGrapher new.

        "We define the size of the charting area"
        b extent: 500 @ 300.
        ds := RTData new.
        "A simple optimization that Roassal offers"
        ds samplingIfMoreThan: 2000.
        "No need of dots, simply a curve"
        ds noDot; connectColor: Color blue.
        ds points: (errors collectWithIndex: [ :y :i | i -> y ]).
        ds x: #key.
        ds y: #value.
        ds dotShape rectangle color: Color blue.
        b add: ds.
        ds := RTData new.
        ds samplingIfMoreThan: 2000.
        ds noDot.
        ds connectColor: Color red.
        ds points: (precisions collectWithIndex: [ :y :i | i -> y ]).
        ds x: #key.
        ds y: #value.
        ds dotShape rectangle color: Color blue.
        b addRight: ds.
        b axisX noDecimal; title: 'Epoch'.
        b axisY title: 'Error'.
        b axisYRight title: 'Precision'; color: Color red.
        ^ b

```

以下方法定义了`errors`和`precisions`变量的可视化:

```
NNetwork>>viewLearningCurveIn: composite
        <gtInspectorPresentationOrder: -10>
        composite roassal2
                title: 'Learning';
                initializeView: [ self viewLearningCurve ]

```

`NNetwork>>viewLearningCurveIn:`方法使用`GTInspector`框架在检查器中添加一个特定的选项卡。

检查以下代码片段会显示错误曲线(参见图 [5-1](#Fig1) ):

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig1_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig1_HTML.jpg)

图 5-1

视觉化学习

```
n := NNetwork new.
n configure: 2 hidden: 3 nbOfOutputs: 2.

data := {#(0 0 0) .
      #(0 1 1) .
      #(1 0 1) .
      #(1 1 0) }.
n train: data nbEpochs: 10000.

```

学习曲线表明了在使神经网络学习时的时期数的影响。蓝线接近`0`的事实是神经网络正在正确学习的一个强有力的指标。红线达到 1.0，表示网络准确。

同样，我们可以使用以下方法来可视化网络拓扑:

```
NNetwork>>viewNetwork
        | b lb |
        b := RTMondrian new.

        b nodes: layers forEach: [ :aLayer |
               b shape circle size: 20.
               b nodes: aLayer neurons.
               b layout verticalLine.
        ].

        b shape arrowedLine; withShorterDistanceAttachPoint.
        b edges connectTo: #nextLayer.
        b layout horizontalLine gapSize: 30; center.

        b build.

        lb := RTLegendBuilder new.
        lb view: b view.
        lb addText: self numberOfNeurons asString, ' neurons'.
        lb addText: self numberOfInputs asString, ' inputs'.
        lb build.
        ^ b view

```

我们需要定义 helper 方法，如下所示:

```
NNetwork>>numberOfInputs
        "Return the number of inputs the network has"
        ^ layers first neurons size

```

并且该方法:

```
NNetwork>>numberOfNeurons
        "Return the total number of neurons the network has"
        ^ (layers collect: #numberOfNeurons) sum

```

同样，我们需要扩展`GTInspector`来考虑`GTInspector`内的可视化(见图 [5-2](#Fig2) ):

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig2_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig2_HTML.jpg)

图 5-2

可视化网络拓扑

```
NNetwork>>viewNetworkIn: composite
        <gtInspectorPresentationOrder: -5>
        composite roassal2
                title: 'Network';
                initializeView: [ self viewNetwork ]

```

您可以单击一个神经元来显示其权重和偏差。

## 5.4 矛盾的数据

蓝色误差曲线量化了网络在学习阶段产生的误差。误差可能会有一些平稳段。在这种情况下，增加历元的数量可能具有降低误差曲线的效果。

在某些情况下，如果误差曲线无法接近`0`，则可能表明数据中存在矛盾。

考虑下面的例子:

```
n := NNetwork new.
n configure: 2 hidden: 3 nbOfOutputs: 2.

data := {#(0 0 0) .
      #(0 0 1) }.
n train: data nbEpochs: 1000.

```

该脚本使用两个相互矛盾的示例来训练神经网络。第一个例子训练网络用输入`0`和`0`输出`0`。第二个例子训练网络对于相同的输入值输出`1`。

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig3_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig3_HTML.jpg)

图 5-3

数据矛盾

图 [5-3](#Fig3) 显示了存在矛盾数据时的误差和精度曲线。该脚本使神经网络针对完全相同的输入值学习两种不同的输出。因此，网络将不得不在学习阶段出错。

在真实且重要的数据集中，这种情况很可能会发生。如果矛盾事件并不常见，那么网络会将这种矛盾视为纯粹的噪音，并倾向于减少这种噪音。

## 5.5 数据分类和一键编码

分类可以定义为根据元素的特征对元素进行分组。共享相似特征的元素被分组在一起。先前的 XOR 数据集可以被认为是一个(简单的)分类模型，其中每个组由两个元素组成。组`0`由元素`[0, 0]`和`[1, 1]`组成，组`1`由`[0, 1]`和`[1, 0]`组成。

您是否注意到，为了学习 XOR 数据集，我们使用了具有两个输出的神经网络？原因是我们使用*一键编码*对输出值进行编码。

一键编码是一种简单的机制，它将分类变量转换为数字形式，可以输入到神经网络中。考虑代表集合`{ "hello", "bonjour", "Buenos dias" }`中的单词的变量`v` *，*。应用一键编码会给每个单词分配一个唯一的号码。例如，`"hello"`与索引`0`关联，`bonjour"`与索引`1`关联，`"Buenos dias"`与`2`关联。由于数据集有三个不同的字，因此`v`的值可以用三个不同的位进行编码。然后我们可以对单词进行编码:

*   `"hello" = [1, 0, 0]`

*   `"bonjour" = [0, 1, 0]`

*   `"Buenos dias" = [0, 0, 1]`

如果变量`v`必须被提供给神经网络，那么三个神经元可以用于该目的。

我们将 XOR 数据集定义如下:

```
n := NNetwork new.
n configure: 2 hidden: 3 nbOfOutputs: 2.

data := {#(0 0 0) .
      #(0 1 1) .
      #(1 0 1) .
      #(1 1 0) }.
n train: data nbEpochs: 10000

```

由于数据集有两个不同的值，`0`和`1`，我们有两个输出神经元:值`0`被编码为`[1, 0]`，而`1`被编码为`[0, 1]`。

既然我们已经解释了独热编码，我们可以继续处理更大的数据集。

## 5.6 虹膜数据集

Iris flower 数据集是机器学习社区使用的流行数据集(参见 [`http://archive.ics.uci.edu/ml/datas`](http://archive.ics.uci.edu/ml/datas) )。这个数据集是罗纳德·费雪在 1936 年收集的，并发表在题为“多重测量在分类学问题中的应用”的开创性论文中该数据集包含三个鸢尾科的 50 个样本，分别称为*鸢尾*、*海滨鸢尾、*和*杂色鸢尾*。我们将这些家族称为*类*。

我们在 [`https://agileartificialintelligence.github.io/Datasets/iris.csv`](https://agileartificialintelligence.github.io/Datasets/iris.csv) 提供了该数据集的副本。在 Pharo 中，可以使用以下表达式获取数据集:

```
(ZnEasy get: 'https://agileartificialintelligence.github.io/Datasets/
       iris.csv') contents.

```

代码获取`iris.csv`文件并返回其内容。CSV 文件头中给出的文件结构如下:

```
sepal_length,sepal_width,petal_length,petal_width,species

```

然而，获取文件只是使文件能够被神经网络处理的第一小步。例如，我们需要将文件的每一行转换成一组数值(记住神经网络只能接受数字作为输入)。

为了向网络提供 IRIS 数据集，我们需要执行以下步骤:

1.  从互联网上获取文件。

2.  将表示为很长文本的文件内容剪切成文本行。

3.  忽略文件的第一行，因为它包含与网络无关的 CSV 文件头。

4.  解析 CSV 文件。

5.  在表格中，用一个数值代替每个花名，可以是`0`、`1`或`2`。

以下脚本执行这五个步骤:

```
"The execution of this script initializes the variable irisData.
This variable is used in the subsequent scripts of this chapter"
irisCSV := (ZnEasy get: 'https://agileartificialintelligence.github.io/
       Datasets/iris.csv') contents.
lines := irisCSV lines.
lines := lines allButFirst.
tLines := lines collect: [ :l |
           | ss |
           ss := l substrings: ','.
            (ss allButLast collect: [ :w | w asNumber ]), (Array with: ss
                 last) ].

irisData := tLines collect: [ :row |
           | l |
           row last = 'setosa' ifTrue: [ l := #( 0 ) ].
           row last = 'versicolor' ifTrue: [ l := #( 1 ) ].
           row last = 'virginica' ifTrue: [ l := #( 2 ) ].
           row allButLast, l ].

irisData.

```

总之，该脚本转换一个很长的字符串，如下所示:

```
'sepal_length,sepal_width,petal_length,petal_width,species
5.1,3.5,1.4,0.2,setosa
4.9,3.0,1.4,0.2,setosa
4.7,3.2,1.3,0.2,setosa
...
'

```

转换成数字的集合，如下所示:

```
#(#(5.1 3.5 1.4 0.2 0) #(4.9 3.0 1.4 0.2 0) #(4.7 3.2 1.3 0.2 0) ...

```

脚本的结果是`irisData`变量的值。在本章的剩余部分，当我们提到 Iris 数据集时，我们实际上指的是`irisData`值。

## 5.7 用 Iris 数据集训练网络

训练一个网络其实很容易，因为我们精心准备了战场。本章的剩余部分假设变量`irisData`的定义如前一节所示。考虑以下代码:

```
n := NNetwork new.
n configure: 4 hidden: 6 nbOfOutputs: 3.
n train: irisData nbEpochs: 1000.

```

这段代码构建了一个有四个输入值的网络，一个隐藏层有六个神经元，一个输出层有三个神经元。输入的数量表示 Iris 数据集中一行的大小减一，这是预期的输出值，不是输入的一部分。我们选择任意的 6 作为隐藏层的大小。隐藏层大小的一般经验法则是包括比输入数量多 50%的神经元。我们在输出层有三个神经元，因为有三个不同的虹膜家族。

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig4_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig4_HTML.jpg)

图 5-4

学习虹膜数据集

图 [5-4](#Fig4) 表示网络的误差曲线。蓝色曲线非常接近`0`，说明网络在学习，数据集没有矛盾。红色曲线与`1.0`非常接近，说明网络的精度非常好。在学习过程中，网络能够学习并达到良好的精度。

我们的网络配置有两个参数:隐藏层中神经元的数量，以及要考虑的时期的数量。关于如何选择这些参数，没有通用的规则。目前，实验和临时尝试仍然是配置网络最简单的方法。这本书的第三部分，关于神经进化，将涵盖使用遗传算法搜索超参数。

## 5.8 学习曲线的效果

当我们在第 [2](02.html) 章中定义`Neuron`类时，我们定义了`learningRate:`方法来设置神经元的学习速率。一般来说，对于单个神经元来说，学习速率越高，它学习的速度就越快。我们可以在下面的例子中很容易地说明这种影响(见图 [5-5](#Fig5) ):

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig5_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig5_HTML.jpg)

图 5-5

学习速率对单个神经元的影响

```
g := RTGrapher new.
#(0.001 0.01 0.1 0.2 0.3)
      doWithIndex: [ :lr :index |
              learningCurveNeuron := OrderedCollection new.
              0 to: 1000 do: [ :nbOfTrained |
                    r := Random new seed: 42.
                    p := Neuron new.
                    p weights: #(-1 -1).
                    p bias: 2.
                    p learningRate: lr.
                    nbOfTrained
                            timesRepeat: [ p train: #(0 0) desiredOutput: 0.
                                  p train: #(0 1) desiredOutput: 0.
                                  p train: #(1 0) desiredOutput: 0.
                                  p train: #(1 1) desiredOutput: 1 ].
                    res := ((p feed: #(0 0)) - 0) abs + ((p feed: #(0 1)) - 0)
                         abs
                         + ((p feed: #(1 0)) - 0) abs + ((p feed: #(1 1)) - 1)
                             abs.
                    learningCurveNeuron add: res / 4 ].
              d := RTData new.
              d label: 'Sigmoid neuron lr = ' , lr asString.
              d noDot.
              d connectColor: (RTPalette c1 at: index).
              d points: learningCurveNeuron.
              d y: #yourself.
              g add: d ].
g legend addText: 'Learning rate effect'.
g

```

图 [5-5](#Fig5) 表示五种不同学习率值(`0.001`、`0.01`、`0.1`、`0.2`、`0.3`)训练时的误差曲线。该图表明，学习率越高，它学习得越快。

在单个乙状结肠神经元*上观察到的效果不能在整个网络上观察到。我们可以针对不同的学习率值来训练虹膜数据集的网络。考虑以下脚本:*

```
n := NNetwork new.
n configure: 4 hidden: 6 nbOfOutputs: 3.
n learningRate: 0.3\. " Repeat the script with a different value"
n train: irisData nbEpochs: 1000.

```

我们为`0.001`、`0.01`、`0.1`和`0.3`值运行脚本。结果如图 [5-6](#Fig6) 所示。

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig6_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig6_HTML.jpg)

图 5-6

神经网络学习速率对虹膜数据集的影响

我们清楚地看到，在低学习率的情况下，精度和误差曲线相当稳定。在相对较高的学习率下，我们会经历非常频繁的高峰。

不幸的是，没有通用的方法来确定适当的学习率或网络的架构。到目前为止，手动调优是常态。一些优化算法，例如 Adam 优化算法，在训练期间改变学习率。

## 5.9 测试和验证

到目前为止，我们已经建立了一个在整个 Iris 数据集上训练的网络:我们使用`.csv`文件中的所有条目来训练网络。网络似乎正确地学习了，因为网络在沿着历元增加精度的同时犯了更少的错误(即，误差曲线变得非常接近`0`)。

误差曲线表明网络学习所提供的数据集的程度。如果我们想知道网络对数据的分类有多好，用训练它的数据来测试它没有多大意义。询问一个网络在用于训练它的数据存在的情况下表现如何并不是一个很大的挑战。然而，一个重要的问题是，在存在从未见过的数据的情况下，网络的表现如何？换句话说，网络对未知数据的分类有多好？

回答这个问题的一种方法是将虹膜数据集分成两个不同的部分:

*   *训练数据集*:用于训练网络的`.csv`文件的一部分。

*   *测试数据集*:文件的第二部分用于查看训练好的网络有多有效。

考虑以下脚本:

```
cut := 0.8.
cutTraining := (irisData size * cut) rounded.
cutTest := (irisData size * (1 - cut)) rounded.
trainingData := irisData first: cutTraining.
testData := irisData last: cutTest.

```

`cut`变量代表原始虹膜数据集用于训练的部分:80%的`irisData`用于训练。`cutTraining`变量代表用于训练的`irisData`元素的数量。类似地，`cutTest`代表测试的元素数量。当发送给浮点值时，`rounded`消息返回最接近浮点值的整数(例如，`4.6 rounded`返回`5`，`4.3 rounded`返回 4，`4.5 rounded`返回`5`)。

我们可以根据`trainingData`训练一个网络:

```
...
n := NNetwork new.
n configure: 4 hidden: 6 nbOfOutputs: 3.
n train: trainingData nbEpochs: 1000.

```

我们看到网络可以适当地学习`trainingData`，因为误差曲线接近于`0`，类似于图 [5-4](#Fig4) 。

考虑这个脚本(它假设前面看到的变量`irisData`存在):

```
cut := 0.8.
cutTraining := (irisData size * cut) rounded.
cutTest := (irisData size * (1 - cut)) rounded.
trainingData := irisData first: cutTraining.
testData := irisData last: cutTest.
n := NNetwork new.
n configure: 4 hidden: 6 nbOfOutputs: 3.
n train: trainingData nbEpochs: 1000.

(((testData collect: [ :d |
    (n predict: d allButLast) = d last
]) select: [ :d | d = true]) size / testData size) asFloat round: 2

```

评估脚本返回`0.9`，它代表了我们网络的准确性:`testData`中包含的元素有 90%被正确预测。

我们现在将详细介绍脚本的最后一部分:

```
(((testData collect: [ :d |
     (n predict: d allButLast) = d last
]) select: [ :d | d = true]) size / testData size) asFloat round: 2

```

对于`testData`的所有元素，我们预测输入的分类(`dallButLast`)，并将网络结果与预期结果(`dlast`)进行比较。`collect:`指令的结果是二进制值列表(`true`或`false`)。我们只选择`true`值，并计算有多少(`size`)。然后，我们计算测试数据大小的比率(`/testDatasize`)。最后，我们只考虑一个有两位小数的浮点值。

*练习:*用`0.6`、`0.5`、`0.4`的截确定网络的精度。考虑一个`0.7`的剪辑，如脚本所示:

```
cut := 0.7.
cutTraining := (irisData size * cut) rounded.
cutTest := (irisData size * (1 - cut)) rounded.
trainingData := irisData first: cutTraining.
testData := irisData last: cutTest.
n := NNetwork new.
n configure: 4 hidden: 6 nbOfOutputs: 3.
n train: trainingData nbEpochs: 1000.

(((testData collect: [ :d |
     (n predict: d allButLast) = d last
]) select: [ :d | d = true]) size / testData size) asFloat round: 2

```

结果是`0.0`，表示网络无法做出预测。为什么不呢？当我们减少训练数据的规模时(例如，如果 cut 等于`0.5`)，网络的准确性会增加。这是数据组织的一个效果。

如果考察`irisData`的 150 个值，我们看到它们实际上是有序的:前 50 个条目是鸢尾(期望值为`0`)，后面 50 个条目是杂色鸢尾(期望值为`1`)，最后 50 个条目是海滨鸢尾(期望值为`2`)。原始数据集是有序的这一事实会对网络的精度产生影响。幸运的是，这个问题很容易解决:一个简单的原始数据洗牌将防止网络遭受进入顺序。

考虑这个新脚本:

```
shuffledIrisData := irisData shuffleBy: (Random seed: 42).
cut := 0.8.
cutTraining := (shuffledIrisData size * cut) rounded.
cutTest := (shuffledIrisData size * (1 - cut)) rounded.
trainingData := shuffledIrisData first: cutTraining.
testData := shuffledIrisData last: cutTest.
n := NNetwork new.
n configure: 4 hidden: 6 nbOfOutputs: 3.
n train: trainingData nbEpochs: 1000.

(((testData collect: [ :d |
     (n predict: d allButLast) = d last
]) select: [ :d | d = true]) size / testData size) asFloat round: 2

```

该脚本引入了一个新变量，称为`shuffledIrisData`。它用`irisData shuffleBy` : ( `Random seed: 42`)初始化，这使用一个随机数创建了一个`irisData`的副本。如果我们不希望使用随机数发生器，因此每次运行的结果都略有不同，我们可以简单地使用`shuffled`代替`shuffleBy:` ( `Random seed` : 42)。

## 5.10 标准化

当我们展示感知器和 sigmoid 神经元时，我们看到激活函数被应用于值`z = w.x + b`。应用于一个有两个输入的神经元，我们有`z = x`<sub>`1`</sub>`.w`<sub>`1`</sub>`+ x`<sub>`2`</sub>`.w`<sub>`2`</sub>`+ b`。在示例中，所有的`xi`和输出值都在相同的区间内，从`0`到`1`。在逻辑门的例子中，每个`xi`或者是`0`或者是`1`。在 Iris 数据集中，我们可以计算每个输入值的最小值和最大值:

```
max := OrderedCollection new.
min := OrderedCollection new.
(1 to: 4) collect: [ :i |
    max add: (irisData collect: [ :d | d at: i ]) max.
    min add: (irisData collect: [ :d | d at: i ]) min.
].
{ max . min }

```

该脚本的结果表明，总的来说，该值的范围是从`0.1`到`7.9`。换句话说，所有输入值的范围都在相同的幅度内。

为什么这很重要？考虑我们之前看到的将二进制数转换为小数的例子:

```
n := NNetwork new.
n configure: 3 hidden: 8 nbOfOutputs: 8.

data := {#(0 0 0 0).
      #(0 0 1 1).
      #(0 1 0 2).
      #(0 1 1 3).
      #(1 0 0 4).
      #(1 0 1 5).
      #(1 1 0 6).
      #(1 1 1 7) }.
n train: data nbEpochs: 1000.

```

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig7_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig7_HTML.jpg)

图 5-7

学习虹膜数据集

图 [5-7](#Fig7) 显示了网络的误差曲线。每个输入值不是`0`就是`1`。我们将通过改变每一列的比例来产生一个不同但等效的数据集。在这个修改后的例子中，我们将第一个输入设为`0`或`0.1`，第二个输入设为`0`或`1000`。请考虑以下情况:

```
n := NNetwork new.
n configure: 3 hidden: 8 nbOfOutputs: 8.

data := {#(0 0 0 0).
      #(0 0 1 1).
      #(0 1000 0 2).
      #(0 1000 1 3).
      #(0.1 0 0 4).
      #(0.1 0 1 5).
      #(0.1 1000 0 6).
      #(0.1 1000 1 7) }.
n train: data nbEpochs: 10000.

```

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig8_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig8_HTML.jpg)

图 5-8

奇怪比例的虹膜数据集

图 [5-8](#Fig8) 显示了误差曲线和沿历元的精度。误差的发展已经达到平稳状态，精度不超过`0.5`。这是因为更改特定输入值的比例会影响这些值的相关性。

sigmoid 函数返回一个介于`0`和`1`之间的值。具有相同的输入范围提高了学习性能。避免数据失真的一种方法是使每个输入范围在`0`和`1`之间。将数据从任意范围转换到受限范围的过程称为*规范化*。幸运的是，规范化数据相当简单。考虑一下`f`的功能:

![$$ f(x)=\frac{\left(x-{d}_L\right)\left({n}_H-{n}_L\right)}{d_H-{d}_L}+{n}_L $$](../images/484203_1_En_5_Chapter/484203_1_En_5_Chapter_TeX_Equa.png)

函数`f(x)`对一个值`x`进行归一化。变量`d`代表数据的高低值。变量`n`代表期望的高和低标准化范围。大多数情况下，我们会有`n` <sub>`L`</sub> `= 0`和`r` <sub>`H`</sub> `= 1`。

因此，我们可以实现以下实用程序类:

```
Object subclass: #Normalization
        instanceVariableNames: ''
        classVariableNames: ''
        package: 'NeuralNetwork'

```

然后我们定义了`normalizeData:`方法，它将一些训练数据作为参数:

```
Normalization>>normalizeData:
      aCollectionOfTrainingDataWithExpectedOutput
       "Normalize the data provided as argument"

       | nbOfColumns min max |
       "We exclude the expected output"
       nbOfColumns := aCollectionOfTrainingDataWithExpectedOutput first
             size - 1.

       min := OrderedCollection new.
       max := OrderedCollection new.
       1 to: nbOfColumns do: [ :index |
             | column |
             column := aCollectionOfTrainingDataWithExpectedOutput collect:
                   [ :row | row at: index ].
             min add: column min.
             max add: column max ].

       ^ self normalizeData: aCollectionOfTrainingDataWithExpectedOutput
           min: min max: max

```

真正的工作发生在第二种方法中:

```
Normalization>>normalizeData:
      aCollectionOfTrainingDataWithExpectedOutput min: minimumValues max:
      maximumValues
         | nbOfColumns result mn mx |
      nbOfColumns := aCollectionOfTrainingDataWithExpectedOutput first
            size - 1.

      result := OrderedCollection new.
      aCollectionOfTrainingDataWithExpectedOutput do: [ :row |
             | t v |
             t := OrderedCollection new.
             1 to: nbOfColumns do: [ :index |
                  v := row at: index.
                  mn := minimumValues at: index.
                  mx := maximumValues at: index.
                  t add: ((v - mn) / (mx - mn)) asFloat
             ].
             t add: row last.
             result add: t asArray ].
      ^ result asArray

```

我们可以测试这些方法。首先我们可以创建一个名为`NormalizationTest`的单元测试:

```
TestCase subclass: #NormalizationTest
       instanceVariableNames: ''
       classVariableNames: ''
       package: 'NeuralNetwork'

```

```
NormalizationTest>>testSimpleNormalization
         | input expectedNormalizedInput |
         input := #( #(10 5 1) #(2 6 0) ).
         expectedNormalizedInput := Normalization new normalizeData: input.
         self assert: expectedNormalizedInput equals: #(#(1.0 0.0 1) #(0.0
             1.0 0))

```

这个小测试方法说明了简单标准化的结果。例如，`input`的两个条目的第一列中，`10`为最高值，`2`为最低值。归一化用`1.0`代替最高值，用`0.0`代替最低值。

请注意，只有当两个或更多条目作为输入提供时，规范化才有意义。我们可以测试错误的案例:

```
NormalizationTest>>testError
         self should: [ Normalization new normalizeData: #( #(10 5 1) ) ]
             raise: Error.
NormalizationTest>>testEmptyError
         self should: [ Normalization new normalizeData: #() ] raise: Error.

```

当神经网络用于回归时，返回值将被规范化。因此，我们需要*去规格化*它们。考虑功能`g` :

![$$ g(x)=\frac{\left({d}_L-{d}_H\right)x-\left({n}_H{d}_L\right)+{d}_H{n}_L}{n_L-{n}_H} $$](../images/484203_1_En_5_Chapter/484203_1_En_5_Chapter_TeX_Equb.png)

为了完整起见，我们给出了反规格化函数。我们将不使用它，因为我们从本章中排除了数据回归。

## 5.11 将规范化集成到网络类中

上一节描述了规范化功能。目前，它与`NNetwork`类断开连接。将标准化集成到我们的神经网络中是无缝受益于它的自然的下一步。`train:nbEpochs:`方法可以重新定义如下:

```
NNetwork>>train: train nbEpochs: nbEpochs
        "Train the network using the train dataset."
        | sumError outputs expectedOutput epochPrecision t normalizedTrain
            |
        normalizedTrain := Normalization new normalizeData: train.
        1 to: nbEpochs do: [ :epoch |
             sumError := 0.
                 epochPrecision := 0.
             normalizedTrain do: [ :row |
                     outputs := self feed: row allButLast.
                     expectedOutput := (1 to: self numberOfOutputs) collect: [ :
                           notUsed | 0 ].
                     expectedOutput at: (row last) + 1 put: 1.
                     (row last = (self predict: row allButLast)) ifTrue: [
                          epochPrecision := epochPrecision + 1 ].
                     t := (1 to: expectedOutput size)
                               collect: [ :i | ((expectedOutput at: i) - (outputs
at: i)) squared ].
                     sumError := sumError + t sum.
                     self backwardPropagateError: expectedOutput.
                     self updateWeight: row allButLast.
             ].
             errors add: sumError.
        precisions add: (epochPrecision / train size) asFloat.
]

```

该方法的修订版使用`Normalization` `new` `normalizeData: train`表达式对输入数据进行归一化。该表达式的结果用于训练网络。运行以下脚本表明快速达到高精度(参见图 [5-9](#Fig9) ):

![../images/484203_1_En_5_Chapter/484203_1_En_5_Fig9_HTML.jpg](../images/484203_1_En_5_Chapter/484203_1_En_5_Fig9_HTML.jpg)

图 5-9

一旦标准化，网络能够快速学习数据

```
n := NNetwork new.
n configure: 3 hidden: 8 nbOfOutputs: 8.

data := {#(0 0 0 0).
      #(0 0 1 1).
      #(0 1000 0 2).
      #(0 1000 1 3).
      #(0.1 0 0 4).
      #(0.1 0 1 5).
      #(0.1 1000 0 6).
      #(0.1 1000 1 7) }.
n train: data nbEpochs: 10000.

```

图 [5-9](#Fig9) 显示精度达到`1.0`。由于归一化，所有输入值对于网络具有相同的相关性。因此，网络能够正确地学习。注意，在这个例子中，我们使用了线性归一化。非线性变换可能会改进学习，尤其是在训练数据中存在异常值的情况下。然而，我们认为非线性数据转换超出了本书的范围。请记住，如果您有一个包含相关异常值的数据集，您可能需要它。

## 5.12 我们在本章中看到了什么？

这一章就像一条探索数据操作不同方面的漫长道路。特别是，它探讨了:

*   监控网络学习的简单可视化方法

*   使网络对非数字数据进行操作的一键编码技术

*   Iris 数据集是应用神经网络对数据进行分类的完整示例

*   在处理数据之前对其进行规范化的相关性

我们邀请读者探索不同的数据集。 [`https://datasetsearch.research.google.com`](https://datasetsearch.research.google.com) 网站包括许多与神经网络或任何其他机器学习算法一起使用的相关数据集。