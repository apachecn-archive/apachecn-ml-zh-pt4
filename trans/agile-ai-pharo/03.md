# 3.神经网络

前一章介绍了单个神经元的设计和实现。本章通过连接多个神经元建立在前几章开始的努力的基础上。我们提供了一个神经网络和反向传播算法的完整实现，这将我们带到本书第一部分的核心。

## 3.1 总体架构

人工神经网络是一种受动物大脑中发现的生物神经网络启发的计算系统。人工神经网络是连接的人工神经元的集合。人工神经元之间的每个连接都可以将信号从一个传递到另一个。接收信号的人工神经元可以对其进行处理，然后向与之相连的神经元发出信号。人工神经网络通常用于执行特定的任务，包括聚类、分类、预测和模式识别。在神经网络中，就像感知器和 sigmoid 神经元一样，知识是通过学习获得的。

![../images/484203_1_En_3_Chapter/484203_1_En_3_Fig1_HTML.jpg](../images/484203_1_En_3_Chapter/484203_1_En_3_Fig1_HTML.jpg)

图 3-1

神经网络的例子

图 [3-1](#Fig1) 显示了一个由五个神经元、三个输入和两个输出组成的简单神经网络。最左边的一列称为*输入层。输入层只是将一些值传输到隐藏层，不做任何特别的事情。在图 [3-1](#Fig1) 中，输入层由三个输入组成— `x1`、`x2`和`x3`。网络的中间包含隐藏层。这个网络只包含一个隐藏层，由三个神经元组成。然而，网络可能包含几个隐藏层。网络最右边的一列称为*输出层，*，它包含两个神经元。*

神经元之间传递的所有值都是数值。到目前为止，我们大部分时间都在处理数字`0`和`1`。然而，乙状结肠神经元接受并产生浮动值。输出值`o1`和`o2`是介于`0`和`1`之间的数字。由于我们考虑的所有神经元都具有 sigmoid 激活函数，因此只有范围在`0`和`1`之间的值在神经元层之间传输。

所描绘的神经网络被称为*全连接*，因为隐藏层的每个神经元都连接到*所有*输入层的神经元和*所有*输出层的神经元。这样的网络对应于最简单的架构。更复杂的架构包括递归神经网络和卷积神经网络，这不在本书讨论范围之内。

本章提供了我们非正式提出的抽象的实现。下一章将揭示全连接网络的一些理论方面。

## 3.2 神经层

我们将层定义为一组神经元。层与层之间相互连接，一组层形成一个神经网络。我们将用`NeuronLayer`类表示一个层。

每一层使用`previousLayer`变量了解前一层，使用`nextLayer`了解后一层。`learningRate`变量是指层的学习率。我们将`NeuronLayer`类定义如下:

```py
Object subclass: #NeuronLayer
        instanceVariableNames: 'previousLayer nextLayer neurons'
        classVariableNames: ''
        package: 'NeuralNetwork'

```

一层包含一些神经元，保存在`neurons`变量中。我们可以默认设置一层的学习速率为`0.1`。可以使用以下方法初始化神经元层:

```py
NeuronLayer>>initializeNbOfNeurons: nbOfNeurons nbOfWeights:
      nbOfWeights using: random
       "Main method to initialize a neuron layer
       nbOfNeurons : number of neurons the layer should be made of
       nbOfWeights : number of weights each neuron should have
       random : a random number generator

       "|
       weights |
       neurons := (1 to: nbOfNeurons) collect: [ :i |
         weights := (1 to: nbOfWeights) collect: [ :ii | random next * 4
- 2 ].
         Neuron new sigmoid; weights: weights; bias: (random next * 4 -
2) ].
       self learningRate: 0.1

```

`initializeNbOfNeurons` : `nbOfWeights` : `using`:方法接受三个参数。第一个，`nbOfNeurons`，是一个整数值，代表层应该包含的神经元数量。第二个参数`nbOfWeights`是一个整数，表示每个神经元应该拥有的权重数。权重的数量反映了图层接受的输入值的数量。最后一个参数`random`，是一个随机数生成器。如前一章所述，使用随机数生成器有助于确定行为。这个随机数发生器用于初始化每个单独的神经元。

该方法首先创建`nbOfNeurons`个不同的神经元，每个神经元具有`nbOfWeights`个权重值。每个权重都是一个介于`2`和`+2`之间的随机数。这些边界是任意选择的。表达式`random next`在`0`和`1`之间产生一个随机数。乘以 4 再减去 2 得到一个在`-2`和`+2`之间的值。由于`sigmoid`消息，每个神经元都有一个 sigmoid 激活功能。

最后，该方法将每个神经元的学习速率设置为`0.1`。`learningRate:`方法定义如下:

```py
NeuronLayer>>learningRate: aLearningRate
        "Set the learning rate for all the neurons
        Note that this method should be called after configuring the
              network, and _not_ before"
        self assert: [ neurons notEmpty ] description: 'learningRate:
              should be invoked after configuring the layer'.
        neurons do: [ : n | n learningRate: aLearningRate ]

```

向前馈送层是一个基本操作，包括馈送每个神经元并将值转发到下一层。我们将`feed:`方法定义如下:

```py
NeuronLayer>>feed: someInputValues
        "Feed the neuron layer with some inputs"

        | someOutputs |
        someOutputs := neurons collect: [ :n | n feed: someInputValues ] as
                : Array.
        ^ self isOutputLayer
               ifTrue: [ someOutputs ]
               ifFalse: [ nextLayer feed: someOutputs ]

```

该方法在它的每个神经元上调用`feed:`(在前一章中详细描述了`Neuron>>feed:`方法)。然后结果被保存为一个数组。然后，该方法检查该图层是否为输出图层。如果是这种情况，该方法的结果仅仅是每个神经元的结果。如果该层不是输出(即，它是隐藏层)，我们将计算值前馈到下一层。

我们需要确定一个神经元层是否是输出层。我们可以使用`isOutputLayer`谓词轻松做到这一点:

```py
NeuronLayer>>isOutputLayer
        "Return true if the layer is the output layer (i.e., the last layer
, right-most, in the network)"
        ^ self nextLayer isNil

```

我们还需要一种方法将各层连接在一起:

```py
NeuronLayer>>nextLayer: aLayer
        "Set the next layer"
        nextLayer := aLayer

```

要访问下一层，我们需要以下方法:

```py
NeuronLayer>>nextLayer
        "Return the next layer connected to me"
        ^ nextLayer

```

同样，我们需要一种方法来设置和访问前一层:

```py
NeuronLayer>>previousLayer: aLayer
        "Set the previous layer"
        previousLayer := aLayer

```

类似地:

```py
NeuronLayer>>previousLayer
        "Return the previous layer connected to me"
        ^ previousLayer

```

需要从一个层访问神经元:

```py
NeuronLayer>>neurons
        "Return the neurons I am composed of"
        ^ neurons

```

我们还需要层的大小是可访问的:

```py
NeuronLayer>>numberOfNeurons
        "Return the number of neurons in the layer"
        ^ neurons size

```

我们现在已经定义了大多数的`NeuronLayer`类。我们可以开始测试这个类了:

```py
TestCase subclass: #NeuronLayerTest
        instanceVariableNames: ''
        classVariableNames: ''
        package: 'NeuralNetwork'

```

一个简单的测试可能是:

```py
NeuronLayerTest>>testBasic
        | nl result r |
        r := Random seed: 42.
        nl := NeuronLayer new.
        nl initializeNbOfNeurons: 3 nbOfWeights: 4 using: r.

        self assert: nl isOutputLayer.

        result := nl feed: #(1 2 3 4).
        self assert: result size equals: 3.
        result
               with: #(0.03700050130978758 0.9051275824569505
                   0.9815269659126287)
               do: [ :res :test | self assert: (res closeTo: test precision:
                   0.0000000001) ]

```

`testBasic`方法创建了一个新的神经元层，由三个神经元组成，每个神经元有四个权重和一个偏差。使用随机数生成器`r`初始化权重和偏差。

我们还可以构建一系列层，看看它们的表现如何:

```py
NeuronLayerTest>>testOutputLayer
        | nl1 nl2 result random |
        random := Random seed: 42.
        nl1 := NeuronLayer new.
        nl1 initializeNbOfNeurons: 3 nbOfWeights: 4 using: random.
        nl2 := NeuronLayer new.
        nl2 initializeNbOfNeurons: 4 nbOfWeights: 3 using: random.
        nl1 nextLayer: nl2.
        self deny: nl1 isOutputLayer.
        self assert: nl2 isOutputLayer.
        result := nl1 feed: #(1 2 3 4).
        "Since nl2 has 4 neurons, we will obtain 4 outputs"
        self assert: result size equals: 4.
        result
               with: #(0.03089402289518759 0.9220488835263312
                   0.5200462953493654 0.20276557516858304)
               do: [ :r :test | self assert: (r closeTo: test precision

:
                   0.0000000001) ]

```

我们现在可以将一系列层包裹成一个神经网络。

## 3.3 神经网络建模

我们将把神经网络表示为`NNetwork`类的一个实例:

```py
Object subclass: #NNetwork
        instanceVariableNames: 'layers errors precisions'
        classVariableNames: ''
        package: 'NeuralNetwork'

```

我们将神经网络简单地定义为多层的容器。我们还添加了一个`errors`实例变量，它将有助于在学习阶段跟踪错误的演变。

网络的初始化通过`initialize`方法完成:

```py
NNetwork>>initialize
        super initialize.
        layers := OrderedCollection new.
        errors := OrderedCollection new.
        precisions := OrderedCollection new.

```

用空集合初始化`layers`、`errors`和`precisions`实例变量。`layers`变量将引用`NeuronLayer`类的一个实例。`errors`和`precisions`变量将包含数值，代表训练过程中的误差和精度。我们将在下一章中对数据进行分类时利用这些变量。

添加层只需通过`addLayer:`方法完成，该方法将层作为一个参数:

```py
NNetwork>>addLayer: aNeuronLayer
         "Add a neural layer. The added layer is linked to the already added
                 layers."
         layers ifNotEmpty: [
                 aNeuronLayer previousLayer: layers last.
                 layers last nextLayer: aNeuronLayer ].
         layers add: aNeuronLayer.

```

各层相互链接。当添加一个层时，它链接到前一个层，该层链接到添加的层。

馈送神经网络包括简单地馈送第一隐藏层:

```py
NNetwork>>feed: someInputValues
         "Feed the first layer with the provided inputs"
         ^ layers first feed: someInputValues

```

我们需要一种容易创建神经网络的方法。如果我们要构建一个具有一个隐藏层和一个输出层的网络，我们可以定义以下方法:

```py
NNetwork>>configure: nbOfInputs hidden: nbOfNeurons nbOfOutputs:
        nbOfOutput
        "Configure the network with the given parameters
        The network has only one hidden layer"
        | random |
        random := Random seed: 42.
        self addLayer: (NeuronLayer new initializeNbOfNeurons: nbOfNeurons
             nbOfWeights: nbOfInputs using: random).
        self addLayer: (NeuronLayer new initializeNbOfNeurons: nbOfOutput
             nbOfWeights: nbOfNeurons using: random).

```

如果我们想要两个隐藏层和一个输出层，我们定义如下:

```py
NNetwork>>configure: nbOfInputs hidden: nbOfNeurons1 hidden:
       nbOfNeurons2 nbOfOutputs: nbOfOutput
       "Configure the network with the given parameters
       The network has only one hidden layer"
       | random |
       random := Random seed: 42.
       self addLayer: (NeuronLayer new initializeNbOfNeurons: nbOfNeurons1
             nbOfWeights: nbOfInputs using: random).
       self addLayer: (NeuronLayer new initializeNbOfNeurons: nbOfNeurons2
             nbOfWeights: nbOfNeurons1 using: random).
       self addLayer: (NeuronLayer new initializeNbOfNeurons: nbOfOutput
             nbOfWeights: nbOfNeurons2 using: random).

```

我们还需要一种方法来获得神经网络可以拥有的输出数量(我们将在关于数据分类的章节中用到这一点):

```py
NNetwork>>numberOfOutputs
         "Return the number of output of the network"
         ^ layers last numberOfNeurons

```

`NNetwork`类定义了`learningRate:`方法来设置每一层的学习速率:

```py
NNetwork>>learningRate: aLearningRate
        "Set the learning rate for all the layers"
        layers do: [ :l | l learningRate: aLearningRate ]

```

`learningRate:`方法对于为我们网络中的所有神经元设置一个唯一的学习速率是有用的。现在已经定义了基本功能。我们可以测试我们的网络实现，如下所示:

```py
TestCase subclass: #NNetworkTest
        instanceVariableNames: ''
        classVariableNames: ''
        package: 'NeuralNetwork'

```

我们的第一个测试可能如下:

```py
NNetworkTest>>testBasic
         | n |
         n := NNetwork new.
         n configure: 2 hidden: 2 nbOfOutputs: 1.
         self assert: ((n feed: #(1 3)) anyOne closeTo: 0.6745388083637036
             precision: 0.0000000001).
         self assert: n numberOfOutputs equals: 1

```

如你所见，`testBasic`相当简单。它构建了一个需要两个输入的简单网络。此外，它还包括一个由两个神经元组成的隐含层和一个只有一个神经元的输出层。然后，测试运行正向馈送。

到目前为止，这个网络相当无用，因为它只能沿着一组随机初始化的神经元前馈一些值。因此，输出是随机值。下一节将介绍神经网络的学习机制。

## 3.4 反向传播

*反向传播*是一种通常用于训练神经网络的算法。反向传播算法的目的是找到一组神经元权重和偏差，以减少网络预测误差。

到目前为止，我们建立了一个由一组神经元组成的网络，每个神经元都用随机权重和随机偏差进行初始化。从概念上讲，反向传播是一种梯度下降的监督学习算法(接下来的几章将讨论这个术语)。在实践中，该算法将找到足够的权重和偏差，以从输入值中识别模式。这一节着重于非正式地介绍算法并提供它的实现。下一章将提供算法的理论基础。这一章主要涵盖了这一理论的实现。

反向传播算法由三个步骤组成:

1.  *正向馈送输入*。我们首先激活我们网络的每个神经元，使网络产生一个输出。正如我们之前看到的，这种前馈从最左边的层到输出层。

2.  *通过网络*反向传播错误。必须将上一步中产生的输出与实际的训练数据集进行比较。因此，我们可以计算网络造成的误差。这个错误是确定我们的网络距离正确预测训练集有多远的关键。这种反向传播从最右边的层(即输出层)到最左边的层(即第一隐藏层)。

3.  *更新神经元权重和偏差*。根据上一步计算的误差，我们充分调整每个神经元的权重和偏差，以有希望减少网络造成的误差。在我们的实现中，我们将从最左边的层开始，到输出层。

### 3.4.1 第一步:向前进给

第一步大部分由`NNetwork>>feed:`方法实现；然而，我们需要稍微改进一下`Neuron`类来记住产生的输出。在前向馈送期间(即当调用`feed:`方法时)，输出由每个神经元产生。在第二步中，必须将该输出与预期输出进行比较。让网络学习是基于神经元的实际输出和预期输出之间的差异。每个神经元必须保留其输出的参考。

我们在`Neuron`类中添加了两个变量`delta`和`output`。因此，我们对`Neuron`的新定义如下:

```py
Object subclass: #Neuron
        instanceVariableNames: 'weights bias learningRate
             activationFunction delta output'

        classVariableNames: ''
        package: 'NeuralNetwork'

```

增量值必须可以从外部访问，如下所示:

```py
Neuron>>delta
        "Return the delta value computed when propagating the error"
        ^ delta

```

我们还需要重写`Neuron`类中的`feed:`方法来记住输出值，如下所示:

```py
Neuron>>feed: inputs
        | z |
        z := (inputs with: weights collect: [ :x :w | x * w ]) sum + bias.
        output := activationFunction eval: z.
        ^ output

```

我们还需要访问给定神经元的输出值，如下所示:

```py
Neuron>>output
        "Return the output value, previous computed when doing a feed:"
        ^ output

```

在这个阶段，运行我们之前定义的单元测试是很重要的。特别是，我们需要确保我们在`Neuron`类中定义的小变化不会破坏任何不变量。我们现在完成了反向传播的第一阶段。

*练习:*运行前一章中编写的单元测试。这对于验证一个函数不变量是否受到最近这些修改的影响是很重要的。

### 3.4.2 步骤 2:错误反向传播

反向传播的第二步包括将输出层中计算的误差传播回网络。我们定义以下方法:

```py
NNetwork>>backwardPropagateError: expectedOutputs
         "expectedOutputs corresponds to the outputs we are training the
              network against"
         self outputLayer backwardPropagateError: expectedOutputs

```

`backwardPropagateError:`的参数对应于学习阶段使用的预期输出值。

我们还定义了以下助手方法:

```py
NNetwork>>outputLayer
         "Return the output layer, which is also the last layer"
         ^ layers last

```

我们添加了`backwardPropagateError:`方法来从输出层反向传播误差:

```py
NeuronLayer>>backwardPropagateError: expected
        "This is a recursive method. The backpropagation begins with
            the output layer (i.e., the last layer)"
        "We are in the output layer"
        neurons with: expected do: [ :neuron :exp |
                | theError |
                theError := exp - neuron output.
                neuron adjustDeltaWith: theError ].

        "We iterate"
        self previousLayer notNil
              ifTrue: [
                     self previousLayer backwardPropagateError ].

```

`backwardPropagateError:`方法将预期的输出值作为参数。它计算输出层中每个神经元的误差，并调用`adjustDeltaWith:`方法。我们很快就会看到这种方法。

一旦输出层中的神经元的 delta 值被调整，先前的层必须被递归地更新。`backwardPropagateError`方法实现了这种行为:

```py
NeuronLayer>>backwardPropagateError
        "This is a recursive method. The backpropagation begins with the
             output layer (i.e., the last layer)"

        "We are in a hidden layer"
        neurons doWithIndex: [ :neuron :j |
               | theError |
               theError := 0.0.
               self nextLayer neurons do: [ :nextNeuron |
                   theError := theError + ((nextNeuron weights at: j) *
                   nextNeuron delta)
              ].
              neuron adjustDeltaWith: theError
        ].

        self previousLayer notNil
              ifTrue: [
                     self previousLayer backwardPropagateError ].

```

递归在第一个隐藏层结束，该层没有前一层。请注意，我们没有显式地对输入层建模，因为它没有任何用途。我们还需要下面的`Neuron`类的帮助器方法:

```py
Neuron>>adjustDeltaWith: anError
       delta := anError * (activationFunction derivative: output)

```

我们现在已经完成了第二阶段。为了创建功能性神经网络，只剩下第三阶段需要实施。

### 3.4.3 步骤 3:更新神经元参数

幸运的是，第三阶段相当简单。我们基于上一步计算的增量递归更新权重和偏差。主要方法是`updateWeight:`，如下:

```py
NNetwork>>updateWeight: initialInputs
        "Update the weights of the neurons using the initial inputs"
        layers first updateWeight: initialInputs

```

该方法简单地在每个第一隐藏层上调用`updateWeight`；

```py
NeuronLayer>>updateWeight: initialInputs
       "Update the weights of the neuron based on the set of initial input
. This method assumes that the receiver of the message invoking
              that method is the first hidden layer."
       | inputs |
       inputs := initialInputs.

       neurons do: [ :n |
              n adjustWeightWithInput: inputs.
              n adjustBias ].

       self nextLayer ifNotNil: [
              self nextLayer updateWeight ]

```

递归发生在`updateWeight`方法中:

```py
NeuronLayer>>updateWeight
        "Update the weights of the neuron based on the set of initial
                input. This method assumes that the receiver of the
                message invoking that method is the first hidden layer.
        We are now in the second hidden layers or in the output layer"
        | inputs |
        inputs := self previousLayer neurons collect: #output.

        self updateWeight: inputs

```

我们需要以下方法来更新神经元的权重:

```py
Neuron>>adjustWeightWithInput: inputs
        inputs withIndexDo: [ :anInput :index |
                weights at: index put: ((weights at: index) + (learningRate *
                     delta * anInput)) ]

```

我们还需要更新偏差，如下所示:

```py
Neuron>>adjustBias
        bias := bias + (learningRate * delta)

```

这结束了反向传播算法的第三和最后阶段。我们现在准备将反向传播阶段联系在一起:

```py
NNetwork>>train: someInputs desiredOutputs: desiredOutputs
        "Train the neural network with a set of inputs and some
              expected output"
        self feed: someInputs.
        self backwardPropagateError: desiredOutputs.
        self updateWeight: someInputs

```

瞧啊。我们已经实现了训练神经网络的必要步骤。

我们现在可以用 XOR 示例测试网络:

```py
NNetworkTest>>testXOR
        | n |
        n := NNetwork new.
        n configure: 2 hidden: 3 nbOfOutputs: 1.

        20000 timesRepeat: [
                n train: #(0 0) desiredOutputs: #(0).
                n train: #(0 1) desiredOutputs: #(1).
                n train: #(1 0) desiredOutputs: #(1).
                n train: #(1 1) desiredOutputs: #(0).
        ].

        self assert: (n feed: #(0 0)) first < 0.1.
        self assert: (n feed: #(0 1)) first > 0.9.
        self assert: (n feed: #(1 0)) first > 0.9.
        self assert: (n feed: #(1 1)) first < 0.1.

```

如果您试图将`20000`降低到一个较低的值，例如`1000`，网络将无法接受足够的训练，测试将最终失败。

## 3.5 我们在本章中看到了什么？

本章涵盖了以下主题:

*   *全连接网络的一般架构*。这种架构推动了我们的实现工作。

*   *神经网络库的实现*。我们构建了一个小 API 来构建神经网络。

*   *反向传播算法*的实现。使神经网络学习是赋予网络意义的基本操作。经过适当训练的网络可以识别模式。本章以一个微不足道的例子 XOR 逻辑门——结束。接下来的章节将涵盖真实的和有代表性的例子。