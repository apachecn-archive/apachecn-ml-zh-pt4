第六章

![image](../images/00004.jpeg)

树木和森林

Making Predictions from Incomplete Data

我侄女最喜欢的游戏之一是猜谜游戏。一个人想到了什么，另一个玩家试图通过只问是或否的问题来找出这个东西是什么。如果你以前玩过这个游戏，你可能已经看到了下面的模式:首先提出问题，排除大类可能的答案，例如“它是动物吗？”，并随着您收集的信息越来越多，逐步缩小问题的范围。这是一个比一开始就问“你想过斑马吗？”更有效的策略一方面，如果答案是“是”，你就完了，因为只有一个可能的答案。另一方面，如果答案是“不”，你就处于一个相当糟糕的境地，基本上什么也没学到。

决策树是分类器，其操作方式类似于那个游戏。它们模仿我们人类做出诊断的方式，通过问一系列问题，试图快速消除不好的答案，并根据我们迄今为止学到的一切决定下一步问什么问题。在这种情况下，特征可以被认为是关于观察的特征的问题。决策树将从训练集中学习，分析它以确定哪个特征提供了最多的信息，给定在该点关于它试图分类的观察的所有已知信息。

在这一章中，我们将处理一个经典数据集，并逐步学习越来越强大的分类器，从决策树桩开始，扩展到决策树和森林。在此过程中，我们将执行以下操作:

*   了解如何构建灵活的决策树，可以处理几乎任何类型的数据，包括数据丢失的情况。我们还将看到如何使用 F# 类型，比如有区别的联合和选项，来非常有效地建模树状结构。
*   引入熵作为度量数据集包含多少信息的方法，并使用它来比较特征并决定应该选择哪些特征。
*   讨论过度拟合的风险，以及解决它的方法。特别是，我们将探索更先进的交叉验证方法，如 k-folds，以及相同的思想如何导致集成方法，一种将多个简单预测器聚合为一个稳健预测器的通用方法。

我们的挑战:在泰坦尼克号上沉浮

我们将在本章中使用的数据集是经典的。利用登上泰坦尼克号的乘客名单，以及他们每个人的一些人口统计信息，我们的目标是预测他们的命运。出于几个原因，数据集和问题都很有趣。首先，手头的问题是一大类问题中的典型问题，一旦你超越了它有些可怕的本质。"访问者会点击网站上这个链接吗？"或者“哪些客户会选择小订单、中订单或大订单？”本质上是同一个问题:利用你所拥有的关于个人的信息和他们过去的行为，你试图建立一个分类模型来从有限的一组可能的结果中预测一个结果。

然后，数据本身是真实世界的典型情况，因为它是混乱的。它混合了不同类型的特征，从数字(乘客为他们的票付了多少钱？)到分类(乘客是男是女？)，以及中间的所有内容(乘客姓名和头衔)。一些数据也丢失了。理想情况下，我们想要一个足够灵活的分类器来处理所有这些变量，而没有太多的麻烦。

最后，由于显而易见的历史原因，这些数据本身也很有趣。随着时间的推移，泰坦尼克号的灾难吸引了人们，尽管为近一个世纪前发生的一次性事故创建预测模型几乎没有实际用途，但数据集提供了一个机会，看看人们可以通过分析数据获得什么样的洞察力。

了解数据集

我们将使用的具体数据集来自于 Kaggle 竞赛《泰坦尼克号:机器学习从灾难中:【https://www.kaggle.com/c/titanic-gettingStarted】T2。变种可以很容易地在网上找到(例如，【http://www.encyclopedia-titanica.org】)。然而，我们认为使用 Kaggle 参考数据集会很有趣；这样，你可以试试你的技能，看看你的进展如何。为了方便起见，我们在 OneDrive 上上传了相同的数据集:[http://1drv.ms/1HUh1ny](http://1drv.ms/1HUh1ny)。

数据集是一个 CSV 文件， titanic.csv，用一个标题行很好地组织起来，描述了 12 个可用列中的每一个。当然，我们可以写一些代码来解析它，但是为什么不使用 CSV 类型提供者呢？让我们使用 F#项目 Titanic 创建一个解决方案，将 titanic.csv 文件添加到该项目中，并将 fsharp.data NuGet 包添加到该项目中。

我们现在准备开始探索数据。在 script.fsx 文件中，我们添加一个对 FSharp 的引用。数据(路径的细节可能会有所不同，这取决于当前最新版本的包)，通过查看我们的示例数据创建一个 Titanic 类型，并添加一个类型别名 Passenger，以便我们可以以更直观的方式引用数据集的每一行。我们现在可以读取数据集了。

***清单 6-1*** 。使用 CSV 类型提供程序读取 Titanic 数据集

```
#r @".\packages\FSharp.Data.2.2.2\lib\net40\FSharp.Data.dll"
open FSharp.Data

type Titanic = CsvProvider<"titanic.csv">
type Passenger = Titanic.Row

let dataset = Titanic.GetSample ()
```

CSV 类型提供者为我们创建了一个很好的类型，带有完整的属性，让我们在浏览数据时可以享受类型的所有好处。我们现在可以开始工作了。如果我们的目标是预测谁生存或死亡，第一步是获得该值的基线。让我们首先计算每个类别中有多少乘客，然后通过将以下代码添加到我们的脚本中并在 F# Interactive 中运行它，计算出每个乘客幸存的概率:

```
dataset.Rows
|> Seq.countBy (fun passenger -> passenger.Survived)
|> Seq.iter (printfn "%A")

dataset.Rows
|> Seq.averageBy (fun passenger ->
    if passenger.Survived then 1.0 else 0.0)
|> printfn "Chances of survival: %.3f"

>
(false, 549)
(true, 342)
Chances of survival: 0.384
```

我们有 891 名乘客，其中只有 342 人幸存；换句话说，在没有任何额外信息的情况下，每位乘客只有 38.4%的生还机会(或者，相当于 61.6%的机会无法活着离开飞船)。如果我们想改进预测，我们需要找到一些特征来帮助我们识别幸存几率超过 50%的乘客。其他任何东西，虽然信息丰富，但不会直接对我们的预测产生实质性的影响，因为它不会改变我们在默认情况下的决定。

看一看特征

考虑到这个框架，让我们开始看看特性。我们在这里寻找的是一种快速的方法，将乘客分成不同的组，然后计算每组的存活率。计算存活率很简单:给定一个乘客样本，简单地计算幸存乘客的比率，除以样本中的乘客总数。然后我们可以简单地根据不同的标准将乘客分组——比如说，性别或阶级——并显示每组的存活率。

***清单 6-2*** 。计算不同组的存活率

```
let survivalRate (passengers:Passenger seq) =
    let total = passengers |> Seq.length
    let survivors =
        passengers
        |> Seq.filter (fun p -> p.Survived)
        |> Seq.length
    100.0 * (float survivors / float total)

let bySex =
    dataset.Rows
    |> Seq.groupBy(fun p -> p.Sex)

bySex
|> Seq.iter (fun (s,g) ->
    printfn "Sex %A: %f" s (survivalRate g))

let byClass =
    dataset.Rows
    |> Seq.groupBy (fun p -> p.Pclass)

byClass
|> Seq.iter (fun (s,g) ->
    printfn "Class %A: %f" s (survivalRate g))
>
Sex "male": 18.890815
Sex "female": 74.203822
Class 3: 24.236253
Class 1: 62.962963
Class 2: 47.282609
```

很明显，这两个特征都提供了丰富的信息。事实证明，有 74%的生还机会，成为泰坦尼克号上的一位女士是一个非常好的主意。类似地，头等舱乘客有相当大的机会活着离开(63%)，而三等舱可怜的 sods 的前景更加暗淡，有 24%的机会幸存。套用乔治·奥威尔的话，“所有乘客都是平等的，但有些乘客比其他人更平等。”这里有一些相当明显的社会学解释，但我会把它留给你:毕竟，数据显示的是什么，但没有说为什么是这样。

构建决策树桩

这些信息足以让我们实现一个超级简单的分类器，以及训练它的程序。获取样本数据，按照我们刚才所做的将样本分成不同的组，并为每组预测最常见的标签。这种类型的模型被称为决策树桩，是你能建立的最短的树。它是树的构造块，是最小的分类器，通常在其他模型中作为构造块出现。

例如，我们如何根据乘客的性别来建立一个树桩呢？首先，我将从我的样本中抽取所有乘客，并根据我选择的特征的值将他们分组，在本例中，是他们的性别值(“男性”或“女性”)。然后，对于每个组，我会查找我选择的标签的值，在我们的情况下，它们是否幸存，并确定每个组中最频繁出现的标签。然后我们可以建立一个分类器(stump):取一个乘客作为输入，计算特征值，在组中查找该值，并返回该组最频繁出现的标签(见[图 6-1](#Fig1) )。

![9781430267676_Fig06-01.jpg](../images/00127.jpeg)

[图 6-1](#_Fig1) 。以性别为特征构建生存决策树桩

![Image](../images/00011.jpeg) **注意**在前面的章节中，我们用术语*类*来描述一个分类器的预测。这里我们就用*标签*代替，这也是常见的。在我们的例子中，这很方便，因为乘客类是我们数据集的一个特性，对两者都使用类会引入一些潜在的歧义。

如果我们将特征和标签都放入从观察中提取值的函数中，我们实际上可以使这个过程完全通用。假设我拥有的不是乘客，而是一系列一般类型为“a”的项目；那么一个特征就是一个函数，当给定一个‘a’时，返回值‘b ’;类似地，标签接受一个“a”并返回给你另一个类型“c”。在我们的特殊情况下，“a”将是一个乘客，“b”是一个字符串(接受值“男性”或“女性”)，而“c”是一个布尔值(幸存，或不)。对我们的类型的唯一附加要求是‘b’和‘c’需要支持相等，因为我们需要根据特性和标签值进行分组和计数。

那么，我们的实现看起来怎么样呢？就像我们描述的那样:

[***清单 6-3***](#_list3) 。学习决策树桩

```
let mostFrequentLabelIn group =
    group
    |> Seq.countBy snd
    |> Seq.maxBy snd
    |> fst

let learn sample extractFeature extractLabel =
    // group together observations that have the
    // same value for the selected feature, and
    // find the most frequent label by group.
    let groups =
        sample
        |> Seq.map (fun obs -> extractFeature obs, extractLabel obs)
        |> Seq.groupBy fst
        |> Seq.map (fun (feat,group) -> feat, mostFrequentLabelIn group)
    // for an observation, find the group with
    // matching feature value, and predict the
    // most frequent label for that group.
    let classifier obs =
        let featureValue = extractFeature obs
        groups
        |> Seq.find (fun (f,_) -> f = featureValue)
        |> snd
    classifier
```

注意，很大程度上出于说明的目的，我没有对清单 6-3 中的两个函数添加任何类型注释。如果您在 F# Interactive 中运行代码，或者简单地将鼠标悬停在学习定义上，您将看到以下内容:

```
val learn :
  sample:seq<'a> ->
    extractFeature:('a -> 'b) -> extractLabel:('a -> 'c) -> ('a -> 'c)
    when 'b : equality and 'c : equality
```

这有点吓人，所以我们来解析一下。这里说的是，学习是一个函数，当给定一个观察样本(一个' a '序列)，一个从观察' a 中提取特征值' b 的函数(' a -> 'b)，一个从观察' a 中提取标签值' c 的函数(' a -> 'c)，将返回一个函数' a -> 'c，一个接受观察并返回标签的函数，这正是我们对分类器的期望。如果这个函数签名的所有细节都不清楚，不要担心，因为它不是非常重要。我提到它的原因是它说明了 F#类型推理系统的强大。我们只描述了我们希望如何处理输入，没有太多的说明，F#正确地创建了一个具有泛型类型的函数，甚至推断出我们需要支持提取的标签和特征值相等。

训练树桩

我们如何着手训练一个树桩？我们需要为标注和要素创建一个规范，并将它们定义为适合数据集的函数。调用 learn 将创建一个功能完备的分类器，我们可以立即运行并评估它(暂时忽略交叉验证)。

***清单 6-4*** 。训练和评估树桩

```
let survived (p:Passenger) = p.Survived
let sex (p:Passenger) = p.Sex

let sexClassifier = survived |> learn (dataset.Rows) sex

printfn "Stump: classify based on passenger sex."
dataset.Rows
|> Seq.averageBy (fun p ->
    if p.Survived = sexClassifier p then 1.0 else 0.0)

>
Stump: classify based on passenger sex.
val it : float = 0.7867564534
```

在整个样本上运行，我们得到了 78.6%的正确预测，明显好于我们最初的 61.6%。我们同样可以根据乘客等级轻松创建一个模型，如下所示:

***清单 6-5*** 。多叉树桩

```
let classClassifier = survived |> learn (dataset.Rows) (fun p -> p.Pclass)
dataset.Rows
|> Seq.averageBy (fun p ->
    if p.Survived = classClassifier p then 1.0 else 0.0)
```

正如预期的那样，有 67.9%的正确率，这是对没有模型的一个改进，尽管比我们在前面的案例中观察到的更适度。不过，我运行这个例子的原因并不是为了提高性能。它说明了两个有趣的方面。首先，与第一个例子不同，这个树桩有三个分支，而不仅仅是两个。一个树桩(以及随后的决策树)可以容纳不止二进制选择。然后，注意我们的两个特性有不同的类型:Sex 返回一个字符串，class 返回整数。类型推理系统自动为我们处理了这个问题。

不适合的功能

显然，这样一个简单的模型，仅使用一个特征来进行预测，只能带我们走这么远，在这一点上显而易见的问题是，“我们如何将多个特征结合到一个单一的模型中？”这就是我们接下来要做的，尝试通过整合更多的数据集信息和组合多个树桩来形成决策树，从而改进我们的预测。然而，在研究更复杂的模型之前，我们需要解决几个问题。我们的决策树桩方法相当灵活，但是它有一些限制。

数字呢？

在我们的特性中，考虑这个:费用。毕竟，如果乘客等级能提供信息，那么他们为机票支付的价格也很有可能是信息。让我们通过计算不同票价的存活率来验证一下。

***清单 6-6*** 。具有连续数值的特征

```
let survivalByPricePaid =
    dataset.Rows
    |> Seq.groupBy (fun p -> p.Fare)
    |> Seq.iter (fun (price,passengers) ->
        printfn "%6.2F: %6.2f" price (survivalRate passengers))
>
  7.25:   7.69
 71.28: 100.00
  7.93:  44.44
 53.10:  60.00
// snipped for brevity...
  5.00:   0.00
  9.85:   0.00
 10.52:   0.00
```

数字是正确的，但这似乎不是实现这一特性的好方法。该费用被分成 248 个不同的个案，其中大多数的存活率为 0%或 100%。如果我们遵循这个模型，为他们的机票支付 39.4000 英镑的乘客有 100%的生还机会；付了 39.6000 的人只有 50%；而那些付了 39.6875 的人活着的几率是 0%。这显然是愚蠢的。

我们这里的问题是，票价和，比如说，等级，不是同一个性质。等级采取离散状态(你在第一、第二或第三等级；没有其他选择)，而 fare 原则上可以取无限多的值。这两个特征的另一个不同之处是 fare 代表一个实际的数字；例如，您可以将两种票价加在一起，或者从另一种票价中减去一种，而乘客等级的总和没有任何意义。

这里的问题是我们的残肢只能处理分类特征；我们如何整合连续的数据？一种方法是将问题简化为已知问题，并将费用转换为分类特征。例如，我们可以创建一个新的功能，“票价水平”，根据票价是低于还是高于平均值，分为两个水平，便宜或昂贵。

***清单 6-7*** 。将费用转换成离散的箱

```
let averageFare =
    dataset.Rows
    |> Seq.averageBy (fun p -> p.Fare)

let fareLevel (p:Passenger) =
    if p.Fare < averageFare
    then "Cheap"
    else "Expensive"

printfn "Stump: classify based on fare level."
let fareClassifier = survived |> learn (dataset.Rows) fareLevel

dataset.Rows
|> Seq.averageBy (fun p ->
    if p.Survived = fareClassifier p then 1.0 else 0.0)

>
val it : float = 0.6621773288
```

结果不是很好，但这不是重点:我们现在有一种方法可以将连续的特征简化为离散的特征。这种方法被称为**离散化**，也是非常通用的方法:将特征分成任意数量的连续区间，并将观测值分配给一个“箱”

我们解决了一个问题，但也发现了另一个问题:在我们可以离散化连续特征的许多方法中，我们应该选择哪一种？这个问题可以归结为:给定两个特征(在这种情况下是同一基础特征的两个离散化)，我们应该选择哪一个来获得最多的信息？在我们处理完另一个问题——缺失数据——之后，我们将在下一节处理这个问题。

丢失数据怎么办？

让我们考虑一下我们可以使用的另一个特性。假设我们想在我们的模型中使用 apolloed(每个乘客的始发港)作为一个特性。我们按始发港来看存活率。

***清单 6-8*** 。具有缺失值的要素

```
let survivalByPortOfOrigin =
    dataset.Rows
    |> Seq.groupBy (fun p -> p.Embarked)
    |> Seq.iter (fun (port,passengers) ->
        printfn "%s: %f" port (survivalRate passengers))

>
S: 33.695652
C: 55.357143
Q: 38.961039
: 100.000000
```

显然，在法国瑟堡寄宿比在皇后镇或南安普敦寄宿有更好的生存机会。但是这个 100%存活率的匿名类别是什么？

它是什么，丢失的数据。对于每一位乘客，我们知道他们的性别和级别，但对于一些人，我们不知道他们在哪里登机。这个问题是真实的，也是很普遍的。不幸的是，在现实生活中，您将获得的大多数数据集都会有一些问题，丢失值是最有可能的问题。那么，我们该怎么办呢？一个诱人的选择是消除不完整的记录。但是，您最终可能会丢弃大量有价值的信息，尤其是当数据集中的多个要素缺少数据时。

然而，我们不能保留一个空字符串作为有效的装货港！最起码要明确标注这是缺失数据，以便以后决定如何处理。我们可以在这里创建一个专用的有区别的联合，但是当我们有可供选择的选项类型时，这看起来有些矫枉过正，有两种情况，一些或者没有。

在这个框架中，有一个选项(很抱歉是双关语)是稍微修改我们定义一个特性的方式，并要求它通过返回一个选项来明确定义丢失的值是什么，None 表示丢失的数据。因此，学习功能现在需要明确用于处理这种情况的策略。清单 6-9 中的代码片段展示了一种实现方式。与前一版本相比，主要的变化是我们从样本中过滤掉缺少值的例子，然后只为这些例子创建一个分支，为了方便起见，将它们存储为一个映射。在缺失值出现的情况下，我们将简单地预测样本中最常见的标签。

[***清单 6-9***](#_list9) 。纳入缺失的值

```
let hasData extractFeature = extractFeature >> Option.isSome
let betterLearn sample extractFeature extractLabel =
    let branches =
        sample
        |> Seq.filter (extractFeature |> hasData)
        |> Seq.map (fun obs -> extractFeature obs |> Option.get, extractLabel obs)
        |> Seq.groupBy fst
        |> Seq.map (fun (feat,group) -> feat, mostFrequentLabelIn group)
        |> Map.ofSeq
    let labelForMissingValues =
        sample
        |> Seq.countBy extractLabel
        |> Seq.maxBy snd
        |> fst
    let classifier obs =
        let featureValue = extractFeature obs
        match featureValue with
        | None -> labelForMissingValues
        | Some(value) ->
            match (branches.TryFind value) with
            | None -> labelForMissingValues
            | Some(predictedLabel) -> predictedLabel
    classifier

let port (p:Passenger) =
    if p.Embarked = "" then None
    else Some(p.Embarked)

let updatedClassifier = survived |> betterLearn (dataset.Rows) port
```

![Image](../images/00011.jpeg) **注意**分类器函数中选项结果的模式匹配形成了一个非常容易理解的逻辑流程。然而，过滤掉缺少值的例子，然后提取出实际的值，是很难看的。Seq 模块包含一个函数 Seq.choose，它一次性完成了这两项工作，但是要求原始序列包含选项，不幸的是，在我们的示例中并不是这样。

测量数据中的信息

我们现在有了一种方法，可以使用单个特征为总体创建分类器，而不管其类型或是否有缺失值。这很好，但是我们不能限制自己一次只使用一个特性，而忽略了我们所拥有的其他特性中潜在的可用信息。

我们将沿着与 20 个问题的游戏中相同的路线扩展我们的模型。我们将轮流进行；在每次转弯时，你可以从一系列特征中提出一个关于观察的问题:乘客是男是女？他旅行是坐头等舱、二等舱还是三等舱？根据这些问题的答案，你需要做一个决定:你有足够的信息来做一个预测吗(生存与否)，或者，给定你得到的答案(“乘客是一个男人”)，你想问另一个问题，可以提高你产生正确答案的几率吗？

这里的关键问题是如何衡量和比较信息，以便我们可以决定哪个问题序列是最好的。这就是我们将在本节中讨论的内容。

用熵测量不确定性

就其本身而言，一个特征不能提供信息；这些信息存在于你可能得到的可能答案中，以及它们如何帮助你决定哪个标签是最有可能的。为了衡量一个问题的信息量，我需要考虑如果我知道了答案，我会过得多好。如果我知道“乘客是一名男性”或“乘客乘坐头等舱”会更好吗？

现在，我们真正关心的不是乘客是男是女，而是，如果我告诉你他是男的，你有多大把握他能活下来？我能告诉你的最糟糕的事情是，“如果乘客是个男人，他有 50%的机会活下来。”这实际上等于说这些信息毫无价值。相反，如果男人要么有 100%的生存机会，要么相反，你就完了:这是完美的信息，没有其他问题可以改善这一点。

这里的直觉是，人口分布越均匀，我们拥有的信息就越少:没有一个猜测比另一个更好。信息论对此有一个确切的定义:熵(具体来说是香农熵)。如果你有一个样本群体，它的熵的测量方法是:

```
entropy(sample) = sum [ - p(x) * log p(x) ]
```

其中 p(x)是 x 在样本中所占的比例，即观察到一个值为 x 的项目的概率。

![Image](../images/00011.jpeg) **注**只要保持一致，用什么底数来计算对数并不重要。不同的基数产生不同的单位(比特、纳特等。)，但不会影响样品的订购方式。出于这个原因，我们将使用 log，自然对数，因为它是从 F#开始最容易使用的。

让我们用 F#来实现它，看看它是如何工作的。给定一个数据样本，我们将计算每个类别中有多少个元素，除以总数，并计算 p log p。注意，x log(x)对于 x = 0 是未定义的；我们用它的右极限零来代替它，最后得到如下结果:

[***清单 6-10***](#_list10) 。样本的香农熵

```
let entropy data =
    let size = data |> Seq.length
    data
    |> Seq.countBy id
    |> Seq.map (fun (_,count) -> float count / float size)
    |> Seq.sumBy (fun f -> if f > 0\. then - f * log f else 0.)
```

![Image](../images/00011.jpeg) **提示** F#内置了一个用于标识的函数 id，即一个将其输入作为输出返回的函数。这在像[清单 6-10](#list10) 这样的情况下会很有用，我们想在一个序列中找到唯一的元素，并计算每个元素有多少个。

让我们通过在 F# Interactive 中运行几个例子来快速探究熵是如何表现的:

```
> [1;1;1;2;2;2] |> entropy;;
val it : float = 0.6931471806
> [1;1;1;1;1;1] |> entropy;;
val it : float = 0.0
```

当样本信息最少时，熵最大(每个标签都被同等地表示)，对于组织完善(且可预测)的样本，熵向零递减。[图 6-2](#Fig2) 对比了几个具有非常不同特征的例子，并说明了熵是如何映射我们对顺序或纯度的直观概念的:我们的样本纯度越高，就越容易猜测，熵就越低。顺便提一下，熵也可以处理两个以上的标签，这使它成为一个方便的度量标准。

![9781430267676_Fig06-02.jpg](../images/00128.jpeg)

[图 6-2](#_Fig2) 。比较样本信息和熵

信息增益

香农熵为我们提供了一种衡量样本中信息有用程度的方法——熵越低，信息越好。这是有用的，但并不完全是我们所追求的。我们想要的如下:如果我们有一个样本，需要从两个问题(特性)中选择一个来询问它，我们应该选择哪个？

为了做出这个决定，我们将使用信息增益的概念。让我们从它的数学表达式开始，然后讨论为什么这有意义。假设你有一个样本，通过观察特征 F，你可以把这个样本分成 n 组:样本(1)，...，样本(n)。特征 F 的熵增益是

```
gain = entropy(sample) - (proba(sample(1)) * entropy(sample(1)) + ... proba(sample(n)) * entropy(sample(n)))
```

为什么这个公式有意义？第一部分，熵(样本)，简单地表示当前的熵——也就是说，我们关于原始样本的信息有多好。你可能已经意识到，第二部分是一个平均值，平均熵，我们将根据我们的特征来分割样本。

在开始实现它之前，让我们用几个图形例子来说明这个想法。在每个图像中，我们将考虑一个虚构的乘客样本，其中幸存者被渲染为白色，否则为黑色，乘客性别用经典的火星和金星符号表示。

在[图 6-3](#Fig3) 中，我们有理想的情况。我们从一个样本开始，这个样本对于预测来说是糟糕透顶的(一半是幸存者，一半是伤亡者)。如果我们按照性别来划分样本，我们最终会得到两个完美的群体，一个完全是幸存者，另一个全部死亡。在这种情况下，性是一个完美的特征；如果我们问乘客的性别，，我们会确切地知道他们的命运——熵增会尽可能高。

![9781430267676_Fig06-03.jpg](../images/00129.jpeg)

[图 6-3](#_Fig3) 。完美信息增益

相比之下，[图 6-4](#Fig4) 代表了最坏的情况。我们从一个没有信息的样本开始，在按性别划分后，我们有两个样本仍然没有信息。在这种情况下，性是完全无信息的，相应的信息增益为零。

![9781430267676_Fig06-04.jpg](../images/00130.jpeg)

[图 6-4](#_Fig4) 。零信息增益

请注意，熵增益是指我们通过分割一个特征(或提出一个问题)获得了多少信息，而不是熵本身。例如，[图 6-5](#Fig5) 显示了这样一种情况，在按性别划分后，我们得到了两个相当有用的样本(无论是男性还是女性，我们都可以以 75%的置信度对结果进行猜测)，但是与原来的情况相比没有任何改进，而且，就像前面的情况一样，熵也没有增加。

![9781430267676_Fig06-05.jpg](../images/00131.jpeg)

[图 6-5](#_Fig5) 。好的信息，但不是收益

熵增加的另一个重要因素是群体规模。考虑[图 6-6](#Fig6) ，它描绘了两种相似但略有不同的情况。在这两种情况下，按性别划分样本可以获得信息。但是，底部的情况比顶部好。在上面的例子中，如果我们知道乘客是男性，我们有完美的信息，但只有 25%的机会听到答案。相比之下，在第二种情况下，我们有 75%的机会知道乘客是女性，并获得完美的信息。使用相同的特征，我们在一种情况下会比在另一种情况下完成得快得多，并且该方面将再次通过构造信息增益的方式来反映，因为它将按组考虑熵，按每个组的大小加权。

![9781430267676_Fig06-06.jpg](../images/00132.jpeg)

[图 6-6](#_Fig6) 。按组的信息增益

换句话说，如果我们想要有效地了解标签，并且一次只能问一个问题，熵增益为我们提供了一个度量，通过这个度量，我们可以比较哪些特征使用起来最有利。

实施最佳特征识别

我们已经有了第一部分。如果我们根据给定的特征分割样本，我们唯一需要的就是平均熵。这相当简单，因为我们唯一需要注意的是样本中潜在的缺失数据。我们简单地过滤掉任何没有特征值的观察值，按特征值分组，并按组计算熵的加权平均值。

***清单 6-11*** 。特征分裂后的平均熵

```
let splitEntropy extractLabel extractFeature data =
    // observations with no missing values
    // for the selected feature
    let dataWithValues =
        data
        |> Seq.filter (extractFeature |> hasData)
    let size = dataWithValues |> Seq.length
    dataWithValues
    |> Seq.groupBy extractFeature
    |> Seq.sumBy (fun (_,group) ->
        let groupSize = group |> Seq.length
        let probaGroup = float groupSize / float size
        let groupEntropy = group |> entropy extractLabel
        probaGroup * groupEntropy)
```

让我们通过计算 F# interactive 中各种特性的信息增益来看看这是如何实现的。

***清单 6-12*** 。比较不同特征的信息增益

```
let survived (p:Passenger) = p.Survived
let sex (p:Passenger) = Some(p.Sex)
let pclass (p:Passenger) = Some(p.Pclass)
let port (p:Passenger) =
    if p.Embarked = ""
    then None
    else Some(p.Embarked)
let age (p:Passenger) =
    if p.Age < 12.0
    then Some("Younger")
    else Some("Older")

printfn "Comparison: most informative feature"
let h = dataset.Rows |> entropy survived
printfn "Base entropy %.3f" h

dataset.Rows |> splitEntropy survived sex |> printfn "  Sex: %.3f"
dataset.Rows |> splitEntropy survived pclass |> printfn "  Class: %.3f"
dataset.Rows |> splitEntropy survived port |> printfn "  Port: %.3f"
dataset.Rows |> splitEntropy survived age |> printfn "  Age: %.3f"

>
Comparison: most informative feature
Base entropy 0.666
  Sex: 0.515
  Class: 0.608
  Port: 0.649
  Age: 0.660
```

这里明显的赢家是性，它具有最低的熵，因此是最具信息性的特征；阶级似乎也有一些潜力，但港口和年龄没有显示出任何明显的改善。这给了我们一个决策树桩，和两个群体，雄性和雌性；我们可以再次重复同样的程序。

***清单 6-13*** 。比较子组中的信息增益

let bySex = dataset.Rows |> Seq.groupBy sex

```
for (groupName, group) in bySex do
    printfn "Group: %s" groupName.Value
    let h = group |> entropy survived
    printfn "Base entropy %.3f" h

    group |> splitEntropy survived sex |> printfn "  Sex: %.3f"
    group |> splitEntropy survived pclass |> printfn "  Class: %.3f"
    group |> splitEntropy survived port |> printfn "  Port: %.3f"
    group |> splitEntropy survived age |> printfn "  Age: %.3f"
>
Group: male
Base entropy 0.485
  Sex: 0.485
  Class: 0.459
  Port: 0.474
  Age: 0.462
Group: female
Base entropy 0.571
  Sex: 0.571
  Class: 0.427
  Port: 0.555
  Age: 0.565
```

如果你运行这个，你会首先看到在性上再次分裂将产生零收益，这是令人放心的(我们已经使用了那个信息)。您还会看到每个组的要素排序不同，在这种情况下，两者的最佳排序仍然是 class，但是您很可能最终会为每个分支指定不同的要素。最后，还要注意的是，在分割后，我们总是以至少相同的熵水平结束，这意味着如果我们只是添加更多的特征并继续递归，我们永远不会变得更糟，并且可能会继续将我们的样本分成越来越小的组，而不管分割是否真的有意义。所以，我们需要考虑一个停止规则，来决定什么时候继续下跌是不值得的。

从编码的角度来看，你可能认为上面的代码迫切需要一些重构。手动编写每个特性是非常乏味的，能够定义一个特性列表会更令人满意。不幸的是，我们这里有点小问题。如果你考虑一下特性，比如说，sex 和 pclass，你会注意到它们的类型是不同的。Sex 产生一个 string 选项，而 pclass 返回一个 int 选项，结果我们不能把它们包含在同一个列表中。

那么，我们该怎么办呢？有几个选项可供选择，但没有一个是最好的。这里为了简单起见，我们将强制特性产生一个字符串选项。当我们这样做的时候，为特性命名也是很方便的。例如，如果我们为特征定义了一个类型，比如 type Feature= string * ' a-> string option，我们可以用相似的方式对待它们，并以更加一致的方式分析它们。

***清单 6-14*** 。分析要素列表

```
let survived (p:Passenger) = p.Survived
let sex (p:Passenger) = Some(p.Sex)
let pclass (p:Passenger) = Some(p.Pclass |> string)

let features =
    [   "Sex", sex
        "Class", pclass ]

features
|> List.map (fun (name, feat) ->
    dataset.Rows
    |> splitEntropy survived feat |> printfn "%s: %.3f" name)
 >
Sex: 0.515
Class: 0.608
```

使用熵离散化数字特征

作为这种方法可以派上用场的特殊情况，让我们重温一下我们之前讨论的问题，即如何将一个连续的特征(如年龄或票价)缩减为一组离散的情况。

在上一节中，我们为年龄创建了一个特征，其中小于 12 岁的乘客被视为“年轻”，其他人被视为“年长”为什么是 12，而不是 5、42 或任何其他值？我们完全随意地选择了那个值。这可能行得通，但它不是非常科学或数据驱动的。

我们可以尝试用熵来解决这个问题，而不是依赖直觉。我们正在寻找的是一个尽可能提供信息的年龄阈值。换一种熵的说法，我们想要一个最大化熵增益的值。这并不十分复杂:只需从数据集中取出每个可能的年龄值，并通过为每个可能的临界值创建一个特征来确定产生最大信息增益的年龄值:

```
let ages = dataset.Rows |> Seq.map (fun p -> p.Age) |> Seq.distinct
let best =
    ages
    |> Seq.minBy (fun age ->
        let age (p:Passenger) =
            if p.Age < age then Some("Younger") else Some("Older")
        dataset.Rows |> splitEntropy survived age)
printfn "Best age split"
printfn "Age: %.3f" best

Best age split
Age: 7.000
```

这种方法并不完美。宁滨连续变量有点像黑暗艺术，也没有什么灵丹妙药。这一个的好处是完全自动化；因此，您可以根据数据生成树桩，而无需人工干预。

![Image](../images/00011.jpeg) **注意**这种方法的一个限制是，它假设有两个类别是离散化值的好方法，这可能合适也可能不合适。解决这个问题的一种方法是递归地应用相同的过程，并检查是否可以有利地进一步分割任何被识别的仓。然而，这可能是棘手的，因为随着垃圾箱变得越来越小，产生人为分割的风险就会增加。我们将把它留在那里，但是为有兴趣进一步探索这个问题的读者提到搜索词“最小描述长度”。

从数据中生长一棵树

我们现在有了构建决策树所需的两个关键要素。有了决策树桩，我们就有了最小的单元，这些单元可以被训练来基于一个特征处理不同分支中的观察。有了熵，我们可以在给定一个观察样本的情况下，衡量一个特定特征的有用程度。我们现在只需要把树桩组合成一棵完整的树。从特征集合和训练集开始，我们将识别最具信息性的特征，为每个结果创建一个树桩，并为每个分支重复该操作。

我们要写的代码相当一般，所以，以类似于第 2 章中的方式，让我们把它提取到它自己独立的模块中。我们将在我们的解决方案中添加一个文件，比如 Tree.fs，开始将我们的一些通用代码移到那里，并在我们的脚本中使用该文件，这将专门关注为 Titanic 数据集制作模型的特定问题。

为树建模

先说最终结果，树。树桩和完整的树之间的主要区别是，虽然树桩的每个分支都给我们一个最终的、明确的答案，但树更复杂:它可以给我们一个最终的答案，但它也可以继续前进，带领我们到另一个树桩，并提出后续问题。

很多时候，当你听到一个领域描述涉及到“或”的语句(“可以是这个也可以是那个”)，F#的解决方案就会涉及到一个有区别的并集。这在这里非常适用:一棵树要么是一个答案，要么是通向另一棵树的树桩。清单 6-15 中的代码相当直接地映射了我们对问题的描述。

[***清单 6-15***](#_list15) 。将决策树建模为有区别的并集

```
namespace Titanic
module Tree =

    type Feature<'a> = 'a -> string Option
    type NamedFeature<'a> = string * Feature<'a>
    type Label<'a,'b when 'b:equality> = 'a -> 'b

    type Tree<'a,'b when 'b:equality> =
        | Answer of 'b
        | Stump of NamedFeature<'a> * string * Map<string,Tree<'a,'b>>
```

这里有一些简短的评论:首先，尽管在泰坦尼克号事件中标签是字符串，但情况并不总是这样。例如，在第一章中，类是整数。我们在这里使用了通用的标签，这样我们的树就可以处理更一般的情况。然后，树数据结构有点复杂—让我们解释一下。它说的是，一棵树会导致两种结果之一:要么我们得到了一个答案(最后一片叶子，它包含标签值‘b’)，要么它会导致一个树桩。在这种情况下，我们需要知道三件事:树桩是用什么特性构建的(命名为 feature<a>)、当特性的值丢失时使用什么值(一个字符串)、对于我们期望看到的特性的每个可能值(树桩的“分支”)，接下来是什么，是一个答案还是另一个树桩。

我们如何用这个来做决定？我们想要的如下:给定一个乘客和一棵树，如果树是一个答案，那么我们就完成了。否则，使用附在树桩上的特征为乘客获取另一个特征值，并在相应的树中继续搜索。如果值是 None，我们遇到了缺失数据，并将使用 Stump 中指定的缺省值。我们还决定，在遇到未知值的情况下，我们也会将其视为默认值。你可能会说，在这种情况下，树应该抛出一个异常；你可以随意这么做，但这将以潜在问题为代价。

***清单 6-16*** 。用一棵树来做决定

```
let rec decide tree observation =
    match tree with
    | Answer(labelValue) -> labelValue
    | Stump((featureName,feature),valueWhenMissing,branches) ->
        let featureValue = feature observation
        let usedValue =
            match featureValue with
            | None -> valueWhenMissing
            | Some(value) ->
                match (branches.TryFind value) with
                | None -> valueWhenMissing
                | Some(_) -> value
        let nextLevelTree = branches.[usedValue]
        decide nextLevelTree observation
```

构建树

快好了！我们需要做的最后一件事是从一个样本和一个初始的特征集合中构建树。我们将简单地为样本找到最有信息的特征，为它创建一个树桩，并递归地继续下去，直到我们没有特征或者剩下的数据太少而无法使用。

清单 6-17 就是这么做的；有点冗长，但不是特别复杂。有两个棘手的问题值得注意。首先，特性作为 Map <字符串传入，特性>；一方面，这使得通过名称访问特性、添加或删除特性变得容易。另一方面，当使用 Map 作为序列时，元素显示为 KeyValuePair。此外，我们不希望在 stump 中出现缺失值的分支:这些将由缺省值 case 来处理。

[***清单 6-17***](#_list17) 。根据样本和特征生成树

```
let mostFrequentBy f sample =
    sample
    |> Seq.map f
    |> Seq.countBy id
    |> Seq.maxBy snd
    |> fst

let rec growTree sample label features =

    if (Map.isEmpty features)
    // we have no feature left to split on:
    // our prediction is the most frequent
    // label in the dataset
    then sample |> mostFrequentBy label |> Answer
    else
        // from the named features we have available,
        // identify the one with largest entropy gain.
        let (bestName,bestFeature) =
            features
            |> Seq.minBy (fun kv ->
                splitEntropy label kv.Value sample)
            |> (fun kv -> kv.Key, kv.Value)
        // create a group for each of the values the
        // feature takes, eliminating the cases where
        // the value is missing.
        let branches =
            sample
            |> Seq.groupBy bestFeature
            |> Seq.filter (fun (value,group) -> value.IsSome)
            |> Seq.map (fun (value,group) -> value.Value,group)
        // find the most frequent value for the feature;
        // we'll use it as a default replacement for missing values.
        let defaultValue =
            branches
            |> Seq.maxBy (fun (value,group) ->
                group |> Seq.length)
            |> fst
        // remove the feature we selected from the list of
        // features we can use at the next tree level
        let remainingFeatures = features |> Map.remove bestName
        // ... and repeat the operation for each branch,
        // building one more level of depth in the tree
        let nextLevel =
            branches
            |> Seq.map (fun (value,group) ->
                value, growTree group label remainingFeatures)
            |> Map.ofSeq

        Stump((bestName,bestFeature),defaultValue,nextLevel)
```

我们的大部分工作都在这里完成。我们现在可以从数据集和要素集合中学习树。让我们试试这个！现在我们的算法变得稳定了，也许是时候将我们的探索转移到一个新的脚本文件了。

***清单 6-18*** 。第一棵泰坦尼克树

```
#r @".\packages\FSharp.Data.2.2.2\lib\net40\FSharp.Data.dll"
#load "Tree.fs"

open System
open FSharp.Data
open Titanic
open Titanic.Tree

type Titanic = CsvProvider<"titanic.csv">
type Passenger = Titanic.Row

let dataset = Titanic.GetSample ()

let label (p:Passenger) = p.Survived

let features = [
    "Sex", fun (p:Passenger) -> p.Sex |> Some
    "Class", fun p -> p.Pclass |> string |> Some
    "Age", fun p -> if p.Age < 7.0 then Some("Younger") else Some("Older") ]

let tree = growTree dataset.Rows label (features |> Map.ofList)

dataset.Rows
|> Seq.averageBy (fun p -> if p.Survived = decide tree p then 1\. else 0.)

>
val tree : Tree<CsvProvider<...>.Row,bool> =
  Stump
    (("Sex", <fun:features@17-2>),"male",
     map
       [("female",
         Stump
           (("Class", <fun:features@18-3>),"3",
            map
              [("1",
                Stump
                  (("Age", <fun:features@19-4>),"Older",
                   map [("Older", Answer true); ("Younger", Answer false)]));
// snipped for brevity
val it : float = 0.8058361392
```

好消息是，运行这段代码会生成一个树，并且有明显的改进，大约有 80%的正确调用。也有一些坏消息。第一，输出树丑得要命。还有，我们完全忽略了到目前为止我们所宣扬的一切，根本没有使用交叉验证来评估我们的工作。我们需要解决这个问题。

![Image](../images/00011.jpeg) **注意**与我们在[第 2 章](2.html#8IL20-841455c729754b8aac560d608a86cf91)中讨论的朴素贝叶斯分类器类似，决策树学习的内容直接基于项目在训练集中出现的频率。因此，重要的是要记住，如果数据集明显偏离“真实”人口，并且没有反映其真实组成，则预测很可能完全不正确。

更漂亮的树

我们收到的树充其量只能勉强可读；让我们编写一个简单的函数，以更人性化的格式显示它。本质上，我们将递归地遍历树，每次到达新的深度级别时增加缩进，并在遍历时打印出特性和分支名称。

***清单 6-19*** 。渲染树

let rec display depth tree =

```
    let padding = String.replicate (2 * depth) " "
    match tree with
    | Answer(label) -> printfn " -> %A" label
    | Stump((name,_),_,branches) ->
        printfn ""
        branches
        |> Seq.iter (fun kv ->
            printf "%s ? %s : %s" padding name kv.Key
            display (depth + 1) kv.Value)
```

这是我们种植的前一棵树的结果:

```
? Sex : female
   ? Class : 1
     ? Age : Older -> true
     ? Age : Younger -> false
   ? Class : 2
     ? Age : Older -> true
     ? Age : Younger -> true
   ? Class : 3
     ? Age : Older -> false
     ? Age : Younger -> true
 ? Sex : male
   ? Class : 1
     ? Age : Older -> false
     ? Age : Younger -> true
   ? Class : 2
     ? Age : Older -> false
     ? Age : Younger -> true
   ? Class : 3
     ? Age : Older -> false
     ? Age : Younger -> false
```

很明显，你可以做一些更花哨的东西，但在我看来，这在很大程度上足以看出模型告诉我们什么；例如，乘坐三等舱的女性，不到七岁，比同舱的年长女性有更好的生存机会。

顺便说一句，我认为这是决策树的关键吸引力之一:它们的结果非常容易被人类理解。一棵树是你可以呈现给一个数学背景有限的人的东西，他们仍然会得到它。虽然树也有它的局限性，正如我们马上会看到的，但是只看模型输出并理解它的能力是不可低估的。

改进树

我们编写了一个完全通用的算法，可以使用任何数据集，并根据数据集构建一个决策树，使用您喜欢的任何功能列表，以尽可能好的顺序。这是不是意味着我们结束了？

不完全是。虽然最复杂的模块已经就位，但是我们的树生长算法可能会出错。按照现在的情况，我们几乎肯定会过度拟合训练数据，并学习一个脆弱的模型，当试图预测新数据时，该模型将表现不佳。

为什么我们会过度合身？

让我们用一个惊人的例子来说明可能发生的问题类型，并考虑在一个单一的特征上建立一个模型，即乘客 ID。当然，这是一个荒谬的模型:观察我们在训练样本中没有的乘客的唯一 ID 不应该帮助我们预测他们的命运。然而，如果您使用该功能种植一棵树，您将得到以下模型:

***清单 6-20*** 。乘客 ID 过拟合

```
let idExample = [ "ID", fun (p:Passenger) -> p.PassengerId |> string |> Some ]
let idTree = growTree dataset.Rows label (idExample |> Map.ofList)

dataset.Rows
|> Seq.averageBy (fun p -> if p.Survived = decide idTree p then 1.0 else 0.0)
|> printfn "Correct: %.3f"

idTree |> display 0

>
Correct: 1.000

 ? ID : 1 -> false
 ? ID : 10 -> true
 ? ID : 100 -> false
 // snipped for brevity
 ? ID : 97 -> false
 ? ID : 98 -> true
 ? ID : 99 -> true
```

这在很多层面上显然是错误的。请注意，数学没有错；只是结果，从实用的角度来看，完全没用。如果我们对新数据使用此模型，这些新的 id 显然不会在树中，因此预测将是默认的、不知情的预测，但是，此模型在建模和预测训练集方面是 100%正确的。

我们已经遇到了最后一个问题，希望现在你在想，“当然，他应该使用交叉验证，”你是对的。我们将在前面使用交叉验证，但我认为提出这一点是有用的，首先，因为它非常简单地说明了事情可能会变得非常糟糕，其次，因为过度拟合对于树来说是一个需要记住的特别重要的问题。

与我们之前考虑的其他算法不同，树是递归学习的。因此，它不是在完整的训练集上一次学习所有的特征，而是在随着算法的进展越来越小的样本上一次学习一个特征。

当你考虑熵是如何工作的时候，这就有点问题了。第一，信息不会消失；在最糟糕的情况下，按特征分割将不会给我们带来任何信息。因此，在当前的实现中，我们总是将我们拥有的每一个特征添加到树中，即使它没有添加任何信息。然后，因为熵是基于比例的，所以当分析小样本和大样本时，它会得出相同的结论，只要它们的成分是相同的。这也有点问题:我对从包含 10 个观测值的样本和包含 100，000 个观测值的样本得出的结论没有同样的信心。

用过滤器限制过度自信

让我们先从解决较简单的问题开始。我们希望避免基于不充分的决定性样本而过于自信地学习。一个简单的方法是在我们的树生长算法中添加过滤器，以便在我们沿着树向下前进时消除弱候选。

让我们考虑我们已经确定的两种情况。我们目前正在挑选具有最高条件熵的特征；我们可以通过仅保留具有严格正熵增益的特征来改进算法。为了执行该操作，我需要一些数据:显然是一个特征，但也是一个样本和一个标签，以计算熵和分裂熵，并检查分裂后的熵是否更好(即严格来说更低)。

如果我只是在算法中硬编码这个规则，我会按照下面的方式修改我们现有的代码:

```
let rec growTree sample label features =

let features =
    features
    |> Map.filter (fun name f ->
        splitEntropy label f sample - entropy label sample < 0.)
```

这是可行的，但用可靠的说法来说，它是不可扩展的。如果我们想要更大的灵活性，比如只保留通过用户定义的阈值的特性，比如说，比以前的值至少有 5%的熵改进，那该怎么办？我们讨论的另一个问题如何，即从不够大的样本中得出结论？

上面的代码暗示了一个潜在的解决方案；在 Map.filter 中，我们应用了“特征过滤器”，该功能检查特征是否通过给定样本和标签的条件。我们可以简单地将一组过滤器传递给我们的算法，只保留满足所有过滤器的特征，而不是对此进行硬编码。让我们修改树模块中的 growTree 代码，并添加几个内置过滤器。

***清单 6-21*** 。将特征过滤器注入树算法

```
let entropyGainFilter sample label feature =
    splitEntropy label feature sample - entropy label sample < 0.

let leafSizeFilter minSize sample label feature =
    sample
    |> Seq.map feature
    |> Seq.choose id
    |> Seq.countBy id
    |> Seq.forall (fun (_,groupSize) -> groupSize > minSize)

let rec growTree filters sample label features =

    let features =
        features
        |> Map.filter (fun name feature ->
            filters |> Seq.forall (fun filter -> filter sample label feature))

    if (Map.isEmpty features)
    // snipped for brevity
```

使用与先前相同的特性应用这个更新的算法，但是传入[entropyGain；作为一个过滤器，这是我得到的树:

```
? Sex : female
   ? Class : 1 -> true
   ? Class : 2 -> true
   ? Class : 3
     ? Age : Older -> true
     ? Age : Younger -> false
 ? Sex : male
   ? Class : 1 -> false
   ? Class : 2 -> false
   ? Class : 3
     ? Age : Older -> false
     ? Age : Younger -> false
```

我们的决策树比之前的版本失去了一些深度。这很好:使用过滤器可以减少一些虚假的分支，结果，我们得到了一个可读性更好、更可靠的树。还要注意，male 下的所有分支都导致相同的结论。这是否表明我们的算法有问题？不一定:当熵发现分支之间的显著差异时，它会决定分裂，不管这种差异对我们的决策是否重要。例如，51%和 99%的生存机会有很大的不同，但是这种不同对于结论来说并不重要，结论是一样的。根据你的需要，你可以用两种不同的方法来改进这个树，要么增加最终分支的概率，要么把包含相同答案的树桩合并成一个答案。

从树木到森林

我们通过在算法中注入一些常识来限制一些最明显的过度拟合风险，但解决方案相当粗糙。由于树的递归性质，过拟合对于树来说尤其成问题；但是，它是机器学习中的一个普遍问题。通过从一个训练样本学习，存在对该样本学习得太好的固有风险。我们在树上应用的过滤器和我们在前面一章提到的正则化方法都用一种策略解决了这个问题:通过应用一种形式的惩罚来限制算法允许学习的量。

在这一节中，我们将介绍思考这个问题的另一种方式，从一个非常不同的角度来看如何解决这个问题。如果你仔细想想，过度拟合是一个问题的原因之一是，我们依赖于单个训练集，并试图从中挤出尽可能多的信息，冒着学习一个过于特定于该特定数据集的模型的风险。与其武断地限制我们可以学习的东西，也许我们可以试着从一组不同的数据中学习，以避免对单一模型的过度依赖。

使用 k-folds 进行更深入的交叉验证

让我们从重新审视我们的模型有多好这个问题开始。我们的数据集包含 891 名乘客；首先，这是没有什么根据的。比方说，如果我们将前 75%用于训练，剩下的 25%用于验证，那么我们只剩下不到 700 个样本来训练我们的分类器，以及大约 220 个样本来尝试模型以评估其性能。而且，就像在太少的数据上训练一个模型可能导致脆弱的结果一样，在太少的数据上评估它可能会不时地导致虚假的值。由于抽签的运气，一些验证样本将是非典型的，并产生虚假的质量度量。

限制这个问题的一种方法是通过以下方式:我们可以构造多个训练/验证样本，并对每个样本重复相同的过程，而不是选择一个单一的和任意的验证样本。例如，我们可以将样本分成 k 个切片，并通过使用一个切片进行验证，剩余的 k - 1 个切片用于训练来创建 k 个组合。例如，我们可以通过将数据集分成三等份，留下其中一份用于验证，而不是简单地将样本分成 2/3 用于训练，1/3 用于验证，来构建三个备选对(参见[图 6-7](#Fig7) )。

![9781430267676_Fig06-07.jpg](../images/00133.jpeg)

[图 6-7](#_Fig7) 。从一个样本构建三个训练/验证组合

那么，我们为什么要这么做呢？这可能看起来毫无意义:不是一个模型和质量度量，我们现在有三个稍微不同的模型，可能有三个不同的质量度量，和一个决策问题。我们的三个模型哪个最好，应该相信哪个评价？这有什么帮助？

如果我们稍微改变一下视角，会有所帮助。我们知道我们得到的验证度量无论如何都是不正确的:这是一个估计。在这种情况下，了解这个估计可能有多错误和估计本身一样有用。我们现在可以感觉到数据的变化会如何影响我们的模型，以及质量会有多不稳定，而不是把一切都挂在一个不正确的数字上。

这种方法被称为 **k 倍**。选择 k 需要一点权衡。在极端情况下，如果我们将 k 增加到其最大可能值，即数据集大小，我们将得到 k 个不同的模型，每个模型都在一个示例上进行验证，这通常被称为“留一个”显然，k 变得越大，运算的代价就越大，因为需要训练更多的模型。此外，随着 k 的增加，每个模型共享更多的公共数据，因此可以预期其行为更像是使用整个数据集的模型。

让我们在我们的模型上试试。首先，我们需要生成 k 对训练和验证样本。下面的实现虽然不太漂亮，但完成了工作:对于 k 个值中的每一个，它计算验证片的开始和结束索引，并返回 k 个样本对的列表，每个样本对包含一个元组中的训练和验证部分。

***清单 6-22*** 。从样本中生成 k 个训练/验证对

```
let kfold k sample =
    let size = sample |> Array.length
    let foldSize = size / k

    [ for f in 0 .. (k-1) do
        let sliceStart = f * foldSize
        let sliceEnd = f * foldSize + foldSize - 1
        let validation = sample.[sliceStart..sliceEnd]
        let training =
            [|
                for i in 0 .. (sliceStart - 1) do yield sample.[i]
                for i in (sliceEnd + 1) .. (size - 1) do yield sample.[i]
            |]
        yield training,validation
    ]
```

让我们尝试对我们的基本树进行 k 倍分析，k = 10:

***清单 6-23*** 。评估十倍

```
let folds = dataset.Rows |> Seq.toArray |> kfold 10
let accuracy tree (sample:Passenger seq) =
    sample
    |> Seq.averageBy (fun p ->
        if p.Survived = decide tree p then 1.0 else 0.0)

let evaluateFolds =
    let filters = [ leafSizeFilter 10; entropyGainFilter ]
    let features = features |> Map.ofList
    [for (training,validation) in folds ->
        let tree = growTree2 filters training label features
        let accuracyTraining = accuracy tree training
        let accuracyValidation = accuracy tree validation

        printfn "Training: %.3f, Validation: %.3f" accuracyTraining accuracyValidation
        accuracyTraining, accuracyValidation]
>
Training: 0.802, Validation: 0.697
Training: 0.788, Validation: 0.843
Training: 0.796, Validation: 0.775
Training: 0.797, Validation: 0.764
Training: 0.803, Validation: 0.798
Training: 0.798, Validation: 0.843
Training: 0.804, Validation: 0.787
Training: 0.805, Validation: 0.775
Training: 0.788, Validation: 0.843
Training: 0.794, Validation: 0.787
 val evaluateFolds : (float * float) list =
  [(0.8017456359, 0.6966292135); // snipped // ; (0.7942643392, 0.7865168539)]
```

这里有几个有趣的结果。首先，虽然差异不大，但训练的平均准确率约为 79.7%，比验证的平均准确率(79.1%)高出约 0.6 个百分点。然后，验证样本的质量度量比训练样本的质量度量更易变。虽然训练的准确度在 78.8%到 80.5%之间，但是验证结果一直在 69.7%到 84.3%之间变化。这并不奇怪，因为每个验证样本都更小，因此更易变。这些结果非常有用，因为它们传达了我们应该从模型中期待的更好的画面。如果我们只进行了基本的交叉验证，我们将只能看到我们用 k-fold 产生的十个结果中的一个；根据我们碰巧选择的训练/验证对，我们可能会得出不同的结论——要么过于乐观，要么过于悲观。相比之下，使用 k-fold，我们可以得出结论，我们的模型可能不会过度拟合(训练和验证的平均误差非常接近)。对于我们的模型可能有多好或多坏，我们也有了更精确的感觉:我们应该预计在大约 79.1%的情况下是正确的，但标准偏差——该值周围的平均误差——是 4.5 个百分点。

希望这个简单的例子能够说明，通过简单地从我们的原始数据集创建多个样本，我们如何能够更好地了解我们的模型质量。这种技术主要用于评估模型中的变更是否是实际的改进；在下一节中，我们将看到如何使用类似的想法来改进模型本身。

将脆弱的树木结合成健壮的森林

通过 k-folds，我们看到，对我们的训练集进行重新采样，可以让我们从其中提取更多信息，而不必添加新的观察值。我们没有将所有鸡蛋放在一个篮子里(或者将所有观察结果放在一个训练集中)，而是生成了多个训练样本，这让我们更好地了解了我们的模型对数据变化的敏感度。

这是一个我觉得既明显又迷人的结果。令人着迷，因为从表面上看，我们似乎凭空创造了额外的信息。利用我们最初拥有的同样数量的信息，我们设法挤出了明显新的和更好的信息。很明显，因为这只是一个焦点的改变。从根本上说，我们在处理不确定性，而对不确定性的最佳描述是分布。通过从我们的原始数据构建多个随机样本，我们可以模拟我们试图预测的现象中存在的可变性类型，从而在输出中复制(和测量)最终的可变性。

同样的想法可以进一步发展，并用于产生潜在的更强大的预测模型。我们可以拥抱意外，接受输入是不稳定的，并基于不同的训练集创建大量的模型，而不是试图找到一个完美的模型。他们有些是对的，有些是错的，但是如果我们把他们所有的预测放在一起看，他们不太可能同时都是错的。例如，我们可以进行多数投票，并希望出现“群体智慧”效应，即一些奇怪的预测会被大多数合理的预测所抵消。这种一般方法被称为“集成方法”，这种方法研究如何将多个弱预测模型组合成一个比其各部分之和更强的整体模型。

在决策树的具体情况下，一种方法是通过随机均匀采样观察值，从原始数据集简单地生成多个训练集，替换时，可能会多次重复相同的观察值。这种方法被称为“bagging”(简称 *bootstrap aggregating* )。这里的直觉是，这将减轻过度拟合的风险，因为每个模型将在不同的样本上训练。每个模型可能仍然会从数据中提取工件，但是是不同的工件，当它们放在一起时可能会互相抵消。

当我们实现决策树时，它还有一个不太明显的问题。因为该过程一次处理一个功能，所以一些功能可能会掩盖其他功能的影响。想象一下，一个特性与另一个特性密切相关(比如，您乘坐的是什么级别，您的票价是多少)，但它本身包含一些信息。一旦树选择了第一个，很可能第二个不会被选择，因为剩余的信息本身不够强。因此，通过按熵的顺序从所有特征中挑选，我们冒了系统地忽略可能传达一些有用信息的特征的风险。

以类似于 bagging 的方式，我们可以通过随机抽取特征来解决这个问题。至少在某些情况下,“隐藏的”特征将在没有竞争的特征的情况下存在，并且如果它被证明是有益的，将有机会参与到那个树中。

这两个想法可以合二为一。我们不是创建一个决策树，而是创建多个更简单的树，一个森林，每个树都使用随机选择的训练样本和一个特征子集。我们将从所有的树中选择多数票，而不是依赖一棵树。每一个单独的模型都比一次使用所有数据的决策树弱，但是，也许矛盾的是，把它们放在一起会提供更可靠的预测。

实现缺失的块

扩展我们当前的代码来生成一个森林而不是一棵树是相当简单的。基本上，我们只需要添加三个元素:从列表中随机选择特征(无重复)，从原始样本中随机选择示例(有重复)，将多棵树的预测组合成多数投票。我们将把所有必要的代码添加到 Tree.fs 文件中，然后在特定的 Titanic 示例上进行试验。

先说第二个元素。如果我们假设我们的初始样本是一个数组，那么重复选取就像选择一个随机索引一样简单，我们需要多少次就选多少次:

```
let pickRepeat (rng:Random) proportion original =
    let size = original |> Array.length
    let sampleSize = proportion * float size |> int
    Array.init sampleSize (fun _ -> original.[rng.Next(size)])
```

没有重复的采摘有点棘手。一种可能是打乱整个集合并挑选第一个元素。另一种可能是递归地处理集合，逐个决定是否应该选择一个元素。如果我想从 N 个元素中挑选 N 个元素，列表的头有 n/N 的机会被选中。如果它被选中，我现在有 n - 1 个元素可以从剩下的 N - 1 个中选择；在另一种情况下，机会是 N/N-1。我们开始吧:

```
let pickNoRepeat (rng:Random) proportion original =

    let size = original |> Seq.length
    let sampleSize = proportion * float size |> int

    let init = ([],size)
    original
    |> Seq.fold (fun (sampled,remaining) item ->
        let picked = List.length sampled
        let p = float (sampleSize - picked) / float remaining
        if (rng.NextDouble () <= p)
        then (item::sampled,remaining-1)
        else (sampled,remaining-1)) init
    |> fst
```

最后，来自森林的预测仅仅是来自其树的最频繁的决策:

```
let predict forest observation =
    forest
    |> Seq.map (fun tree -> decide tree observation)
    |> Seq.countBy id
    |> Seq.maxBy snd
    |> fst
```

种植森林

让我们把所有这些放在一起:唯一需要的是一个函数，它将创建用户定义数量的树。我们还将更紧密地包装事物，并直接返回一个可用于进行预测的函数。虽然一棵单独的树是有用的，因为它可以被检查，但我们不打算查看数百棵树，希望发现模式或诸如此类的东西。

***清单 6-24*** 。从决策树创建森林

```
let growForest size sample label features =
    let rng = Random ()

    let propFeatures =
        let total = features |> Seq.length |> float
        sqrt total / total

    let featSample () = pickNoRepeat rng propFeatures features
    let popSample () = pickRepeat rng 1.0 sample
    let filters = [ leafSize 10; entropyGain ]

    let forest = [
        for _ in 1 .. size ->
            let sample = popSample ()
            let features = featSample () |> Map.ofList
            growTree filters sample label features ]

    let predictor = predict forest
    predictor
```

我们通过为过滤器预先选择合理的默认值来简化函数签名；我们还为总体(100%)和要素(要素数量的平方根)选择了默认比例。提供一个细粒度的版本不会很复杂；只需要编写该函数的完全扩展的签名，并从简化的“默认”版本中调用它。

尝试森林

现在我们已经有了算法，让我们试一试，并将结果与我们之前得到的树进行比较。我们将创建一个特征列表，并且使用与之前相同的十个训练/验证对，我们将训练一个由 1，000 棵树组成的森林(每次使用稍微不同的特征和观察的组合)，然后在训练和验证集上计算算法的准确性。

***清单 6-25*** 。测试森林

```
let forestFeatures = [
    "Sex", fun (p:Passenger) -> p.Sex |> Some
    "Class", fun p -> p.Pclass |> string |> Some
    "Age", fun p -> if p.Age < 7.0 then Some("Younger") else Some("Older")
    "Port", fun p -> if p.Embarked = "" then None else Some(p.Embarked) ]

let forestResults () =

    let accuracy predictor (sample:Passenger seq) =
        sample
        |> Seq.averageBy (fun p ->
            if p.Survived = predictor p then 1.0 else 0.0)

    [for (training,validation) in folds ->

        let forest = growForest 1000 training label forestFeatures

        let accuracyTraining = accuracy forest training
        let accuracyValidation = accuracy forest validation

        printfn "Training: %.3f, Validation: %.3f" accuracyTraining accuracyValidation
        accuracyTraining,accuracyValidation ]

forestResults ()
>
Training: 0.803, Validation: 0.753
Training: 0.800, Validation: 0.775
Training: 0.808, Validation: 0.798
Training: 0.800, Validation: 0.775
Training: 0.798, Validation: 0.798
Training: 0.792, Validation: 0.854
Training: 0.800, Validation: 0.775
Training: 0.812, Validation: 0.764
Training: 0.792, Validation: 0.854
Training: 0.804, Validation: 0.820
val it  : (float * float) list =
  [(0.8029925187, 0.7528089888); // snipped //; (0.8042394015, 0.8202247191)]
```

结果，平均来说，比用树的结果好一点点，但是没有什么特别令人印象深刻的。然而，更有趣的是，在训练集和验证集上测量的准确性之间的差异要小得多。此外，验证集上的准确性也更加一致。在树的例子中，准确率从 69.7%到 84.3%不等；对于森林，我们得到的范围要窄得多，从 75.3%到 85.4%。这很好:这意味着，通常情况下，一个森林不太容易过度适应，并且模型将很好地概括；也就是说，它在以前没有见过的新数据上的表现不应该与它在训练数据上的表现有明显不同。

![Image](../images/00011.jpeg) **注意**森林相对于普通树木的一个额外好处是，它们可以在完整的数据集上进行训练，而不必留出验证集。每个观察将仅用于一些生成的树，因此可以使用不包括该示例的所有树进行训练，并将它们用作预测该示例的标签的树，然后检查预测是否正确。对每个例子重复这个过程将产生所谓的“预算外估计”，这是一种交叉验证的形式。这个想法很简单，但是实现有点繁琐，所以我们不会在这里介绍它，而是把它作为众所周知的“读者练习”

那么，我们学到了什么？

我们在这一章中涉及了相当多的领域。

首先，我们提出了决策树。决策树的部分吸引力可能在于它们的直观性。最后，一棵树只不过是一系列问题，旨在尽可能快地产生一个诊断。它们模仿了人类做决定的方式，也很容易理解，即使是非专业人士也能理解。虽然听起来很傻，但这可能是非常宝贵的。有可能的是，一旦你完成了一个机器学习模型的开发，你将需要说服其他人它是可行的；当你的模型在直觉层面上“有意义”时，这种对话就容易多了。

然而，也有技术上的原因来解释为什么树木是一个值得放在你的工具箱里的工具。我们用作基础的泰坦尼克号数据集是典型的，因为它是杂乱的。它包含不同类型的特征，从数字(票价)到分类(男性/女性)，并且一些观察值有缺失值-然而，很容易建立一个模型来处理所有这些潜在的问题，而没有太大的问题。树非常灵活，几乎适用于任何数据集。

除了我们刚刚提到的情况，树可以使用两个以上的标签进行分类，这没有任何特殊的技巧。这与许多经典分类器形成了有趣的对比，经典分类器本质上是二元的，需要额外的复杂度来处理三个或更多标签。就这一点而言，通过一点额外的努力，树甚至可以用来处理回归问题。

作为额外的奖励，树是直接基于概率的；不仅能够产生预测，而且能够产生相应的概率，这是非常方便的。另一方面，模型质量对训练样本质量很敏感。如果训练样本不能很好地复制一般人群的构成，学习算法将根据不正确的信息得出结论，就像我们在第[章第 2](2.html#8IL20-841455c729754b8aac560d608a86cf91) 中讨论的朴素贝叶斯模型一样。

在这个过程中，我们需要一个一致的方法来量化一个特性中有多少信息，这样我们就可以决定我们应该关注哪个特性。我们引入了香农熵(Shannon entropy)，这是一个衡量数据集包含多少杂质(以及可以对其做出多可靠的预测)的指标，并扩展到条件熵，它衡量我们在提出问题后有多大可能处于更好的信息状态。

我们还演示了熵的另一种用途，即离散化数字特征。从结构上来说，树对分类特征进行操作；即区分有限的一组离散案例的特征。因此，任何数值输入都需要减少到离散的箱中，熵提供了一种方法来决定使用什么样的截止值来提取最大信息。

最后，我们还讨论了决策树是如何由于其递归性质而具有过度拟合的自然趋势的。通过构建，树逐渐添加特征，随着算法的进展，对越来越小的样本进行操作，从而可能包括基于不充分结论性数据的特征，并以脆弱的模型结束。这让我们探索了与过度拟合相关的两个想法:使用 k 折叠的交叉验证和随机森林。通过 k-folds，我们看到了如何创建训练和验证样本的多种组合，使我们能够生成一系列描述模型质量的值，使我们更深入地了解模型的可靠性，以及训练数据的微小变化对模型的影响程度。

基于同样的想法，我们实现了一个简单版本的随机森林，这是一个分类器，属于被称为集成方法的更广泛的模型类别。我们没有依赖单一的树，而是训练了许多树，使用随机选择的训练数据和每个树的特征，并将它们的预测合并成一个多数票。这种有些自相矛盾的方法，即我们选择忽略一些我们可以获得的数据，在许多方面都接近于“群体智慧”的概念。通过创建多个更小、更不精确的模型，每个模型使用稍微不同的信息来源，我们最终设法创建了一个非常健壮的整体模型，因为许多独立的模型不太可能同时都是错误的，并且在某些方面，这个模型不仅仅是各个部分的总和。

**有用的链接**

你可以在这里找到 Kaggle“泰坦尼克号:机器从灾难中学习”的页面:[https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)。这是一个在真实的竞争中测试你的技能的有趣机会，而且还有一些有趣的教程。

如果你有兴趣了解更多关于随机森林的知识，Leo Breiman(他创造了算法)在这里有一页满满的有用信息:[https://www . stat . Berkeley . edu/~ brei man/random forests/cc _ home . htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)。