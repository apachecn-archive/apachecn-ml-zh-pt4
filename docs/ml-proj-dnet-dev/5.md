第五章

![image](../images/00004.jpeg)

你不是独一无二的雪花

Detecting Patterns with Clustering and Principle Components Analysis

有时候，你一天的机器学习是从一个非常具体的问题要解决开始的。你能判断收到的邮件是否是垃圾邮件吗？你能多准确地预测销售额？你已经收到了行军命令:你从一个问题开始，寻找可用的数据，并着手建立你能回答这个问题的最佳模型。

然而，生活往往不是那么清晰，你的一天从一个更加模糊的问题开始。你有的只是数据，还没有明确的问题。也许有人找到你，简单地问:“你能告诉我一些关于我的数据的有趣的事情吗？”也许你想对一个数据集进行一些分析，但想先了解你手上的牌。也许数据集很大，包含许多要素，如果要素之间有明显的关系，就很难掌握。手动数据探索是一个艰苦而乏味的过程——如果我们可以将一些工作交给机器，让它判断我们的数据中是否存在模式，那就太好了。

这是**无监督学习**的领域。在监督学习方法(如分类或回归)中，我们从一个问题和标记的数据开始——也就是说，带有问题答案的例子——然后我们用它来学习一个适合数据的模型。相比之下，在无监督学习中，我们拥有的只是数据，我们希望计算机帮助我们找到其中的一些结构。希望这个结构在人类的意义上是“有趣的”,并且可以用来以一种有意义的方式组织各个元素。

在本章中，我们将执行以下操作:

*   介绍两种经典的数据分析技术，k 均值聚类和主成分分析，这两种技术都旨在从数据集中自动提取信息，并提供更简单、更可用的表示。我们将把这些技术应用于 StackOverflow 网站的历史数据，看看出现的模式是否有趣。
*   勾勒出同样的一般想法——检测数据中的模式——如何用于根据用户过去的行为向他们提出建议,以及它与我们在其他用户中观察到的模式有多相似。

![Image](../images/00011.jpeg) **注意**本章中的一些更复杂的图表已经包含在源代码包中，以允许在必要时查看更大的格式。

检测数据中的模式

如果你想一想，你会意识到这不是一件容易的事情。你如何定义数据的“有趣之处”？有趣没有明确的衡量标准，那么我们到底要求计算机为我们做什么呢？

数据有很多“有趣”的方式如果我们从反面来看，不感兴趣的数据是没有什么突出的数据。从这个意义上来说，[图 5-1](#Fig1) 显示了不感兴趣的数据。

![9781430267676_Fig05-01.jpg](../images/00101.jpeg)

[图 5-1](#_Fig1) 。没有模式的数据

相比之下，[图 5-2](#Fig2) 显示的是“有趣的事情”似乎有某种组织，而不是均匀、同质的排列。观察结果被分成两组，随机分布在两个中心。这种类型的模式被称为**聚类**，聚类分析是在数据集中自动识别这种类型结构的过程。

![9781430267676_Fig05-02.jpg](../images/00102.jpeg)

[图 5-2](#_Fig2) 。组织在两个集群中的数据

然而，套用列夫·托尔斯泰的话，“所有无趣的数据集都是相似的；每个有趣的数据集都以自己的方式有趣。”聚类只是数据集有趣的众多方式之一。例如，考虑图 5-3 中的。数据清楚地显示了一个有趣的模式，观察值大致遵循一条直线，但没有聚类。

![9781430267676_Fig05-03.jpg](../images/00103.jpeg)

[图 5-3](#_Fig3) 。线性模式

聚类和线性结构是数据集有趣的两种方式，但实际上数据可以有无限多种方式展示模式；例如，考虑图 5-4 中的[，这里的数据](#Fig4)看起来是以环形组织的。它有点聚集(我们可以区分三组)，有点类似于[图 5-3](#Fig3) (数据遵循直线)——但也非常不同。

![9781430267676_Fig05-04.jpg](../images/00104.jpeg)

[图 5-4](#_Fig4) 。复杂模式

结构可以有多种形式，因此，没有一种方法适用于所有情况。每个数据集都是独一无二的，数据挖掘有点像一门黑暗的艺术。要找出可行的方法，通常只需尝试不同的算法，以及操作数据并将其转换为更合适的功能。

我们人类非常善于发现模式。你甚至不需要思考就能发现我刚才展示的一些数字中有一些模式。在这种情况下，图表是你的朋友:制作一个散点图来描绘一个特征与另一个特征之间的关系是一种非常廉价且有效的方法来观察数据结构。

同时，散点图只能给你带来这么多。回顾数据集中每一个变量对之间的相互作用变得非常痛苦。此外，也许更重要的是，数据中可能存在一些模式，通过一次只观察两个变量很难发现这些模式。因此，虽然您应该尽可能花时间查看数据，但您也需要一种系统的、机械的方法来筛选数据集并找到潜在的“有趣事实”。

为什么我们首先关心寻找“有趣的模式”？对于我们上面给出的所有例子，我们可以用一句话来描述数据集:“我们有两组”，“数据沿着一条直线”，“数据围绕三个环组织。”换句话说，我们可以对整个数据集提供一个更加简洁的描述。这是非常有用的，至少在两个方面。

首先，我们现在有了更好的数据表示。数据集不是数字的随机集合，而是可以以有意义的方式描述，并可能提供围绕它的词汇表。例如，如果图 5-2 中的聚类图中的每个数据点代表客户，我们现在可以开始描述两种类型的客户概况。

第二，就像我们可以用英语简洁地描述这些情况一样，我们也可以提供一个简洁的数学表示。所有这些数据集都是相似的，因为它们遵循简单的数学结构，上面有一点统计噪声。

我们的挑战:理解 StackOverflow 上的主题

在本章中，我们将应用一些经典的无监督学习技术，看看我们能否从 StackOverflow 使用数据中挖掘出一些信息。这是一个有趣的数据集，因为它的领域应该为大多数开发人员所熟悉。因此，我们将能够尝试算法和修改，并以非科学的方式判断该方法是否产生合理的结果。

具体来说，我们将研究是否可以识别 StackOverflow 问题中使用的标签中的模式。StackOverflow 上的每个问题都用最多五个词来标记，描述它涵盖的主题。我们的目标是，当我们随着时间的推移观察许多用户和问题，以及个人在哪些标签中最活跃时，我们是否可以看到一些结构和常见组合的出现。

为此，我使用我们在[第 3 章](3.html#9H5K0-841455c729754b8aac560d608a86cf91)中讨论的方法创建了一个数据集——使用 JSON 类型提供者从 StackExchange API 收集数据。数据集 userprofiles-toptags.txt 可以从这里下载:[http://1drv.ms/1ARuLg9](http://1drv.ms/1ARuLg9)。我确定了当时 StackOverflow 上最常见的 30 个标签，并为每个标签检索了最活跃的用户，将回答最多的用户和提问最多的用户分组在一起(前几名回答者和前几名提问者，有史以来和本月)。这给了我一个大约 1600 个不同 StackOverflow 用户的列表。

对于每个用户，我检索了他们在 2015 年活跃的前 100 个标签，以及相应的活跃程度，并只保留了与 30 个主要标签相关的数据。最后，我重新排列了最终数据集中的数据，有 30 列(每个标签一列)，1，600 行(每个用户一行)，每行和每列都有该用户在 2015 年对该标签的活动次数。注意，因为我只检索了用户的前 100 个标签，所以每个标签的信息都不完整；如果一个特定的用户对于 XML 是活跃的，但是这只是他的第 101 个最活跃的标签，我们会把它记为 0。

数据集是如何创建的？

我们使用[https://api.stackexchange.com/docs/tags](https://api.stackexchange.com/docs/tags)检索了 StackOverflow 上 30 个最受欢迎的标签。在创建时，会生成以下列表:

。net、ajax、android、arrays、asp.net、asp.net-mvc、c、c#、c++、css、django、html、ios、iphone、java、javascript、jquery、json、linux、mysql、objective-c、php、python、regex、ruby、ruby-on-rails、sql、sql-server、wpf、xml

我们使用以下方法按标签创建最活跃用户的列表，并提取他们在哪些标签中最活跃:

[https://api.stackexchange.com/docs/top-answerers-on-tags](https://api.stackexchange.com/docs/top-answerers-on-tags):选择所有时间或最近一个月回答最多的用户

[https://api.stackexchange.com/docs/top-askers-on-tags](https://api.stackexchange.com/docs/top-askers-on-tags):选择所有时间或最近一个月中问题最多的用户

[https://api.stackexchange.com/docs/tags-on-users](https://api.stackexchange.com/docs/tags-on-users):对于一组选定的用户，返回最活跃标签的活动

了解我们的数据

鉴于我们任务的探索性质，我们将再次从 F#脚本环境开始工作。让我们用一个 F#库项目创建一个新的解决方案， Unsupervised，为了方便起见，将数据文件添加到解决方案本身。数据集是文本文件 userprofiles-toptags.txt 的形式，可以从以下链接下载:[http://1drv.ms/1M727fP](http://1drv.ms/1M727fP)。如果您从 Visual Studio 中查看它，您应该会看到类似这样的内容:

```py
UserID,.net,ajax,android,arrays,asp.net,asp.net-mvc,c,c#,c++,css,django,html,ios,iphone,java,javascript,jquery,json,linux,mysql,objective-c,php,python,regex,ruby,ruby-on-rails,sql,sql-server,wpf,xml
1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
1000343,0,0,0,0,0,0,0,0,0,3,0,5,0,0,0,0,0,0,0,0,0,0,2,52,0,0,0,0,0,0
100297,0,0,0,26,0,0,0,0,0,0,99,62,0,0,0,29,0,182,0,26,0,0,4478,172,0,0,32,0,0,27
100342,0,0,0,0,0,1,0,0,0,0,0,3,0,0,0,16,5,0,0,0,0,0,0,0,1,7,0,0,0,0
```

第一行是描述每列内容的标题。第一列包含用户 ID，后面是 30 个以逗号分隔的列，每一列包含特定用户和标记的活动级别。

在深入研究算法之前，让我们从基本的统计数据开始，了解一下情况。我们将从打开 Script.fsx 文件开始。我们将读取每一行，删除用户 id(我们并不真正需要)，并将每一行解析成一个浮点数组。我们也可以将值保持为整数，但是由于我们很可能要执行诸如平均之类的运算，我们还不如直接转换为易于使用的类型。我们还将把标题保存在一个单独的数组中，以便以后可以将列映射到正确的标记名。

***清单 5-1*** 。读取内存中的数据集

```py
open System
open System.IO

let folder = __SOURCE_DIRECTORY__
let file = "userprofiles-toptags.txt"

let headers,observations =

    let raw =
        folder + "/" + file
        |> File.ReadAllLines

    // first row is headers, first col is user ID
    let headers = (raw.[0].Split ',').[1..]

    let observations =
        raw.[1..]
        |> Array.map (fun line -> (line.Split ',').[1..])
        |> Array.map (Array.map float)

    headers,observations
```

现在我们有了数据，让我们通过计算基本统计数据，比如平均值、最小值和最大值，来看看每个变量是什么样子的。我们在这里使用了两个小技巧。Array.iteri 允许我们遍历一个数组，并在遍历过程中捕获当前索引；因此，我们可以对标题进行迭代，并使用当前索引 I 从观察值中提取第 I 列。在第二个技巧中，我们在 printfn 中使用高级格式化程序来强制实现一致的列宽和小数位数，从而使输出更具可读性。例如，printfn " % 4.2f " 12.34567；使用格式化程序%4.2f，它将只保留小数点后两位数，并且(除非它不够)将把列填充为四个字符。

***清单 5-2*** 。基本数据集统计

```py
printfn "%16s %8s %8s %8s" "Tag Name" "Avg" "Min" "Max"
headers
|> Array.iteri (fun i name ->
    let col = observations |> Array.map (fun obs -> obs.[i])
    let avg = col |> Array.average
    let min = col |> Array.min
    let max = col |> Array.max
    printfn "%16s %8.1f %8.1f %8.1f" name avg min max)
>
        Tag Name      Avg      Min      Max
            .net      3.5      0.0    334.0
            ajax      1.7      0.0    285.0
         android      7.1      0.0   2376.0
          arrays      6.4      0.0    327.0
         asp.net      2.6      0.0    290.0
// snipped for brevity
```

简单地浏览一下输出，有几点很突出。首先，每个标签的最小值都是零，这意味着在每个标签中没有用户是活动的。然后，在每种情况下，我们都有一个较低的平均值，与最大值相比，相当接近于零。这表明，对于每个标签，大部分用户的活跃度很低或没有活跃度，少数用户活跃度很高。这并不完全令人惊讶，因为我们收集了每个标签的顶级用户的使用数据。与经典的钟形分布不同，在钟形分布中，值很好地分布在平均值周围，每个标签的活动水平分为两组(非常活跃或非常不活跃的用户)，中间没有任何东西。

数字表格往往水平太低，无法辨别更大的趋势。让我们加上 FSharp。通过 NuGet 绘制图表，并在条形图上绘制每个变量([图 5-5](#Fig5) )以获得对事物更好的感觉。

***清单 5-3*** 。按标签绘制平均使用情况

```py
#r @"..\packages\FSharp.Charting.0.90.9\lib\net40\FSharp.Charting.dll"
#load @"..\packages\FSharp.Charting.0.90.9\FSharp.Charting.fsx"
open FSharp.Charting

let labels = ChartTypes.LabelStyle(Interval=0.25)

headers
|> Seq.mapi (fun i name ->
    name,
    observations
    |> Seq.averageBy (fun obs -> obs.[i]))
|> Chart.Bar
|> fun chart -> chart.WithXAxis(LabelStyle=labels)
```

![9781430267676_Fig05-05.jpg](../images/00105.jpeg)

[图 5-5](#_Fig5) 。标签的平均使用率

该图显示了几个明显比其他标签大的标签(Javascript、Java、C#、Python、jQuery、PHP、SQL 和 C++)，后面的标签相比之下要小得多。这是有用的信息，但是有限的。我们希望看到一个更全面的视角，而不是孤立的事实，来帮助我们理解这些主题之间可能存在的关系。这是我们将在接下来的部分中研究的内容，首先使用 k-means 聚类来识别具有相似行为的用户组，然后使用主成分分析来更好地理解变量之间的关系。

用 K-均值聚类法寻找聚类

例如，当我们说“这个数据集有两个聚类”时，我们的意思是什么我们的想法是这样的:观测不是均匀分布的，而是集中在两个区域。另一种说法是，“在这个数据集中，大致有两种类型的观察，类型 1 和类型 2。”两个高密度区域分别包含类型 1 和类型 2 的观测值——即彼此相当接近的观测值(见[图 5-6](#Fig6) )。

![9781430267676_Fig05-06.jpg](../images/00106.jpeg)

[图 5-6](#_Fig6) 。数据聚类

我们可以用一种稍微不同的、或许更简单的方式来思考这个问题。一个聚类中的所有观测值都是理想化的基础观测值的微小变化。这大大简化了问题:我们可以将一个集群简化为整个群体的典型代表，而不必检测集群的边界。这些“虚构的代表”被称为**。**

 **![9781430267676_Fig05-07.jpg](../images/00107.jpeg)

 。簇和质心

为什么是质心？如果我想要一个好的集群代表，它应该在集群中占据中心位置，以便远离任何集群观察。在这个框架中，确定一个聚类的质心的一个明显的候选是其成分的平均值。相反，这形成了一个非常简单的分类规则:如果我想确定一个观察值属于哪个簇，我只需查找所有可用的质心并选择最接近的一个。

改进簇和质心

这一切都很好，但我们如何找到这些惊人的质心？在这一节中，我们将以一种定义更好的聚类的方式来充实更新质心的方法。如果我们重复应用这个更新过程，我们将有一个算法，从任意质心开始，逐步细化聚类。这是 **k-means 聚类算法**的核心。

![Image](../images/00011.jpeg) **注意**由于名字有些相似，人们有时会混淆 k-means 和 k 近邻算法。他们几乎没有共同之处——小心潜在的混淆！

让我们假设，通过某种魔法，您碰巧知道应该在数据集中找到多少个聚类，比如说，两个。假设你开始对质心进行两次随机猜测。然后，您可以做的第一件事是确定数据集中的哪个观察值属于哪个质心。对于每个观察，只需寻找最近的质心，并将观察分配给它。

[图 5-8](#Fig8) 取自[图 5-6](#Fig6) 中的虚构数据集，两个“真实的”集群用浅灰色边界标出。我们从两个任意的质心 A 和 B 开始，用一个将每个数据点分配给最近的质心。较暗的边界大致代表了基于当前质心的簇的样子。

![9781430267676_Fig05-08.jpg](../images/00108.jpeg)

[图 5-8](#_Fig8) 。向质心分配观测值

我们刚刚做的是定义聚类候选。分配给相同质心的所有观察值形成一个可能的聚类。如果当前聚类质心是“好质心”，则它将位于该聚类的中间。让我们调整质心位置 ( [图 5-9](#Fig9) )并将其移向集群中的平均位置，即中心。

![9781430267676_Fig05-09.jpg](../images/00109.jpeg)

[图 5-9](#_Fig9) 。调整质心

当我们这样做时，请注意一些有趣的事情发生了。将质心移向集群内的平均位置意味着它倾向于向高密度区域移动，高密度区域将更多的重量集中在一个小区域，并充当“吸引子”作为一个例子，考虑图 5-10 中 [的质心 B 正在发生什么；它自然地向位于左侧的“真实”星团移动。同时，它也远离了孤立的边缘观测，比如右边的两个。这将倾向于从聚类中去除非特征元素，并给数据点一个被另一个质心吞噬的机会。](#Fig10)

![9781430267676_Fig05-10.jpg](../images/00110.jpeg)

[图 5-10](#_Fig10) 。更新的质心和簇

一旦执行了更新，我们可以重复该过程，并重新计算每个观察被分配到哪个质心，以及质心的更新位置。

我们应该什么时候停止？注意，如果在更新质心位置之前和之后将相同的精确观测值分配给相同的聚类，那么质心将不再移动。每个质心已经占据了一个位置，该位置是其聚类的平均值，更新它不会改变它的位置。

这就是 k-means 聚类算法的基本工作方式:将数据集分成 k 个聚类，选取 k 个任意质心，并逐步更新它们的位置，直到它们达到稳定的位置。希望这一解释足以提供一些关于为什么这种方法可行的见解，并可能导致位于高密度星团中的稳定质心；我们会就此打住，甚至不会去证明它是否真的有效。

该算法可以看作是定点方法的一个应用。该过程假设稳定的质心存在，并且通过逐步消除质心定义的聚类和聚类定义的质心之间的差异来尝试实现自洽(可以这么说)。仍然有一些重要的问题我们没有回答，例如我们如何计算 k 或簇的数量——但是首先，让我们继续下去，并开始在 F#中实现该算法，假设我们知道 k 的值是正确的，并看看这会将我们引向何处。

实现 K 均值聚类

算法描述中出现了一些概念:给定一组观察值，我们需要能够根据最短距离将它们分配给 k 个质心中的一个，并且我们还需要将一组观察值缩减为一个质心。到目前为止，没有什么听起来特别适合我们的数据集，所以让我们尝试在它自己的单独模块中编写一个通用版本的算法，并通过稍后将其加载到脚本中来解决手头的问题。

这给了我们一个坚实的起点。从外部来看，该算法应该是这样的:我们需要提供一个观察值集合和多个聚类 k。我们还需要一个距离定义，以便我们可以将一个观察值分配给最近的质心，以及一种将来自同一聚类的观察值减少到单个聚合观察值(更新的质心)的方法。在伪代码中，下面是我们将要实现的内容的概要:

```py
clusterize
       1\. initialize
               assign every observation an initial centroid ID of 0
               pick k distinct observations as the initial centroids 1 to k
       2\. search
               assign each observation to the closest centroid
               if no assignment changes, done: we have good, stable centroids
               else
                      group observations assigned to the same cluster
                      reduce the observations to their average centroid
                      repeat with the new centroids
       3\. search, starting from the initial centroids
```

至少，我希望输出返回算法识别的所有质心；另外，如果能得到一个“分类器”函数就更好了，给定一个任意的观察值，它会直接告诉我它被分配到哪个质心。

让我们开始实施。首先，我们需要一个新文件——姑且称之为 k means . fs——然后我们可以开始处理一个 clusterize 函数。在开始迭代更新集群和分配之前，我们需要初始化这两者，并决定如何表示它们。让我们给每个集群一个 ID，它只是一个整数，并将一个赋值定义为一个附有集群 ID 的观察。当然，我们可以为赋值和观察创建专用的类型，但是这看起来并不值得——毕竟，这完全是算法实现的内部工作。因此，为了避免创建不必要的类型，我们将在这里简单地使用元组，在这两种情况下，将集群 ID 存储在第一个槽中，将观察值存储在第二个槽中。

然后我们可以用几行代码初始化这个过程。创建一个随机数生成器，随机选取 k 个观察值作为质心的起始候选值，聚类 ID 从 1 到 k。这并不是不合理的:选取现有的观察值可以保证质心位置不会完全不现实。同时，这也意味着我们有选择“坏的”初始质心的风险。我们暂时忽略那个问题，以后再来讨论。

我们的第一个问题是从我们的数据集中随机选取 k 个不同的观察值，我们将使用它们作为起始质心。让我们用简单的方法来做，从 0 到大小- 1 的区间中挑选随机数，并将它们添加到一个集合中(该集合将自动检测并消除重复)，直到所选观测值的数量为 k，我们想要的聚类数量。这并不完美；特别是，如果我们试图挑选一大组不同的索引，与数据集大小相比，这将是非常低效的。然而，假设集群的数量相比之下非常小，这就可以了。

***清单 5-4*** 。选择 k 个观测值作为初始质心

```py
namespace Unsupervised
module KMeans =

    let pickFrom size k =
        let rng = System.Random ()
        let rec pick (set:int Set) =
            let candidate = rng.Next(size)
            let set = set.Add candidate
            if set.Count = k then set
            else pick set
        pick Set.empty |> Set.toArray
```

至于赋值，我们给它们一个任意的起始质心 id 0，它不匹配任何真正的质心 ID。假设所有质心的 id 都大于 1，这意味着在我们的第一遍中，每个观察值都保证被分配一个不同的质心，并且该算法将至少完成一个完整的更新周期。

***清单 5-5*** 。初始化聚类分析算法

```py
let initialize observations k =
    let size = Array.length observations

    let centroids =
        pickFrom size k
        |> Array.mapi (fun i index ->
            i+1, observations.[index])

    let assignments =
        observations
        |> Array.map (fun x -> 0, x)

    (assignments,centroids)
```

现在我们有了一个起点，我们需要做的就是编写更新程序，将每个观察值分配给它最近的质心，这给了我们一个新的分配，并将其与前一个进行比较。如果没有观测值改变聚类，我们就完成了(每个聚类都将保持不变，质心也是如此)，否则，按聚类 ID 对观测值进行分组，将它们减少到它们的平均质心，并继续进行。我们现在将在 clusterize 函数中定义一个递归函数搜索，生成初始值，并开始递归更新质心。

***清单 5-6*** 。递归集群更新

```py
let clusterize distance centroidOf observations k =
    let rec search (assignments,centroids) =
        // Centroid ID of the closest centroid
        let classifier observation =
            centroids
            |> Array.minBy (fun (_,centroid) ->
                distance observation centroid)
            |> fst
        // Assign each observation to closest centroid
        let assignments' =
            assignments
            |> Array.map (fun (_,observation) ->
                let closestCentroidId = classifier observation
                (closestCentroidId,observation))
        // Check if any observation changed cluster
        let changed =
            (assignments,assignments')
            ||> Seq.zip
            |> Seq.exists (fun ((oldClusterID,_),(newClusterID,_)) ->
                not (oldClusterID = newClusterID))

        if changed
        then
            let centroids' =
                assignments'
                |> Seq.groupBy fst
                |> Seq.map (fun (clusterID, group) ->
                    clusterID, group |> Seq.map snd |> centroidOf)
                |> Seq.toArray
            search (assignments',centroids')
        else centroids,classifier

    // start searching from initial values
    let initialValues = initialize observations k
    search initialValues
```

就是这样:我们有一个聚类算法！给定一组观察值和一个 k 值，如果我们为想要使用的距离和缩减函数提供一个定义，我们应该得到聚类和一个将任何观察值分配给适当聚类的函数。

集群堆栈溢出标签

让我们回到脚本文件，看看如果我们对数据集进行聚类会发生什么。为了运行我们的聚类算法，我们需要几个元素:加载并打开脚本中的 KMeans 模块、观察值、处理观察值的距离和缩减函数，以及 k 值，即我们正在搜索的聚类数。

运行聚类分析

让我们使用欧几里得距离，将一组观察值简化为它们的平均值。这可以通过我们脚本中的以下代码轻松完成:

***清单 5-7*** 。定义聚类的距离和归约

```py
#load "KMeans.fs"
open Unsupervised.KMeans

type Observation = float []

let features = headers.Length

let distance (obs1:Observation) (obs2:Observation) =
    (obs1, obs2)
    ||> Seq.map2 (fun u1 u2 -> pown (u1 - u2) 2)
    |> Seq.sum

let centroidOf (cluster:Observation seq) =
    Array.init features (fun f ->
        cluster
        |> Seq.averageBy (fun user -> user.[f]))
```

我们现在准备搜索集群。我们将排除没有使用标签的观察结果(也就是说，在此期间用户在所选标签上完全不活动)，完全任意地为 k 选择值 5—并调用 clusterize:

***清单 5-8*** 。数据集聚类

```py
let observations1 =
    observations
    |> Array.map (Array.map float)
    |> Array.filter (fun x -> Array.sum x > 0.)

let (clusters1, classifier1) =
    let clustering = clusterize distance centroidOf
    let k = 5
    clustering observations1 k
>
val clusters1 : (int * Observation) [] =
  [|(1,
     [|2.47860262; 1.063755459; 9.765065502; 5.454148472; 2.333624454;
       2.293449782; 5.173799127; 10.98689956; 4.822707424; 5.772052402;
       1.693449782; 7.274235808; 9.717030568; 1.220087336; 23.49956332;
       10.12751092; 5.999126638; 2.011353712; 2.509170306; 7.932751092;
       6.620087336; 12.78777293; 7.658515284; 7.425327511; 6.794759825;
       5.561572052; 9.624454148; 4.525764192; 1.031441048; 4.483842795|]);
    // snipped for brevity
    (2,
     [|0.0; 0.0; 19.0; 17.0; 19.0; 0.0; 0.0; 69.0; 0.0; 0.0; 0.0; 0.0; 0.0;
       0.0; 68.0; 0.0; 0.0; 0.0; 0.0; 4159.0; 0.0; 821.0; 17.0; 56.0; 0.0; 0.0;
       7054.0; 1797.0; 0.0; 0.0|])|]
val classifier1 : (Observation -> int)
```

正如预期的那样，我们得到了两个东西:一个由五个集群组成的数组，每个集群都有一个 ID 和一组值，这些值表示每个标签的集群配置文件。举个例子，在上面的输出中，(2，[| 0.0；0.0;19.0;// snipped // 0.0|])表示 ID 为 2 的群集；该群中的普通用户从未使用过标签。NET 或 ajax(我们列表中的前两个标签)，但是使用了 19 次 Android 标签。请注意，因为我们随机选取初始质心，所以很可能每次运行您都会观察到不同的结果(我们稍后将回到这一点)。

分析结果

让我们面对现实吧，很难解释当前形式的输出。第一件有帮助的事情是绘制每个集群的地图，并将标签名称与我们拥有的数字对齐。让我们这样做:

```py
clusters1
|> Seq.iter (fun (id,profile) ->
    printfn "CLUSTER %i" id
    profile
    |> Array.iteri (fun i value -> printfn "%16s %.1f" headers.[i] value))

>
CLUSTER 4
            .net 2.9
            ajax 1.1
         android 6.1
          arrays 5.0
         asp.net 2.4
         // snipped for brevity
```

这个好一点；至少我们可以开始扫描每一个星团，寻找某种模式的出现。然而，这些信息仍然有点让人不知所措——让我们看看图表是否能帮助我们更清楚地可视化我们的输出(见[图 5-11](#Fig11) ):

```py
Chart.Combine [
    for (id,profile) in clusters1 ->
        profile
        |> Seq.mapi (fun i value -> headers.[i], value)
        |> Chart.Bar
    ]
|> fun chart -> chart.WithXAxis(LabelStyle=labels)
```

![9781430267676_Fig05-11.jpg](../images/00111.jpeg)

[图 5-11](#_Fig11) 。集群配置文件(详细信息请参见源代码包)

得到的图表更容易解析。在这种情况下，我看到的是

*   cluster 2 对于 Java 和 Android，以及 Python 和 Regex(！);
*   cluster 4 在 Javascript、JQuery、html 和 css 以及 PHP 上有很强的峰值；
*   集群 5 在 C#上很强。NET 和 WPF——还有 SQL、SQL Server 和 MySQL
*   cluster 3 有针对 C 和 C++的尖峰，也有针对 Linux 和数组的暗示；和
*   簇 1 没有可辨别的尖峰。

这很有意思。一方面，聚类算法确实运行了，并且产生了聚类，出现的一些关联是有意义的。看到 Java 和 Android 放在一起并不奇怪。将 Javascript、JQuery 和 html 等 web 客户端技术放在一起，或者将 SQL、MySQL 和 SQL server 等数据库相关的标签放在一起，这是合乎逻辑的。我们的算法确实提取了有意义的关系。同时，似乎也有一些奇怪的地方:Java、Python 和 Regex 真的相关吗？这个奇怪的混合桶呢。NET 和数据库开发人员？

在进一步讨论之前，我们先来看另一条信息。每个星团有多大？

```py
observations1
|> Seq.countBy (fun obs -> classifier1 obs)
|> Seq.iter (fun (clusterID, count) ->
    printfn "Cluster %i: %i elements" clusterID count)
>
Cluster 4: 1137 elements
Cluster 2: 22 elements
Cluster 1: 23 elements
Cluster 5: 20 elements
Cluster 3: 15 elements
```

那不太好。我们这里有一个巨大的集群，其中包含了我们大约 93%的观测数据，还有四个集群，每个都包含了不到 2%的数据集。在一个完美的世界中，我们希望看到在规模上有一定可比性的集群。这是怎么回事？

好簇，坏簇

让我们后退一步，想一想我们的方法可能会出什么问题，以及我们如何有希望改进事情。首先，有一个明显的可能性:也许数据中根本没有聚类。在这种情况下，我们真的无能为力——如果它不在那里，它就不在那里。然而，考虑到我们在最初的分析中看到了一些明智的标签组，我们将继续假设我们没有被淘汰。

我们的算法至少有两种可能出错的方式:我们可能在寻找数量不足的聚类，或者特征的尺度差异太大，无法以有意义的方式进行比较。让我们仔细看看这两个问题，并希望找出改进我们的集群的想法。

我们选择了一个完全任意数量的集群进行搜索——如果这个数字是错的，我们应该期待什么呢？考虑图 5-12 ，它展示了三个相当干净的星团，现在想象一下你被要求找出两个星团。

![9781430267676_Fig05-12.jpg](../images/00112.jpeg)

[图 5-12](#_Fig12) 。搜索不够多的聚类

该算法很可能以三种方式中的一种来识别两个聚类:其中一个聚类是“正确的”，而另一个聚类将聚集剩余的两个聚类，如图 5-13 所示。

这实际上是相当合理的:如果我们只能选择两个组，从剩下的两个组中选择一个好的组并创建一个平庸的组是我们所能得到的最好的结果。然而，假设我们随机选择起始质心，那么聚类将几乎完全取决于我们从哪三个随机观测开始。如果我们重复这个过程多次，我们将看到三个不同的结果出现，以不同的方式组合“真正的”聚类。在我们有三个以上集群的情况下，我们期望看到更多的组合出现。

![9781430267676_Fig05-13.jpg](../images/00113.jpeg)

 。从三个集群中搜索两个集群

这个问题听起来和我们在这里观察到的一致。如果多次运行聚类分析函数，您可能会观察到以下情况:每次运行的结果都不同，但是有一些一致的存储桶经常一起出现，但也可能与其他存储桶混在一起。举个例子，Java 和 Android 的组合频繁出现，不管有没有 Python。

如果我们想解决这个问题，我们需要将集群的数量增加到一个实际可行的值；也就是说，找到一个接近实际聚类数的 k 值。

在研究了 k-means 聚类出错的另一种方式之后，我们将在下一节中再次讨论这个问题。考虑图 5-14 中描述的情况。我们这里有两个集群，以{-2 为中心；0}和{ 2；0}，但是在 X 轴和 Y 轴上具有非常不同的伸长。而 X 值主要在[-3；-1]和[1；3]范围，Y 值在-20 和 20 之间变化。

![9781430267676_Fig05-14.jpg](../images/00114.jpeg)

[图 5-14](#_Fig14) 。比例差异很大的特征

如果我们在数据集上释放 k-means，要求两个聚类，会发生什么？同样，有一个随机因素，取决于我们选择的初始质心，但一个合理的结果是:我们可以很容易地得到两个集群，一个集中在{ 0；10}，另一个在{ 0；-10}.这是为什么呢？

这里的问题是，这两个特征处于非常不同的尺度上，因此，X 轴上存在的任何差异(第一个特征)与 Y 轴上的扩展相比都将完全相形见绌。换句话说，使用原始数据上的距离很难显示两点之间的距离(见[图 5-15](#Fig15) )。

![9781430267676_Fig05-15.jpg](../images/00115.jpeg)

[图 5-15](#_Fig15) 。关闭点

我们如何解决这个问题？简单:如果问题源于不同比例的要素，我们需要转换数据，以便相同量级的差异在所有要素之间具有可比性。换句话说，当某个观察值在某个要素上具有较低或较高的值时，对距离函数的影响应该是相当的。

一种常见的方法是使用以下公式重新调整每个特征:x' 图 5-16 ):

![9781430267676_Fig05-16.jpg](../images/00116.jpeg)

[图 5-16](#_Fig16) 。特征，重新缩放

这不是重新缩放要素的唯一方法；根据数据，其他方法可能更合适。稍后我们将看到另一种重新调整特征的方法。要素缩放有点像一门黑色艺术，而且没有一种独特的方法。作为一个经验法则，问问你自己你使用的距离是否是在比较可比较的事物。

重新调整我们的数据集以改进聚类

我们最初的聚类提取了一些似乎合理的信息；它也有明显的问题，这与聚类数量不足(我们每次运行算法都会得到不同的结果)和特征尺度的差异是一致的。在处理确定要搜索的足够数量的聚类这一更困难的任务之前，让我们从重新调整数据集的简单问题开始。

您可能已经意识到了这样一个事实，我们最初的集群检测到的峰值对应于使用频率最高的标签:Javascript、Java、C#、Python 等等。一般来说，虽然该算法每次运行都会产生不同的聚类，但是会出现相同的标签集。这是有道理的，因为高容量标签会产生很大的距离差异。这不是很好，因为我们的算法本质上是在寻找高流量标签，而不会在低流量标签上发现更多微妙的差异。

这里的大问题是每个标签上的活动量相当不同:JavaScript 的平均使用量比 iPhone 标签高 20 倍以上。我们可以按照前面描述的方法重新调整要素的比例，并重新映射它们，使其范围从 0 到 1。我们将采取不同的方向。我们可以这样描述我们的数据集:我们相信用户可以通过一些兴趣档案进行分类，但是他们在 StackOverflow 上的活跃程度有很大的不同。为了直接比较他们的个人资料，不管他们在网站上有多活跃，我们想要的是消除由于活跃程度造成的差异。例如，考虑这三个假设的用户:

|   | 

Java Script 语言

 | 

C#

 |
| --- | --- | --- |
| 用户 1 | One hundred | Twenty |
| 用户 2 | Ten | Two |
| 用户 3 | Fifty | Fifty |

从原始距离来看，用户 1 和用户 3 最相似，因为他们比用户 2 活跃得多。然而，可以说，用户 1 和用户 2 非常相似——比用户 1 和用户 3 更相似。虽然用户 1 比用户 2 活跃十倍，但是他们花时间的地方是相同的。解决这个问题的一个方法是按用户缩放:对于每一行，将最大的标签缩放到 100%，并相应地划分其他标签。

|   | 

Java Script 语言

 | 

C#

 |
| --- | --- | --- |
| 用户 1 | 100% | 20% |
| 用户 2 | 100% | 20% |
| 用户 3 | 100% | 100% |

如果我们这样做，用户 1 和用户 2 会突然变得完全一样，而用户 3 则完全不同。我们在这里真正做的是从等式中去除用户活动水平。这里的隐式模型可以这样表述:“有几个原型 StackOverflow 用户对相同的标记感兴趣。然而，他们中的一些人比其他人有更多的时间，所以他们的活动比例会更高。”请注意，这也意味着我们现在忽略了用户在网站上的活跃程度。这样对吗？也许是，也许不是——这个问题没有明确的答案。

无论如何，让我们尝试一下，仍然使用“坏”数量的集群。实际上，我们需要做的就是进行观察，并将每个数字除以该行中的最大值。

***清单 5-9*** 。将观察结果标准化为类似活动

```py
let rowNormalizer (obs:Observation) : Observation =
    let max = obs |> Seq.max
    obs |> Array.map (fun tagUse -> tagUse / max)

let observations2 =
    observations
    |> Array.filter (fun x -> Array.sum x > 0.)
    |> Array.map (Array.map float)
    |> Array.map rowNormalizer

let (clusters2, classifier2) =
    let clustering = clusterize distance centroidOf
    let k = 5
    clustering observations2 k
```

我们仍然没有处理 k 应该是什么，并且，作为结果，我们很可能仍然寻找太少的集群；每次运行的结果仍然不稳定。然而，即使这样，我们也应该注意一些事情。首先，集群大小现在更加平衡了。运行与之前相同的代码，我们会得到以下结果:

```py
observations2
|> Seq.countBy (fun obs -> classifier2 obs)
|> Seq.iter (fun (clusterID, count) ->
    printfn "Cluster %i: %i elements" clusterID count)

>
Cluster 4: 480 elements
Cluster 5: 252 elements
Cluster 2: 219 elements
Cluster 1: 141 elements
Cluster 3: 125 elements
val it : unit = ()
```

此外，一些关联现在看起来更清楚了，比如 C/C++对，甚至是以前没有注册的对，比如 Python 和 Django，或者 Ruby 和 Ruby on Rails，其中一个元素的活动太少，以至于不能改变距离(参见[图 5-17](#Fig17) )。

![9781430267676_Fig05-17.jpg](../images/00117.jpeg)

[图 5-17](#_Fig17) 。行规范化后的聚类(详细信息请参见源代码包)

确定要搜索多少个聚类

我们仍然没有解决计算出我们应该搜索多少个集群的关键问题。不幸的是，正如我们前面讨论的，这不是一个我们可以忽视的问题:搜索太少的集群将导致不稳定的结果。因此，让我们首先弄清楚如何处理这个问题，然后将它应用到我们的数据集。

什么是好的集群？

我将再次重申，识别集群不是一项无足轻重的任务，因为关于什么是集群存在一些模糊性。考虑[图 5-18](#Fig18) :你看到了多少个集群？您可以使用三个大的集群，或者七个集群——以及两者之间的更多组合(根据记录，这是由七个集群生成的)。

![9781430267676_Fig05-18.jpg](../images/00118.jpeg)

[图 5-18](#_Fig18) 。集群是不明确的

希望这个例子至少传达了我们所说的不完全清晰的集群的含义。如果我们不能同意或解释，即使是非正式地，我们所说的集群是什么意思，那么很有可能机械算法也会发现这项任务具有挑战性。

那么，我们在集群中寻找什么？有几件事，真的。聚类是一组彼此相当相似的观察值，并且与其他聚类明显不同。换句话说，相对于数据集中的总体离差，每个观测值都应靠近其质心。这就提出了一种测量一组质心代表数据有多好的方法:获取每个观测值，测量它离质心有多远，然后将这些值相加。这个度量被称为**残差平方和**，通常缩写为 **RSS** 。

RSS 本质上采用了我们在[第四章](4.html#AFM60-841455c729754b8aac560d608a86cf91)的回归上下文中已经看到的一个想法，并将它转置到集群中。在回归的情况下，我们测量模型与成本函数的数据匹配程度，计算每个数据点与相应预测值之间的平方差，并取它们的总和。在我们现在的例子中，属于一个聚类的每个观察值被建模为它的质心，我们通过将每个观察值与对应的质心进行比较来测量我们的聚类与数据的拟合程度。

我们试试不同的 k 值，找出最小化 RSS 的那个怎么样？嗯，这有点问题。假设我们有一个包含 N 个观察值的数据集，有一个明显的值可以最小化 RSS，那就是 k = N，如果我们为每个观察值创建一个质心，我们当然会得到一个完美的拟合。还要注意，如果我们有 k 个质心，增加一个质心将总是减少 RSS(更准确地说，RSS 永远不会增加)。

这是有用的，因为它澄清了我们在追求什么。我们想要的是一个“有效的”数据总结，同样遵循奥卡姆剃刀原理。我们希望聚类能够很好地解释数据，但又尽可能简单。只考虑 RSS 忽略了第二点:复杂性没有损失，我们的聚类会过度拟合并简单地复制数据集本身，没有任何形式的摘要。

那么，我们该怎么办呢？一般的答案是，如果我们有一个聚类质量的度量，它可能应该包括 RSS(模型与数据的拟合程度)，以及复杂性的惩罚项。同样，这是一个棘手的问题，这里没有灵丹妙药。有几种流行的方法。首先，假设您计算了值为 k 的 RSS。现在计算 k + 1 的 RSS:如果您几乎没有改进 RSS，那么增加集群的数量是不经济的。这种方法被称为肘方法——寻找 RSS 的相对改进下降的地方。

这个总的方向可以用来计算 k 的特定值有多好的数值度量。**赤池信息准则**，或者 **AIC** ，都是的热门选择。它将 RSS 和罚项 2 * m * k 结合在一个度量中，其中 m 是特征的数量，k 是聚类的数量。换句话说，更喜欢聚类少的模型，对特征多的模型更保守。还有几个其他的经典度量标准 (我们在这里不会实现)，比如 BIC(贝叶斯信息标准)或者 Calinski-Harabasz，它们遵循大致相同的一般原则。

哪个是对的？不幸的是，这要看情况。同样，集群是一个有点模糊的任务；所有这些方法都是启发式的，虽然它们遵循相似的一般方法，但它们不一定一致；识别有效的东西通常需要一些尝试和错误。在下一节中，我们将简单地使用 AIC，因为它特别容易实现。

在 StackOverflow 数据集上识别 k

让我们看看这是如何在手头的数据集上实现的。我们将用于校准 k 值的第一个试探法是行业中常见的“经验法则”,建议 k 值如下:

```py
let ruleOfThumb (n:int) = sqrt (float n / 2.)
let k_ruleOfThumb = ruleOfThumb (observations2.Length)
```

这种方法显然过于简单，无法在所有情况下都很好地工作；它唯一考虑的是数据集有多大！另一方面，它有简单的好处。我建议简单地用它来大致了解什么范围的值是合理的。在这种情况下，建议值为 25，我们将使用它作为 k 的上限。接下来，让我们试试 AIC。

***清单 5-10*** 。赤池信息标准(AIC)

```py
let squareError (obs1:Observation) (obs2:Observation) =
    (obs1,obs2)
    ||> Seq.zip
    |> Seq.sumBy (fun (x1,x2) -> pown (x1-x2) 2)

let RSS (dataset:Observation[]) centroids =
    dataset
    |> Seq.sumBy (fun obs ->
        centroids
        |> Seq.map (squareError obs)
        |> Seq.min)

let AIC (dataset:Observation[]) centroids =
    let k = centroids |> Seq.length
    let m = dataset.[0] |> Seq.length
    RSS dataset centroids + float (2 * m * k)
```

为了找出一个合适的 k 值，我们将尝试 k = 1 到 k = 25 之间的每一种可能性。因为聚类本身对于低 k 值将是不稳定的，所以我们将对每个值运行该算法几次，并平均出相应的 AIC，以便我们可以消除由于例如不幸的初始值选择而导致的潜在侥幸:

```py
[1..25]
|> Seq.map (fun k ->
    let value =
        [ for _ in 1 .. 10 ->
            let (clusters, classifier) =
                let clustering = clusterize distance centroidOf
                clustering observations2 k
            AIC observations2 (clusters |> Seq.map snd) ]
        |> List.average
    k, value)
|> Chart.Line
```

[图 5-19](#Fig19) 显示了我们的搜索结果，绘制了不同 k 值的平均 AIC 值。随着 k 从 1 增加到 5，AIC 值稳步下降，保持缓慢下降，直到我们达到 k = 10，并开始向上攀升，以获得更大的 k 值。这表明我们应该使用 k = 10 的值，这将最小化 AIC，并应该给我们提供干净集群的最佳机会。然而，请注意，曲线在 10 左右相当平坦。

![9781430267676_Fig05-19.jpg](../images/00119.jpeg)

[图 5-19](#_Fig19) 。寻找 k-最小化 AIC

我们最后的集群

现在我们已经根据 AIC 为 k 选择了值 10，让我们看看会发生什么。我们现在可以多次运行聚类算法，以避免失误，并选择使特定 k 值的 RSS 最小的模型

***清单 5-11*** 。最终聚类

```py
let (bestClusters, bestClassifier) =
    let clustering = clusterize distance centroidOf
    let k = 10
    seq {
        for _ in 1 .. 20 ->
            clustering observations2 k
    }
    |> Seq.minBy (fun (cs,f) ->
        RSS observations2 (cs |> Seq.map snd))
```

我们的集群中有什么？让我们检查一下，然后打印出每个集群中使用率最高的标签。请注意，一般来说，只选择高值可能不是一个好主意；我们真正寻找的是与其他观察结果显著不同的值。一般来说，从平均值中选择大的绝对差值可能是一个更好的主意，但是在我们的情况下，平均值非常低，我们可以安全地简单地寻找异常高的值。

```py
bestClusters
|> Seq.iter (fun (id,profile) ->
    printfn "CLUSTER %i" id
    profile
    |> Array.iteri (fun i value ->
        if value > 0.2 then printfn "%16s %.1f" headers.[i] value))

>
CLUSTER 5
         android 0.3
            java 0.5
CLUSTER 7
          django 0.3
          python 0.9
CLUSTER 4
            html 0.3
      javascript 0.9
          jquery 0.5
CLUSTER 6
            ruby 0.7
   ruby-on-rails 0.7
CLUSTER 10
               c 0.4
             c++ 0.8
CLUSTER 2
      javascript 0.3
           mysql 0.2
             php 1.0
CLUSTER 8
            .net 0.3
              c# 0.9
CLUSTER 9
           mysql 0.3
             sql 0.8
      sql-server 0.6
CLUSTER 1
             ios 1.0
     objective-c 0.6
CLUSTER 3
             css 0.9
            html 0.7
      javascript 0.5
          jquery 0.3
```

这看起来很不错。我怀疑您可以很容易地为每个概要文件想象出一幅画面，它对应于清晰的开发栈。这很棒:没费多大力气，我们就设法从原始数据中自动提取出有意义的信息。在对这些标签的含义没有任何理解的情况下，仅仅通过查看数据集和历史使用模式，k-means 就得出 Ruby 和 Ruby on Rails 是相关的，或者说一些我们称为 DBA 的人主要关注 SQL 相关的技术。

使用 k-means，我们可以从数据集中挤出更多的信息；然而，如果我们继续朝着这个方向发展，很可能会越来越难获得新的有趣的结果，因为我们目前的结果已经相当不错了。我们要做的是让它停留在 k-means，并引入一种完全不同的技术，主成分分析。我们将把它应用到同一个问题上，这样我们就可以比较这两种方法。

检测功能是如何相关的

尽管 k-means 聚类有很多优点，但它并不是全部。特别是，它没有提供这些特性如何协同工作的全面描述。我们可以通过检查每个集群配置文件找出一些关系，寻找明显高于或低于典型的特征，但这是一个乏味的手动过程。我们现在将探索的方法、**主成分分析**或 **PCA** 简而言之，在某种意义上是对 k-means 的补充:k-means 提供了一幅清晰的个人概况图，但没有太多关于特征如何相互关联的信息，PCA 主要是检测特征中的一些结构，而不是个体中的一些结构。

协方差和相关性

数据中的模式可以有无限多种形式。虽然检测所有这些模式是不可能的，但有些模式既有用又容易发现。一个这样的例子如下:“每当数量 A 增加，数量 B 也增加。”这就是**协方差**和**相关性**的意义所在。

两次测量 X 和 Y 之间的协方差定义为:

```py
Cov(X,Y) = average [(X-average(X)) * (Y-average(Y))]
```

一旦你开始分解它，你会发现这是一条合理的路。如果两个值 X 和 Y 一起上升和下降，那么当一个高于平均值时，另一个也应该高于平均值，乘积将为正。类似地，当一个低于平均值时，另一个也应该如此——它们的差的乘积(现在都是负数)仍然是正的。根据同样的推理，您会认为两个向相反方向移动的要素具有负协方差。

![Image](../images/00011.jpeg) **边注**重要的是要认识到协方差(或相关性)为零并不意味着特征之间没有关系。只是说明他们的关系不是一起上下移动那么简单。例如，描述一个正圆的数据点的相关性为零，即使它们的关系是完全确定的

协方差很大，但它有一个问题:它依赖于要素的比例，本身没有明显的比例，这使得解释它很困难。什么是高协方差？这要看数据。相关性是对协方差的一种修正，可以解决这个问题。形式上，特征 X 和 Y 之间的相关性定义为:

```py
Corr(X,Y) = Cov(X,Y) / (stdDev(X) * stdDev(Y))
```

其中 stdDev 代表**标准差**。根据您对统计数据的熟悉程度，这可能看起来有些吓人。不应该——让我们把它分解一下。我们关于协方差的问题是 X 和 Y 之间的尺度差异。为了使不同的特征具有可比性，我们需要将(X-average(X))和(Y-average(Y))两项缩减到相似的尺度。用(X-average(X))除以它的平均值——X 与其平均值之间的平均距离(相当拗口！)—会这样做，这正是标准差，通常表示为 sigma。

![Image](../images/00011.jpeg) **提示**标准偏差(及其平方，方差)通常被描述为一个特征的离差的度量。我个人觉得下面的描述很有帮助:*标准差 sigma 衡量的是到平均值的平均距离*。如果你对你的样本进行随机观察，标准差就是它与样本平均值的距离。

您可以扩展协方差部分，并将公式稍微重组为这个等价版本，这有助于我们更好地理解相关性是如何工作的:

```py
Corr(X,Y) = average [(X-avg(X))/stdDev(X) * (Y-avg(Y))/stdDev(Y)]
```

(X-average(X))/stdDev(X)项从协方差的平均值中提取差值，并通过 sigma 对其进行重新调整，使得平均差值现在为 1.0。X 项和 Y 项现在都在一个可比较的范围内，因此(我们不会在这里证明)，相关性有一个令人愉快的特性，它从-1.0 到 1.0。值 1.0 表示 X 和 Y 总是在一条完美的直线上一起生长，-1.0 表示在相反方向上的一条完美直线，0.0 表示没有检测到这种关系。相关性越接近 1.0 或-1.0，这种模式就越强，也就是说，要素大多一起移动，具有一定程度的不确定性。

顺便提一下，将测量值 X 转换为(X-average(X))/stdDev(X)是重新缩放要素的经典方法，称为 **z 值**。这比我们之前讨论的方法要复杂一些，我们只是简单地将[min；max]到[0；1]，但它也有一些优点。变换后的要素以 0 为中心，因此正值或负值可以立即解释为高或低。如果特征是合理的正态形状，即围绕其平均值呈钟形，结果也可能更“合理”。虽然根据定义，最小值和最大值是具有极值的异常值，但标准偏差通常是缩放观察值的更好度量。

StackOverflow 标签之间的相关性

让我们来看看它在我们的数据集上是如何工作的。为了方便起见，我们不在当前脚本中添加代码，而是创建一个新脚本 PCA.fsx 和一个模块 PCA.fs，我们将从中提取主成分分析代码。在本节中，我们还将大量使用线性代数和统计学，因此，与其重新发明轮子，不如让我们通过 NuGet 将 F#的 Math.NET 数值学和 Math.NET 数值学的参考资料添加到我们的项目中。我们的脚本将以与上一个脚本相同的方式开始，加载对 fsharp 的引用。从文件中绘制和读取标题和观察结果。

事实证明，Math.NET 有一个内置的关联矩阵。相关函数希望数据以要素的形式呈现，而不是以观察值的形式呈现，也就是说，我们需要将 1，600 个观察值转置成 30 行，与 30 个要素相对应。

***清单 5-12*** 。计算相关矩阵

```py
#r @"..\packages\MathNet.Numerics.3.5.0\lib\net40\MathNet.Numerics.dll"
#r @"..\packages\MathNet.Numerics.FSharp.3.5.0\lib\net40\MathNet.Numerics.FSharp.dll"

open MathNet
open MathNet.Numerics.LinearAlgebra
open MathNet.Numerics.Statistics

let correlations =
    observations
    |> Matrix.Build.DenseOfColumnArrays
    |> Matrix.toRowArrays
    |> Correlation.PearsonMatrix

>

val correlations : Matrix<float> =
  DenseMatrix 30x30-Double
          1    0.00198997  -0.00385582     0.11219     0.425774  ..     0.172283   0.0969075
 0.00198997             1   -0.0101171    0.259896      0.16159  ..  -0.00783435    0.025791
-0.00385582    -0.0101171            1   0.0164469  -0.00862693  ..  -0.00465378   0.0775214
// snipped for brevity
```

正如所料，我们得到了一个 30 x 30 的矩阵，其中每个单元对应于两个特定特征之间的相关性。令人放心的是，对角线全是 1(一个特征应该和自身完美相关)，矩阵是对称的(A 和 B，B 和 A 的相关性，应该是相同的)。这不是特别有用，因为数据太多了，所以让我们获取每个标签对，从矩阵中获取它们的相关性，并提取绝对值最大的 20 个，因为大的正值或负值表示相当强的相关性:

```py
let feats = headers.Length
let correlated =
    [
        for col in 0 .. (feats - 1) do
            for row in (col + 1) .. (feats - 1) ->
                correlations.[col,row], headers.[col], headers.[row]
    ]
    |> Seq.sortBy (fun (corr, f1, f2) -> - abs corr)
    |> Seq.take 20
    |> Seq.iter (fun (corr, f1, f2) ->
        printfn "%s %s : %.2f" f1 f2 corr)

> 
ios objective-c : 0.97
ios iphone : 0.94
mysql sql : 0.93
iphone objective-c : 0.89
sql sql-server : 0.88
css html : 0.84
.net c# : 0.83
javascript jquery : 0.82
ajax javascript : 0.78
mysql sql-server : 0.76
html javascript : 0.73
html jquery : 0.71
ajax jquery : 0.70
ajax html : 0.66
ajax json : 0.60
javascript json : 0.56
asp.net c# : 0.53
c c++ : 0.50
ruby ruby-on-rails : 0.50
mysql php : 0.48
```

![Image](../images/00011.jpeg) **提示**在编写的时候，F#中还没有内置的降序排序。作为替代，你可以用否定排序；例如，在 Seq.sortBy (fun (corr，f1，f2) - > - abs corr)中。

结果很合理。我们立即看到了几组齐头并进的功能:iOS、objective-c 和 iPhoneSQL、MySQL 和 SQL-server 等等。相关矩阵不费吹灰之力就从我们的数据集中提取了大量信息。

用主成分分析识别更好的特征

我们将相关矩阵视为特性之间关系的有用总结。另一种解释是，我们的数据集包含大量冗余信息。举个例子，iOS、objective-C 和 iPhone 的相关性非常接近 100%。这意味着我们几乎可以将这三个功能合二为一——比如说，“iOS 开发者。一旦你知道了三个中的一个是高(或低)，你就知道了另外两个。

与此同时，一些相关性处理起来有点棘手。例如，如果您深入研究数据集，您会发现 MySQL 和 SQL、SQL 和 SQL-Server 以及 MySQL 和 PHP 高度相关。这是否意味着，通过传递性，PHP 和 SQL-Server 走到了一起？我对此表示怀疑——从我个人的经验来看，我认为这是两种不同的关系，一边是 DBA，另一边可能是 LAMP 开发人员。他们碰巧对 MySQL 有一个共同的兴趣，但他们不是。

在本节中，我们将演示主成分分析(简称 PCA)，这是一种数据挖掘技术，它查看协方差或相关矩阵的结构，并使用它将数据重新组织为一组新的特征，将现有的特征组合成更有效地表示数据并避免冗余的组合。

用代数重新组合特征

对 PCA 为什么起作用的完整解释会让我们在代数上走得更远，但最终并不那么有用。我们会尝试通过一个说明性的例子来传达重点，在战略时刻故意使用一些摆手，让如此倾向的读者自行深入挖掘。

我们在这里追求的是一种重新组织数据集的方法。假设一个数据集只有两个特征，而不是 30 个。一种表示特征变换的方法是使用 2 x 2 矩阵。如果我们将一个观察值(两个元素的向量)乘以那个矩阵，我们将得到一个新的向量，它仍然具有两个特征:

```py
let observation = vector [ 2.0; 5.0 ]
let transform =
    matrix [ [ 1.0; 3.0 ]
             [ 2.0; 1.0 ] ]
transform * observation |> printfn "%A"

>
seq [17.0; 9.0]
```

变换矩阵中的每一行都可以看作权重。例如，第一行表示第一个变换后的要素组合了第一个原始要素的 1.0 部分和第二个要素的 3.0 部分。如果转换是单位矩阵(对角线上是 1，其他地方是 0)，转换将简单地返回原始值。

我们要找的是那种形式的矩阵。显然，我们不能选择任意的矩阵；它只会重新排列特征，但没有任何先验的理由来产生任何特别有用的东西。那么，我们想在这里实现什么呢？在这种特殊情况下，我们希望通过转换来重新排列特征，从而限制我们在相关矩阵中发现的信息冗余。我们还希望有一种方法来区分数据集中的主要趋势和不重要的差异。

这是用力挥手的地方。事实证明，有一种线性代数技术正是我们想要的:将矩阵分解成特征向量和特征值。在非常高的层次上，一个 N×X 的方阵 M 可以唯一地分解成多达 N 对特征值和特征向量(一个浮点数，和一个 N 元素的向量)，每一对都满足下面的等式:M x 特征向量=特征值 X 特征向量。换句话说，一个特征向量是一个方向，它将 M 拉伸了与其特征值相对应的幅度。

如果我们将这种技术应用于协方差矩阵，我们会得到什么？首先，每个特征向量代表一个通过组合现有特征构建的新特征。请记住，协方差矩阵测量每个要素平均值的差异；具有最大特征值的特征向量表示具有最大伸展的特征。因此，在协方差矩阵的情况下，它意味着我们从数据中心观察到最多联合变化的方向。换句话说，具有最大特征值的特征向量是以某种方式组合现有特征的特征，这种方式解释了我们在数据集中观察到的大多数差异。一旦第一个特征的影响被移除，第二大特征值隔离第二大信息组合，以此类推。

PCA 运行的一个小预览

到目前为止，这些都相当抽象。在开始实现 PCA 并将其应用于整个数据集之前，让我们用一个小例子来说明我们在追求什么。我们将使用即将编写的代码，忽略细节，只显示预期的结果类型，以阐明 PCA 的工作原理。

我们之前执行的相关性分析显示了两对特性之间的密切关系:iOS 和 objective-c，以及 SQL 和 SQL Server。让我们忽略所有其他特征，只对这四个进行主成分分析。

分析提取了 4 个特征值，量值分别为 98.73、38.59、5.69 和 0.61，共计 143.61。这些值代表每个新特性解释了多少原始信息。例如，第一个也是最大的分量足以捕捉数据中 98.73 / 143.61 ~ 69%的变化，第二个分量为 27%。换句话说，仅使用两个主要特征——两个主要的主成分——我们就可以从原始数据集中重现超过 95%的信息。最后两个组成部分只占不到 5%的信息，可能可以忽略，因为它们几乎没有带来额外的信息。

这些新功能是什么样子的？我们只需要看看特征向量就能弄清楚。[表 5-1](#Tab1) 以表格形式显示特征向量。每列对应一个组件，按重要性降序排列。每一行对应一个原始特征，每个值代表原始特征在组件中的“权重”。

[表 5-1](#_Tab1) 。主成分

![Table5-1.jpg](../images/00120.jpeg)

在这种情况下，我们看到的是组件 1 结合了 SQL 和 SQL Server，组件 2 Objective-C 和 iOS。这是非常好的:PCA 本质上将我们的数据集从四个简化为两个功能，“iOS /Objective-C”和“SQL / SQL Server”，几乎没有任何信息损失。

我们可以通过表示原始特征如何映射到主成分上来可视化该信息，如图 5-20 中的[所示。随着特征数量的增加，这尤其方便:它使得在数字表中很难发现的特征之间的眼球关系成为可能。](#Fig20)

![9781430267676_Fig05-20.jpg](../images/00121.jpeg)

[图 5-20](#_Fig20) 。投影到主要组件上的原始特征

另一种通常有用的图表是在原始观察值从原始特征转换成主要成分后，将原始观察值可视化。作为一个例子， [图 5-21](#Fig21) 显示 StackOverflow 用户通常不关心两个主题(除了一个孤独的个体)；积极参与 SQL 的人——也就是职位在图表右边的人——不关心 iPhone 开发，反之亦然。

![9781430267676_Fig05-21.jpg](../images/00122.jpeg)

[图 5-21](#_Fig21) 。投影在主成分上的原始观察值

希望这个小例子能让你对 PCA 有更好的理解。使用一点线性代数，我们基本上成功地将数据集缩小了一半，几乎没有任何信息损失。我们最终得到了两个特征(两个主要的主成分)，而不是最初的四个特征，这两个特征浓缩了信息，并为我们的数据集提供了更清晰的图像。现在我们已经看到了 PCA 能为我们做什么，让我们深入研究如何实现它，然后将它应用到整个数据集。

实施 PCA

实现本身并不复杂。我们将把通用代码移动到一个名为 PCA.fs 的单独代码文件中。PCA 依赖于数据集协方差矩阵的特征分解。Math.NET 没有内置协方差矩阵，但它很容易产生；我们计算每个特征列之间的协方差，并利用矩阵是对称的这一事实(即，行 r 和列 c 处的值等于行 c 和列 r 处的值)来避免重复工作。

***清单 5-13*** 。计算协方差矩阵

```py
namespace Unsupervised
module PCA =

    open MathNet
    open MathNet.Numerics.LinearAlgebra
    open MathNet.Numerics.Statistics

    let covarianceMatrix (M:Matrix<float>) =
        let cols = M.ColumnCount
        let C = DenseMatrix.create cols cols Matrix.Zero
        for c1 in 0 .. (cols - 1) do
            C.[c1,c1] <- Statistics.Variance (M.Column c1)
            for c2 in (c1 + 1) .. (cols - 1) do
                let cov = Statistics.Covariance (M.Column c1, M.Column c2)
                C.[c1,c2] <- cov
                C.[c2,c1] <- cov
        C
```

现在，让我们添加另一个方便的函数。PCA 期望特征被居中并且在相似的尺度上；为此，数据集通常会被**归一化**，首先减去平均值，使每个特征的平均值为零，然后通过标准偏差将其缩小，使其在平均值周围的分布为 1。

***清单 5-14*** 。特征标准化

    let normalize dim (observations:float[][]) =

```py
        let averages =
            Array.init dim (fun i ->
                observations
                |> Seq.averageBy (fun x -> x.[i]))

        let stdDevs =
            Array.init dim (fun i ->
                let avg = averages.[i]
                observations
                |> Seq.averageBy (fun x ->
                    pown (float x.[i] - avg) 2 |> sqrt))

        observations
        |> Array.map (fun row ->
            row
            |> Array.mapi (fun i x ->
                (float x - averages.[i]) / stdDevs.[i]))
```

对于主成分分析本身，几乎没有什么需要实现的了。从由多行观察值组成的数据集，我们计算协方差矩阵和特征分解。为了方便起见，我们还返回一个函数(稍后命名为“projector ”),该函数接收一个单独的观察值，并计算它在 PCA 所识别的新空间中的表示。

***清单 5-15*** 。主成分分析

    let pca (observations:float[][]) =

```py

        let factorization =
            observations
            |> Matrix.Build.DenseOfRowArrays
            |> covarianceMatrix
            |> Matrix.eigen

        let eigenValues = factorization.EigenValues
        let eigenVectors = factorization.EigenVectors

        let projector (obs:float[]) =
            let obsVector = obs |> Vector.Build.DenseOfArray
            (eigenVectors.Transpose () * obsVector)
            |> Vector.toArray

        (eigenValues,eigenVectors), projector
```

还记得我们之前描述的转换矩阵吗？这正是我们在这里所做的；我们只需要转置特征向量矩阵，因为因式分解返回的是列，我们需要的是行。

将主成分分析应用于 StackOverflow 数据集

是时候在我们的脚本中尝试一下了！使用我们的算法就像这样简单:

***清单 5-16*** 。在 StackOverflow 数据集上运行 PCA

#load "PCA.fs"

```py
open Unsupervised.PCA

let normalized = normalize (headers.Length) observations

let (eValues,eVectors), projector = pca normalized
```

让我们来看看输出。通常检查特征值，它提供了每个主成分(我们的新特征)包含多少信息的感觉。为了做到这一点，我们将简单地计算我们提取的 30 个特征值的总幅度，然后写出每个特征值占总幅度的百分比，以及到那时为止我们通过仅使用最具信息性的特征覆盖了总幅度的多少。

***清单 5-17*** 。特征权重分析

```py
let total = eValues |> Seq.sumBy (fun x -> x.Magnitude)
eValues
|> Vector.toList
|> List.rev
|> List.scan (fun (percent,cumul) value ->
    let percent = 100\. * value.Magnitude / total
    let cumul = cumul + percent
    (percent,cumul)) (0.,0.)
|> List.tail
|> List.iteri (fun i (p,c) -> printfn "Feat %2i: %.2f%% (%.2f%%)" i p c)

>
Feat  0: 19.14% (19.14%)
Feat  1: 9.21% (28.35%)
Feat  2: 8.62% (36.97%)
// snipped for brevity
Feat 28: 0.07% (99.95%)
Feat 29: 0.05% (100.00%)
```

这与我们的相关矩阵一致:前五个特征解释了超过 50%的数据集，前十个特征覆盖了接近 80%。相比之下，后十个特征各占不到 1%的信息。

![Image](../images/00011.jpeg) **注意**习惯上是按照特征值递减的顺序来看特征，但 Math.NET 是以相反的顺序返回特征值和特征向量。结果，例如，最重要的主分量将在特征向量矩阵的最后一列中找到。小心点！

分析提取的特征

我们能看看这些特征吗？每个特征向量直接将旧特征映射到新特征或主分量。我们所需要做的就是从我们之前从 PCA 获得的分析结果中获取特征向量，并将它的每个值映射到相应的标签名称。[清单 5-18](#list18) 根据任何一对组件绘制原始特征；请注意，当我们重建组件 x 时，我们是如何检索特征向量的第 30 个<sup class="calibre16">–x 列的，因为分析是按照重要性的增加来返回它们的。</sup>

[***清单 5-18***](#_list18) 。根据提取的零部件绘制原始特征

```py
let principalComponent comp1 comp2 =
    let title = sprintf "Component %i vs %i" comp1 comp2
    let features = headers.Length
    let coords = Seq.zip (eVectors.Column(features-comp1)) (eVectors.Column(features-comp2))
    Chart.Point (coords, Title = title, Labels = headers, MarkerSize = 7)
    |> Chart.WithXAxis(Min = -1.0, Max = 1.0,
        MajorGrid = ChartTypes.Grid(Interval = 0.25),
        LabelStyle = ChartTypes.LabelStyle(Interval = 0.25),
        MajorTickMark = ChartTypes.TickMark(Enabled = false))
    |> Chart.WithYAxis(Min = -1.0, Max = 1.0,
        MajorGrid = ChartTypes.Grid(Interval = 0.25),
        LabelStyle = ChartTypes.LabelStyle(Interval = 0.25),
        MajorTickMark = ChartTypes.TickMark(Enabled = false))
```

然后，我们可以通过简单地调用 principalComponent 1 2 来可视化这两个主要的主成分；；在 F#互动中。作为示例，[图 5-22](#Fig22) 显示了 PCA 确定的一些最大特征，即部件 1 与部件 2 以及部件 3 与部件 4。毫不奇怪，考虑到相关矩阵，主要特性非常清楚地映射到 SQL、MySQL 和 SQL-Server，而不是其他任何东西；让我们称之为“数据库管理员”第二个特征解释起来不太明显，似乎反对 WPF，在较小程度上也反对 C#-和。NET 相关的技术，比如 ASP.NET 和 ASP.NET MVC，到其他很多东西，特别是 Python/Django 和 Ruby。特性 3 也不完全明显，但似乎反对 web 和非 web，一边是 Ruby on Rails、Django、HTML 和 Javascript。特性 4 清楚地将 Python 和 Ruby 对立起来。

![9781430267676_Fig05-22a.jpg](../images/00123.jpeg)

![9781430267676_Fig05-22b.jpg](../images/00124.jpeg)

[图 5-22](#_Fig22) 。主要组件(详见源代码包)

我通常会深入挖掘，查看每个顶级组件，试图理解它们捕捉到了什么。然而，这将涉及制作更多的图表，而不会给讨论增加太多内容。所以我就说到这里——请随意进一步研究，看看从这个数据集中还能学到什么！

让我们来看看另一个问题:当透过新特征的镜头看时，原始的观察结果是什么样的？我们只需要使用投影函数(这是 PCA 输出的一部分)将观察值转换成新的坐标。请注意，我们预先计算了最小值和最大值，以便以可比较的比例显示每个图表。

***清单 5-19*** 。根据主成分绘制观察值

let projections comp1 comp2 =

```py
    let title = sprintf "Component %i vs %i" comp1 comp2
    let features = headers.Length
    let coords =
        normalized
        |> Seq.map projector
        |> Seq.map (fun obs -> obs.[features-comp1], obs.[features-comp2])
    Chart.Point (coords, Title = title)
    |> Chart.WithXAxis(Min = -200.0, Max = 500.0,
        MajorGrid = ChartTypes.Grid(Interval = 100.),
        LabelStyle = ChartTypes.LabelStyle(Interval = 100.),
        MajorTickMark = ChartTypes.TickMark(Enabled = false))
    |> Chart.WithYAxis(Min = -200.0, Max = 500.0,
        MajorGrid = ChartTypes.Grid(Interval = 100.), 
        LabelStyle = ChartTypes.LabelStyle(Interval = 100.),
        MajorTickMark = ChartTypes.TickMark(Enabled = false))
```

[图 5-23](#Fig23) 显示了我们之前看到的相同组件对的结果。这些图表不是最令人激动的，但它们仍然显示了一些有趣的模式。第一个图表在 X 轴上绘制了“数据库”组件，与 WPF/相对。Y 轴上的净分量。首先，大多数观测值都落在任意一个轴上，在原点附近有一个非常大的块。这表明，关心一个主题的人通常不关心另一个主题(在 SQL 上得分高的人在另一个轴上得分不高，反之亦然)，大多数人对这两个主题都不积极。此外，观察值也不是均匀分布的:有少数观察值得分很高，大量接近于零，很少介于两者之间。鉴于我们通过为每个标签收集最活跃的人来构建我们的数据集，这并不奇怪:我们应该会看到一些用户的分数远远高于平均水平。

![9781430267676_Fig05-23a.jpg](../images/00125.jpeg)

![9781430267676_Fig05-23b.jpg](../images/00126.jpeg)

[图 5-23](#_Fig23) 。根据主成分绘制的原始观察值

![Image](../images/00011.jpeg) **提示**图表显示了一些大的**异常值**，与平均值相差甚远的观察值。在数据集中，离群值是一个需要注意的潜在问题。几个大的异常值会产生扭曲的模型，从而变得不太能代表典型的平均观察值。

第二个图表很有趣，它在一个轴上描绘了 web 开发，在另一个轴上描绘了 Ruby/Rails 与 Python/Django 的对比。我们仍然有一大群接近原点的人，他们对这些话题都不感兴趣，但是，当我们向左移动时，人群分成了两组。我在这里的解释是，Ruby 和 Python web 应用程序开发人员有一些共同感兴趣的技术(JSON、javascript)，但在语言和框架上有不可减少的差异。

同样，如果我们真的想对这个数据集有一个坚实的了解，我们应该进一步调查，但是如果你愿意，我们会让你去做。我们在这里的主要目标是说明什么是主成分分析，以及它可以有所帮助。在我们的情况下，我们设法从数据集中提取了一些有趣的事实，而没有指定我们在寻找什么。PCA 发现了一个与我们称之为“数据库”的主题一致的话题，它认识到尽管 Rubyists 和 Pythonistas 在使用什么框架上有分歧，但他们中的许多人都致力于 web 应用程序开发。

主成分分析和聚类分析有一些相似之处，因为它们都可以在没有监督的情况下创建有效的数据汇总。然而，他们处理问题的方式大相径庭。与提取具有相似行为的观察组的聚类分析不同，PCA 重组数据，创建新的特征，将原始特征重组为描述观察之间行为的广泛差异的新特征。在此过程中，它创建了一个高级的、全面的数据集“地图”，通过使用更少但信息更丰富的要素以更有效的方式描述数据。

与其在 PCA 上花更多的时间，我们将转向另一个不同的主题:提出建议。如果你想一想，使用 k-means 和 PCA，我们只是简单地获取一个数据集，并搜索用户之间的相似性和差异，查看他们的历史行为——在这两种情况下，我们都找到了模式。也许我们可以使用类似的想法来寻找数据中的模式，以便提出建议——例如，向用户建议他们可能感兴趣的标签。

提出建议

如果你后退一步，考虑我们用聚类和主成分分析所做的事情，在这两种情况下，我们观察了许多人随着时间的推移的行为，并寻找他们之间的相似之处。而且，在这两种情况下，它都成功了。我们都有自己的个性，但最终，很少有人是完全独一无二的。找一个足够大的群体，你会找到和你相当相似的人。

这就是**协同过滤**背后的基本思路。简而言之，这个想法是这样的:如果你和我对一组项目表达了相同的偏好，那么我们很有可能对其他项目也有相似的口味。例如，如果我们碰巧喜欢和不喜欢同样的十部电影，并且我知道你碰巧喜欢另一部我还没有看过的电影，那么我也会喜欢那部电影似乎是合理的，并且你推荐我观看那部电影也是合理的。

当然，也有可能这只是侥幸。也许我们碰巧同意的十部电影有一些共同的特点，比如说，动作电影，而另一部电影非常不同。你可能喜欢浪漫，但我不喜欢。然而，如果我们不只是比较你和我的口味配置文件，而是开始查看许多用户，并且有一个普遍的共识，即喜欢这些电影的人也喜欢另一部电影，我们可以更有信心地认为这是一个好的推荐。相反，如果这是一种侥幸，那么就不会出现任何协议:一些喜欢动作片的人会喜欢浪漫，一些人不会，不会出现任何模式。

请注意，我们也可以从不同的方向考虑这个问题。而不是从用户的角度来看(这两个人的口味相似吗？)，我们可以从电影的角度来看，并比较电影简介。例如，如果给电影 A 评分的人倾向于以同样的方式给电影 B 评分，那么根据你对电影 A 的看法，猜测你对电影 B 的看法是合理的，并根据你预测的兴趣程度提出建议。

一种原始标签推荐器

让我们将这个想法应用到我们的 StackOverflow 数据集。这个例子有点做作，主要是作为一个概述如何解决这个问题的草图。我们在这里要做的是从我们的数据集中抽取 100 个“测试”用户，并根据前 20 个标签，尝试预测他们可能对后 10 个标签中的哪一个感兴趣。换句话说，我们假设这些都是比较新的用户，我们唯一知道的就是他们目前为止都活跃在哪些标签里。基于他们的历史，我们能建议标签吗？假设我们也知道他们实际上对剩下的 10 个标签做了什么，我们将能够通过将它们与真实行为进行比较来验证我们的猜测。

让我们尝试基于用户简档的推荐。我们这里的方法很简单:取一个用户，对于我们知道的 20 个标签，将他(或她)的个人资料与已知的用户进行比较，确定他们有多相似。然后，通过计算已知用户对这些标签的平均兴趣水平，并根据它们与目标用户的相似程度进行加权，来预测其余 10 个标签的兴趣水平。

如果我们要使用相似性计算加权平均值，我们需要两样东西。首先，我们需要一个随着个人资料相似性而增加的衡量标准:如果我找到了一个与你非常相似的人，在试图预测你可能喜欢或不喜欢什么时，我会比其他人更多地考虑那个人的偏好。然后，我们需要一个始终为正的相似性值，以便我们可以计算加权平均值。

我们可以用距离来计算相似度吗？经典的欧几里德距离符合一个标准——它总是正的——但不符合另一个标准:两个轮廓越接近，距离就越小。不过，这并不难解决。如果我们取距离的倒数，而不是距离，我们现在就有了一个随着距离减小而增加的量。然而，有一个小问题:如果两个轮廓是相同的，那么距离将为零，而倒数为无穷大。让我们稍微调整一下，通过以下方式测量相似性:

```py
Similarity(X,Y) = 1.0 / (1.0 + Distance (X,Y))
```

现在相同的项目将具有 1.0 的相似度(或者 100%，如果你喜欢的话)，并且用户相距越远，相似度越小，限制为 0.0。如果你认为这听起来更像是猜测而不是科学，那么你是对的，至少在某种程度上是对的。就像使用什么距离或如何缩放要素没有明显的唯一选择一样，定义正确的相似性取决于数据。最终,“正确的”将是效果最好的，这是通过交叉验证确定的。

实现推荐器

我们将在一个新的脚本文件中探索这一点，以免混淆我们现有的两个脚本。我们将以与其他两种方法相同的方式开始，在内存中打开 userprofiles-toptags.txt 文件，提取观察结果和标题，并过滤掉只包含零值的观察结果。

为了使用户配置文件具有可比性，我们需要解决我们在集群中遇到的相同问题；也就是说，用户之间的活动水平差异很大。让我们从重新调整观察值开始，使最大的标记值为 100%，最小的为 0%，并提取出 100 个观察值——我们将用于交叉验证的测试对象。

***清单 5-20*** 。准备数据集

```py
let scale (row:float[]) =
    let min = row |> Array.min
    let max = row |> Array.max
    if min = max
    then row
    else
        row |> Array.map (fun x -> (x - min) / (max - min))

let test  = observations.[..99]  |> Array.map scale
let train = observations.[100..] |> Array.map scale
```

在进入预测部分之前，我们需要两个助手:一个相似性函数，它将使用距离。为了方便起见，我们还将编写一个权重函数，该函数将获取一个浮点数组并对其进行重新缩放，以便它们的总和达到 100%并可用于加权平均，以及一个 split，该函数将把 20 个第一标签与 10 个“未知”标签分开。

***清单 5-21*** 。相似性和效用函数

```py
let distance (row1:float[]) (row2:float[]) =
    (row1,row2)
    ||> Array.map2 (fun x y -> pown (x - y) 2)
    |> Array.sum

let similarity (row1:float[]) (row2:float[]) =
    1\. / (1\. + distance row1 row2)

let split (row:float[]) =
    row.[..19],row.[20..]

let weights (values:float[]) =
    let total = values |> Array.sum
    values
    |> Array.map (fun x -> x / total)
```

![Image](../images/00011.jpeg) **提示**| |>操作符可以与同时对两个集合进行操作的函数一起使用，例如 Array.map2 或 Array.zip。与向函数传递两个参数不同，它们可以作为一个元组与“双管转发”操作符一起输入，这在操作管道中看起来更一致。

我们现在准备进入预测部分。给定一个用户历史，我们将只保留我们应该知道的部分(前 20 个标签)，然后我们将计算它与训练集中每个用户的相似性。完成后，我们将获取最后十列(我们正在尝试预测)，并为每一列计算与已知使用水平的相似度的和积，这将生成一个由十个值组成的数组，即十个感兴趣标签的预测兴趣水平。

***清单 5-22*** 。为用户计算预测

```py
let predict (row:float[]) =
    let known,unknown = row |> split
    let similarities =
        train
        |> Array.map (fun example ->
            let common, _ = example |> split
            similarity known common)
        |> weights
    [| for i in 20 .. 29 ->
        let column = train |> Array.map (fun x -> x.[i])
        let prediction =
            (similarities,column)
            ||> Array.map2 (fun s v -> s * v)
            |> Array.sum
        prediction |]
```

我们现在准备进行预测。让我们在第一个测试对象上试试:

```py
let targetTags = headers.[20..]
predict test.[0] |> Array.zip targetTags
>
val it : (string * float) [] =
  [|("objective-c", 0.06037062258); ("php", 0.1239587958);
    ("python", 0.1538872057); ("regex", 0.06373121502); ("ruby", 0.1001880899);
    ("ruby-on-rails", 0.09474917849); ("sql", 0.07666851406);
    ("sql-server", 0.05314781127); ("wpf", 0.02386000125);
    ("xml", 0.03285983829)|]
```

对于这 10 个标签中的每一个，我们得到了从 0%到 100%的预测兴趣水平。在这种情况下，我们的首选是 Python，占 15%，其次是 PHP (12%)和 Ruby (10%)。这个预测有什么好处吗？我们可以将它与真实发生的情况进行比较:

```py
> test.[0] |> split |> snd;;
val it : float [] =
  [|0.0; 0.0; 0.03846153846; 1.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0|]
```

如果我们要推荐一个标签，基于最大的预测兴趣水平 15.4%，我们会推荐第三个标签(Python)，这确实是用户活跃的标签。这很好。与此同时，我们可能会错过这个用户真正喜欢的 Regex，它在我们的预测中只占 6%,与得分最高的推荐相去甚远。

验证建议

我们不能只看一个例子就决定我们的推荐者是否好。让我们假设我们将按照以下方式使用我们的推荐器:计算对每个未知标签的预测兴趣，并提出具有最高预测水平的标签。衡量我们的推荐有多好的一个方法是看用户是否对推荐的标签表现出兴趣。这是一个有点弱的评价；例如，在前面的例子中，我们认为“Python”是一个好的推荐，尽管我们错过了一个更好的推荐，“Regex”同时，这也是一个不错的建议，因为我们提出了一些用户可能真正感兴趣的东西。

尽管这可能不是最好的衡量标准，但这是相当容易做到的，所以我们将继续这样做。我们将选取 100 个测试对象中的每一个，并简单地将预测与用户活跃的标签相匹配的每一个案例都算作阳性。我们只需要获取我们感兴趣的 10 个标签中每个标签的实际活动水平，预测每个标签的活动水平，获取最大的活动水平，并检查对于推荐的标签，观察到的活动水平是否确实大于 0。每次出现这种情况时，我们都会将其视为成功，并计算正确呼叫的比例。

***清单 5-23*** 。正确建议的百分比

```py
let validation =
    test
    |> Array.map (fun obs ->
        let actual = obs |> split |> snd
        let predicted = obs |> predict
        let recommended, observed =
            Array.zip predicted actual
            |> Array.maxBy fst
        if observed > 0\. then 1\. else 0.)
    |> Array.average
    |> printfn "Correct calls: %f"
```

根据这个标准，我们得到了 32%的正确推荐。让我们面对现实吧——这并不太令人印象深刻。然而，我们真的没有一个基线来比较它。我们能做出的最天真的预测是简单地预测平均评分最高的标签，并完全忽略用户之间的任何相似性。让我们对这种方法做一个简单的评估。

***清单 5-24*** 。天真的建议准确性

```py
let averages =  [|
    for i in 20 .. 29 ->
        train |> Array.averageBy(fun row -> row.[i]) |]

let baseline =
    test
    |> Array.map (fun obs ->
        let actual = obs |> split |> snd
        let predicted = averages
        let recommended, observed =
            Array.zip predicted actual
            |> Array.maxBy fst
        if observed > 0\. then 1\. else 0.)
    |> Array.average
    |> printfn "Correct calls: %f"
```

使用一个简单的建议，我们的准确率下降到 13%。虽然这并没有让我们的 32%变得伟大，但它证明了我们基于相似性的方法正在做正确的事情。

我们将保持我们的推荐引擎不变，不会试图进一步改进它。我相当肯定我们可以做得更好，但我不认为我们会在这个过程中获得更多的洞察力。相反，如果您想进一步探索这个问题，无论是在那个数据集上还是在另一个数据集上，我会用几个值得记住的注意事项来结束本文。

首先，我们的示例并不是使用这种方法的最典型案例。在原型场景中，你知道人们如何评价某些项目，但不知道他们为其他人做了什么；例如，一个人看了什么电影，他们如何评价这些电影，从糟糕到棒。这与我们的情况有点不同，表现在两个方面:在那种情况下，我们知道一个项目是否被评级，所有评级都有一致的尺度。相比之下，我们的数据集对每个用户都有非常不同的尺度，因为我们测量的是活动，我们没有明确区分空值(用户从未见过这个标签)或不感兴趣的标签(用户有机会查看那个标签，但不感兴趣)。因此，比较用户或标签的相似性变得有点复杂。

在更典型的情况下，我们会面临稍微不同的挑战。一行将具有大部分未知的值(用户尚未评级的所有项目)，并且将该用户与其他人进行比较将需要仅在共同的项目之间进行比较，这比简单地计算两个完整行之间的距离更复杂。这也指出了协同过滤方法的局限性，这在我们的例子中可能不太明显。当一个全新的用户出现在我们的系统中时，该用户没有可用的历史记录，因此我们没有可用的历史记录来产生推荐。这就是所谓的“冷启动”问题。如果您进一步使用我们的模型，您将或多或少地看到以下内容:默认情况下，推荐器将预测 Python，这是我们的十个目标标签中最受欢迎的标签。只有在我们拥有强大的、定义良好的用户特征的情况下，引擎才会产生不同的推荐。这是一个你通常会想到的结果:在信息有限的情况下，引擎会推荐广泛流行的商品，并且只开始为提供大量信息的用户提供“有趣”的选择。

关于这个主题的最后一点:为了产生一个推荐，我们必须扫描每一个用户。这显然是一个昂贵的操作，这意味着明显的缩放问题。首先，我们可以尝试通过特征来解决问题，而不是通过用户，并尝试检测 30 个标签之间的相似性，而不是 1600 个用户；我们甚至可以预先计算列之间的相似性，这将减少相当多的计算量。

那么我们学到了什么？

在这一章中，我们介绍了无监督学习。在监督学习方法(如分类或回归)中，算法旨在回答一个特定的问题，由一组训练样本来指导学习。相比之下，无监督学习是在没有用户明确指导的情况下，自动识别“简单”数据中有趣的特征或结构。在探索无监督技术的过程中，我们讨论了相当多的话题。我们的目标是获取一个数据集，看看我们是否可以应用更机械的方法从数据中系统地提取和总结信息，而不是手动探索数据，希望发现有趣的模式。

我们对数据应用了两种互补的技术:用 k-means 算法进行聚类，以及主成分分析。它们有着相似的特征:都试图通过在数据中寻找模式和简化信息，将相似的元素根据一定的距离概念组合在一起，从而将数据集简化为更精简的表示。这两种方法在寻找什么类型的模式上有所不同。K-means 聚类试图将观察值分组到簇中，这样，在理想情况下，同一簇中的项目彼此接近，而簇本身尽可能不同。K-means 确定了形心，形心本质上是“原型”，足以描述一大类观察值。

相比之下，主成分分析并不关注观察结果本身之间的相似性，而是关注哪些特征一起变化，以及观察结果如何不同于平均值。通过检查协方差矩阵，它可以识别一起移动的要素，并尝试提供替代表示，将通常以相同或相反方向发展的要素组合在一起，并以强调观测值之间最大差异因素的方式对其进行组织。

在这两种情况下，一个关键的好处是数据集的描述更简单、更紧凑。在聚类的情况下，我们可以根据一些原型来描述数据集，而不是许多独特的观察，这些原型表现出一致的行为。在主成分分析的情况下，我们得到的不是多个不相关的特征，而是几个新的特征，即主成分，它们强烈地区分了观察结果。因此，我们可以开始理解这些维度，而不是大量难以解释的事实，这比原始数据更有意义，也更容易推理。这些方法允许我们开发一个词汇来更有效地谈论数据，并且也许可以更好地理解数据如何“工作”以及我们如何从中获得洞察力。

同时，这两种技术都只是在数据中搜索特定类型的模式。不能保证这种模式真的存在，即使存在，也不能保证我们会对它们感兴趣。我们可能会发现没有实际意义的集群；例如，它们可能是不可操作的，或者陈述平凡显而易见的事实。

我们讨论的另一个方面是距离的概念和考虑规模的重要性。聚类将相近的观察值分组，在这种情况下，为算法提供有意义的距离是至关重要的。在这种情况下，我们结束了预处理我们的观察，以便将它们减少到可比较的用户简档。PCA 将与平均值有相似变化的要素分组，这就引出了关于归一化和相关性的讨论，这两个概念围绕着重新调整测量值的思想，以便它们的变化具有可比性。

最后，我们概述了如何使用类似的想法，不是从数据集中提取信息，而是产生建议。在这三个案例中，我们观察了多个个体以及他们的行为方式，并意识到尽管存在个体差异，但总体上还是存在一些模式。在最后一个例子中，我们假设在许多情况下以类似方式行事的人在其他情况下也可能会有类似的行为，我们探索了如何通过检测相似性并应用它们来提出建议，从而确定在未知情况下可能会出现什么行为。**