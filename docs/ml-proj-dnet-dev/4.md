第四章

![image](../images/00004.jpeg)

自行车和人

Fitting a Regression Model to Data with Gradient Descent

到目前为止，我们所解决的问题包括在有限的可能类别中对项目进行分类。虽然分类在许多实际情况下都有应用，但一个更常见的问题是预测一个数字。例如，考虑下面的任务:给定一辆二手车的特征(车龄、里程、引擎大小等等)，你将如何预测它的售价？这个问题不太符合分类的模式。这里我们需要的是一个至少在两个方面不同于分类模型的模型:

*   该模型预测的是实际数字，可以取各种各样的值，并且可以进行有意义的比较。
*   我们对特征和它们的预测值之间的函数关系感兴趣；也就是说，特征的变化如何影响预测。

在我们的汽车示例中，我们寻找的是一款符合以下思路的车型:

![9781430267676_Eq4-1.jpg](../images/00046.jpeg)

与我们之前看到的分类模型不同，预测的价格并不局限于一组固定的可能选择；它们可以取各种各样的值。此外，如果您要在模型中插入不同的英里值，您可以直接观察要素的变化如何影响预测值，并可能将其与年龄值的变化进行比较，以了解价格对各种要素的敏感程度。

这种类型的模型被称为 **回归模型**，它有着相对年轻的历史，在十九世纪开始有了重大的发展。它起源于物理学和根据实验观察验证物理模型的需要，这导致了现代统计学的基础。这些方法扩展到生物学、经济学和其他领域，并不断发展和适应，以处理每个学科特有的问题，是当今机器学习的重要组成部分。

在这一章中，我们的目标是在给定天气条件或星期几等信息的情况下，预测某一天使用自行车共享服务的人数。我们将从一个相当简单的模型开始，逐步建立改进，以提高准确性，同时引入重要的机器学习技术。随着我们的前进，我们将

*   解释如何找到一条与数据拟合良好的直线，即找到总体误差最低的直线；
*   介绍梯度下降，这是一种非常强大和通用的技术，可以找到一个函数的最小值，然后将其付诸行动，创建我们的第一个预测模型；
*   探索线性代数如何帮助编写更短的模型和算法，并潜在地加快计算速度；
*   提出一种简单的方法来轻松地创建和修改模型，使用函数来定义特征，这些特征选择我们想要使用哪部分数据来进行预测；和
*   进一步完善我们的预测，说明如何通过处理不同的数据类型和创建更复杂的非线性特征来使用更多的数据。

了解数据

在这一章中，我们将使用来自加州大学欧文分校机器学习知识库的另一个数据集，“自行车共享数据集”，你可以在这里找到:[https://archive.ics.uci.edu/ml/datasets/Bike](https://archive.ics.uci.edu/ml/datasets/Bike)+共享+数据集。

该数据集基于 Capital Bikeshare 项目。用它自己的话说，

首都自行车共享让 2500 多辆自行车触手可及。您可以在华盛顿特区、阿灵顿和亚历山大、弗吉尼亚州和马里兰州蒙哥马利县的 300 多个车站中选择任何一个，然后将它返回到目的地附近的任何一个车站。为您的工作、地铁、出差、购物或拜访朋友和家人之旅挑选一辆自行车。

来源:http://www.capitalbikeshare.com

完整的数据集结合了 Capital Bikeshare 公开提供的数据和其他公共数据源，如天气和公共假期。

数据集中有什么？

在进入模型构建部分之前，让我们快速看一下我们正在处理什么。数据集本身可以在[https://archive . ics . UCI . edu/ml/machine-learning-databases/00275/](https://archive.ics.uci.edu/ml/machine-learning-databases/00275/)找到；zip 文件 Bike-Sharing-Dataset.zip 包含三个文件:day.csv、hour.csv 和 readme.txt。我们将处理的文件是 day.csv，它包含按天聚合的数据；hour.csv 是相同的数据，但是以每小时为单位进行聚合。我将让您大胆猜测 readme.txt 包含什么内容！

像往常一样，我们先用脚本文件创建一个 F#库项目，然后将我们的数据文件 day.csv 添加到该项目中。如果你点击 Visual Studio 中的文件，你会看到我们数据的内容，如图 4-1 中的[所示。](#Fig1)

![9781430267676_Fig04-01.jpg](../images/00047.jpeg)

[图 4-1](#_Fig1) 。day.csv 文件的内容

该文件遵循惯例。csv 格式，第一行包含描述列名的标题，后面是 731 行观察值，每天一行。我们这里有什么？如果您仔细阅读 readme.txt 文件，您会发现每一列都有以下定义:

*   即时:记录索引*(表示从数据集的第一次观察开始经过的天数)*
*   dteday:日期
*   季节:季节(1:春天，2:夏天，3:秋天，4:冬天)
*   年:年(0: 2011，1:2012)
*   月:月(1 至 12)
*   假日:一天是否是假日(摘自[http://dchr.dc.gov/page/holiday-schedule](http://dchr.dc.gov/page/holiday-schedule))
*   工作日:一周中的某一天
*   工作日:如果一天既不是周末也不是假日，则为 1，否则为 0。
*   天气状况:
    *   1:晴朗，少云，部分多云，部分多云
    *   2:薄雾+多云，薄雾+碎云，薄雾+少云，薄雾
    *   3:小雪，小雨+雷雨+散云，小雨+散云
    *   4:暴雨+冰粒+雷雨+薄雾，雪+雾
*   temp:以摄氏度为单位的标准化温度。这些值除以 41(最大值)
*   atemp:归一化的感觉温度，单位为摄氏度。这些值除以 50(最大值)
*   哼:标准化湿度。这些值除以 100(最大值)
*   风速:归一化风速。这些值除以 67(最大值)
*   临时:临时用户数
*   已注册:注册用户数
*   计数:租赁自行车总数，包括休闲自行车和注册自行车

我们有几个不同的功能可供使用:时间/日历信息，无论是正常工作日还是“特殊”工作日，与天气相关的多个测量值，最后是那天租用自行车的人数，分为注册用户和临时用户。

我们在这里的重点将是创建一个模型来预测 cnt，即在特定的一天，使用我们当天可用的测量值(当然，不包括临时和注册，因为这不会有太多的乐趣，因为根据定义，临时+注册= cnt)来租赁自行车的人数。请注意，我们的数据并不是同质的:虽然有些特征是数字，即实际数字(例如，温度)，但其中相当一部分是分类的，即每个数字编码一个州(例如，天气或工作日)。

用 FSharp 检查数据。制图

像往常一样，让我们从处理脚本文件开始，仔细看看数据，使用 fsharp 的 CSV 类型提供程序将数据加载到内存中。数据，我们首先从 NuGet 安装:

```
#I @"packages\"
#r @"FSharp.Data.2.2.1\lib\net40\FSharp.Data.dll"

open FSharp.Data

type Data = CsvProvider<"day.csv">
let dataset = Data.Load("day.csv")
let data = dataset.Rows
```

与我们之前看到的一些问题不同，这个数据集有一组非常明显的特征。在这种情况下，可能值得简单地手工检查数据，看看我们是否能发现任何模式。让我们从绘制一段时间内自行车的使用情况开始。为此，我们将使用另一个 F#库 fsharp。图表，它提供了一个经典图表的集合。让我们用 Nuget 安装这个库，并创建我们的第一个图表，将总使用量绘制成一条线:

```
#load @"FSharp.Charting.0.90.10\FSharp.Charting.fsx"
open FSharp.Charting

let all = Chart.Line [ for obs in data -> obs.Cnt ]
```

![9781430267676_Fig04-02.jpg](../images/00048.jpeg)

 。自第一次观察以来，每天的自行车使用情况

![Image](../images/00011.jpeg) **提示** FSharp。Charting 可以作为一个常规的库使用，但也附带了一些实用程序，使得它可以在交互式脚本环境中友好地使用。特别是创建一个图表。行[...]，也将自动显示它，而不必调用。显示图表()。要使用“交互模式”，只需加载 fsharp。而不是打开 dll fsharp.Charting。

有几件事立刻凸显出来。首先，数据非常不规则——每天都有很大的波动。就此而言，也有更大规模的波动:随着时间的推移，使用率似乎经历了高低阶段。最后，总的来说，似乎还有上升的趋势。

用移动平均线发现趋势

我们能证实我们的直觉吗？有一种趋势正在发生。一种方法是使用所谓的移动平均线。为了减少数据中的日常噪音，我们可以在一定长度的窗口内对观察值进行平均，然后绘制结果曲线，以便从短期波动中分离出更深层次的趋势。F# Seq 模块包含一个方便的函数 Seq.windowed，它将一个序列转换成一个包含连续观测数据块的新序列:

```
let windowedExample =
    [ 1 .. 10 ]
    |> Seq.windowed 3
    |> Seq.toList
```

这将系列 1 分成几块..10 分成三个连续值的块:[[| 1；2;3|];[|2;3;4|];[|3;4;5|];[|4;5;6|];[|5;6;7|];[|6;7;8|];[|7;8;9|];[|8;9;10|]].由此，生成移动平均值只是计算每个块的平均值的问题:

```
let ma n (series:float seq) =
    series
    |> Seq.windowed n
    |> Seq.map (fun xs -> xs |> Seq.average)
    |> Seq.toList
```

让我们绘制覆盖了七天和三十天移动平均线的原始序列:

```
Chart.Combine [
    Chart.Line count
    Chart.Line (ma 7 count)
    Chart.Line (ma 30 count) ]
```

![9781430267676_Fig04-03.jpg](../images/00049.jpeg)

 。七日和三十日移动平均线

虽然移动平均线本身不是结论性的，但它确实暗示了一个更长期的趋势，并清楚地显示了一年中波动的某种规律性，包括季节性高峰。

根据数据拟合模型

我们下一步的目标是建立一个模型，根据数据集中可用的信息预测自行车的使用情况。在深入研究之前，让我们量化一下，并为我们的任务建立一个基线。我们如何定义什么是好的或坏的结果？我们能创造的最天真的模型是总是预测同一个数字，平均值。在这种情况下，我们的典型错误是什么？

```
let baseline =
    let avg = data |> Seq.averageBy (fun x -> float x.Cnt)
    data |> Seq.averageBy (fun x -> abs (float x.Cnt - avg))
```

使用这种简单的方法，平均来说，我们的预测会与正确的值相差 1581.79——这是需要打破的数字。让我们看看我们是否能做得更好。

定义基本直线模型

最简单可行的模型是什么？随着时间的推移，平均使用量似乎在增加，所以我们可以通过整合趋势来尝试改进我们的天真模型，如下所示:

![9781430267676_Eq4-2.jpg](../images/00050.jpeg)

换句话说，我们用一条直线来表示使用情况，从初始值开始，随着时间线性增加。这可能不是我们所能做到的最好的——例如，这将无法解释我们发现的季节效应——但它应该有望比我们天真的“预测平均值”模型表现得更好。

t 的任何时间尺度都可以，但碰巧的是，数据集观测值都包含一个字段 instant，即从第一次观测开始经过的天数，它被用作历元。以稍微正式一点的方式，我们可以用以下方式指定预测模型:

![9781430267676_Eq4-3.jpg](../images/00051.jpeg)

换句话说，我们可以定义一整类预测模型，称为**线性回归模型**，它们都遵循相同的一般模式。我们试图使用一个或多个输入变量(在本例中是 obs)来预测一个值。即时)。每个输入变量都有自己的“通用”乘数θ，由指数确定，即**回归系数**。预测是通过将每个变量乘以其系数并相加计算出来的，形成所谓的**线性组合**。

稍后，我们将研究使用多个输入变量。目前，我们将坚持我们的单个变量 obs.Instant。如果我们设置θ0 和θ1 的值(或者，简而言之，θ，以表示值θ0 和θ1 的组合)，我们将获得一个特定的预测模型。这直接翻译成 F#:

```
type Obs = Data.Row

let model (theta0, theta1) (obs:Obs) =
    theta0 + theta1 * (float obs.Instant)
```

让我们举例说明两种任意情况，一种是模型 0，其中θ0 被设置为系列的平均值，另一种是模型 1，其中我们任意将θ0 设置为 6000.0，θ1 设置为-4.5。我们现在可以根据实际数据绘制这两个模型的预测:

```
let model0 = model (4504., 0.)
let model1 = model (6000., -4.5)

Chart.Combine [
    Chart.Line count
    Chart.Line [ for obs in data -> model0 obs ]
    Chart.Line [ for obs in data -> model1 obs ] ]
```

![9781430267676_Fig04-04.jpg](../images/00052.jpeg)

[图 4-4](#_Fig4) 。可视化两个演示线性回归模型

显然，这两个模型都不太适合这些数据。从好的方面来看，我们有一个通用的结构来定义各种各样的预测模型。然而，我们面临一个新问题:我们如何为θ选择“好”的值？

寻找成本最低的模型

这里的第一个问题是更明确地定义我们的目标是什么。非正式地说，我们寻找的是最符合数据的那条线。换句话说，我们想要的是一条预测值曲线，当与实际观察值比较时，它能给我们提供尽可能小的误差，或者，换句话说，是一条与实际数据距离最小的曲线。我们在[第 1 章](1.html#7K4G0-841455c729754b8aac560d608a86cf91)中已经看到了类似的方法！我们可以简单地修改距离函数；我们将计算预测曲线和实际曲线之间的欧几里德距离，比较每个数据点的真实值和预测值，而不是逐个像素地计算两幅图像之间的距离。

你可能会问为什么我选择欧几里德距离，而不是别的。毕竟，我们在第一章中从曼哈顿距离开始，因为它很容易计算。做出这一选择有几个原因。第一个是由建模考虑驱动的。欧几里德距离通过求差的平方来惩罚误差；因此，较大的错误将比较小的错误受到更重的惩罚，这是一件好事。当试图最小化距离时，我们将主要关注避免大错误，而不是小错误。第二个原因是技术性的:它使数学变得相当简单。

作为一个例子，让我们计算我们的两个模型的成本:

```
type Model = Obs -> float

let cost (data:Obs seq) (m:Model) =
    data
    |> Seq.sumBy (fun x -> pown (float x.Cnt - m x) 2)
    |> sqrt

let overallCost = cost data
overallCost model0 |> printfn "Cost model0: %.0f"
overallCost model1 |> printfn "Cost model1: %.0f"
```

首先，我们创建一个类型模型，它捕获我们所有模型所做的事情——也就是说，将一个观察 Obs 转换成一个 float，Cnt 的预测值，即用户数。计算特定数据集的成本非常简单，只需将每个 Obs 和相应预测值之间的平方差相加，然后求平方根。然后，我们可以使用 partial application 创建一个 overallCost 函数，该函数将计算整个数据集的成本，然后在我们的两个模型上进行测试，结果如下:

```
>
Cost model0: 52341
Cost model1: 71453
```

在这种情况下，model0 的成本比 model1 低得多，这表明它更好地拟合了数据，并且与[图 4-4](#Fig4) 一致。在这一点上，我们可以重申我们的问题:我们试图实现的是找到θ的值，使得成本函数最小化。太好了！既然我们知道自己想做什么，那就去做吧！

用梯度下降法求函数的最小值

我们将使用的方法叫做**梯度下降**。想法是从θ值开始，观察预测误差，并逐渐调整θ以逐渐减小误差。为了做到这一点，我们需要的是一种方法，利用我们当前的模型，将θ0 和θ1 移向减小误差的方向。梯度下降

在进入我们的实际问题之前，我们将从一个小的微积分复习开始。我们的目标是解释一点梯度下降的机制。如果你对微积分不太适应(或不感兴趣),不要担心——你仍然可以应用梯度下降。然而，理解它是如何工作的是很有趣的。这是一个相当通用的技术，适用于您想要找到函数的最大值的情况。特别是，它广泛用于机器学习中，以确定符合数据集的最佳参数，这就是我们在这里要做的。

让我们从一个简单的例子开始:

![9781430267676_Eq4-4.jpg](../images/00053.jpeg)

函数 f 描述了一条直线，它的导数是:

![9781430267676_Eq4-5.jpg](../images/00054.jpeg)

这个数字代表什么？导数对应于 f 在 x 值附近的斜率，或者换句话说，如果我们将 x 值从当前值“稍微”增加，y 值应该增加多少。这个特殊的例子并不十分有趣:不管我们在直线上的什么位置，斜率都是恒定的。这也暗示了 f 没有最小值:对于任何值 x，我们可以通过减少 x 来减少 f(x)，然而，让我们考虑一个更有趣的例子:

![9781430267676_Eq4-6.jpg](../images/00055.jpeg)

函数 g 是二次多项式，其导数为:

![9781430267676_Eq4-7.jpg](../images/00056.jpeg)

这就有趣多了。在这种情况下，斜率随着 x 的每个值而改变。此外，它改变符号:对于 x < 2, positive for x > 2 它是负的，并且在 x = 2 时为零。这意味着，对于 x > 2，斜率是向上的:如果我们减少 x“一点”，我们将沿着斜率下降，g(x)应该减少。相反，对于低于 2 的 x 值，导数是负的，x 的小幅度增加将沿着斜率向下并减少 g。

作为一个例子，图 4-5 显示了 g 及其在 x = 3 时的导数。g'(3) = 4:在 x = 3 附近，x 增加一点将导致 y 增加四倍。

![9781430267676_Fig04-05.jpg](../images/00057.jpeg)

[图 4-5](#_Fig5) 。用导数求曲线的斜率

x = 2 怎么样？对于该值，斜率为 0，这意味着我们到达了一个平坦的区域，可以说:没有明显的地方可去。导数为 0 的点称为**极值**，可以是两种情况之一:最小值或最大值。

那么，这为什么有趣呢？因为我们可以用这个来设计一个通用的策略，来寻找一个函数的最小值。从任意值 x 开始，我们可以测量导数(或梯度),并向下降方向迈出一小步:

![9781430267676_Eq4-8.jpg](../images/00058.jpeg)

值α被称为**学习速率**。斜率只有在接近 x 时才是正确的，α允许我们在更新 x 时调整步长，还要注意变化如何取决于 g’。当斜率很陡时，我们会对 x 做更大的改变；当 g '很小时，我们大概接近最小值，我们将做较小的调整。使用这种方法，如果一切都按计划进行，我们可以生成一系列值 x0、x1 等等，每个值都会生成 g 的一个较低值，这就是梯度下降算法。

使用梯度下降来拟合曲线

让我们把这个想法应用于手头的问题。我们将在这里稍微扭曲一下我们的问题:我们将考虑误差或成本函数，这是我们试图减少的值，而不是关注我们的模型预测的值。在另一种情况下，我们不再将 x，即我们的观测值，作为输入参数，我们现在将转换表格，将θ作为我们的参数。在该框架中，使用参数θ测量模型成本的成本函数是:

![9781430267676_Eq4-9.jpg](../images/00059.jpeg)

换句话说，如果我们将参数θ0 和θ1 用于我们的模型，这个函数测量我们将从观察 obs 得到的误差。如果我们将θ1 增加一点，误差会改变多少？这是成本对θ1 的导数，我们来计算一下。首先，让我们把事情弄清楚，把成本重写为θ1 的函数，稍微扩展一下，把各项重新组织成 a*θ1<sup class="calibre16">2</sup>+b *θ1+c:

![9781430267676_Eq4-10.jpg](../images/00060.jpeg)

这是一个简单的多项式，记住 a * x <sup class="calibre16">2</sup> 的导数是 2 * a * x，b * x 的导数是 b，我们得到成本的导数，即:

![9781430267676_Eq4-11.jpg](../images/00061.jpeg)

太好了！这是一个很大的工作量，所以让我们看看这是否可行。在导数之后将θ1 移动“一点”应该会减少预测误差。让我们将“一点点”作为参数α，现在我们有一个更新过程来迭代地修改θ1，减少每一步的观察误差:

![9781430267676_Eq4-12.jpg](../images/00062.jpeg)

请注意，这个公式有一个相对简单的解释，帮助我们澄清算法做什么。等式的最后一项正是误差项。假设 obs.inst 为正，如果预测值低于 obs.cnt(我们试图预测的实际值),则模型欠调，算法会将θ1 的值增加一点，这将使我们朝着正确的方向前进。相反，如果相反，则θ1 的值将减小。还要注意的是，obs.inst 的值越大，校正就越重要。

最后，还要注意，虽然这种方法会将θ1 移向正确的方向，但根据α的值，我们可能会过度补偿。我们很快就会看到，alpha 是一个需要手动调整的参数。如果α足够小，算法将很好地朝着θ1 的最佳值前进。然而，较小的值也意味着较慢的校正，这可能意味着要等待很长时间以使该过程产生良好的θ1 值。

更一般的模型公式

在将这种方法应用于手头的问题之前，当数学还在我们脑海中记忆犹新的时候，让我们做两个评论，这将允许我们使我们的算法更通用。想象一下，我们有 N 个特征 X1，X2，...XN，并试图拟合一个预测模型的形式

![9781430267676_Eq4-13.jpg](../images/00063.jpeg)

这与我们正在处理的型号完全相同，只是有更多的特点。我们简单的直线模型恰好有 N = 1。第一个方便的技巧是在我们的模型中引入一个“虚拟”特征 X0，并为每一次观察设置 X0 = 1。这很方便的原因是，它现在允许我们以一致的方式为所有θ写预测模型，不需要对常数项θ0 进行特殊处理:

![9781430267676_Eq4-14.jpg](../images/00064.jpeg)

结果，你现在可以重写成本函数，对于θ的任何值，它看起来像这样:

![9781430267676_Eq4-15.jpg](../images/00065.jpeg)

这里的美妙之处在于，如果你现在试图找出如何更新 tak，对于 0 和 N 之间的任何 ks，在执行推导并重复我们上面所做的步骤后，你将得到下面的公式:

![9781430267676_Eq4-16.jpg](../images/00066.jpeg)

这太棒了。使用这种方法，我们现在有了一种算法，可以用来同时更新任意数量特征的所有参数θ(包括适用于常数项的θ0)。

让我们来说明如何在我们的模型中使用这个想法——一个真实的例子可能有助于使它不那么抽象。例如，我们可以尝试使用即时和温度来预测用户数量，这将扮演 X1 和 X2 的角色。在这种情况下，回归模型将如下所示:

![9781430267676_Eq4-17.jpg](../images/00067.jpeg)

如果我们试图更新值θ2 以减少预测误差(成本)，应用我们的过程将产生以下更新规则:

![9781430267676_Eq4-18.jpg](../images/00068.jpeg)

此外，相同的规则现在可以应用于更新θ0 和θ1；唯一需要的修改是替换公式中正确的 Thetak 和 Xk。

实施梯度下降

现在我们有了一个具有梯度下降的大锤，让我们回到我们的自行车问题，并使用我们的算法来最终拟合通过我们的数据集的直线。我们将从直接应用我们的更新方法开始，然后分析结果，看看我们如何改进该方法。

随机梯度下降

我们可以采取的第一种方法是简单地迭代观测值，并在每一步执行一个小的调整。让我们试试那个。首先，我们需要一个函数，在给定一个观察值和一个θ值的情况下，以学习速率α更新θ:

```
let update alpha (theta0, theta1) (obs:Obs) =
    let y = float obs.Cnt
    let x = float obs.Instant
    let theta0' = theta0 - 2\. * alpha * 1\. * (theta0 + theta1 * x - y)
    let theta1' = theta1 - 2\. * alpha * x *  (theta0 + theta1 * x - y)
    theta0', theta1'
```

这是我们之前推导的公式的直接实现。我们计算 Theta 的更新值，并将其作为元组返回。让我们用一个例子来证实这种方法是可行的，例如，使用第一百次观察，并从θ的初始值(0.0，0.0)开始:

```
let obs100 = data |> Seq.nth 100
let testUpdate = update 0.00001 (0.,0.) obs100
cost [obs100] (model (0.,0.))
cost [obs100] (model testUpdate)
```

看起来一切都在工作:在我们应用更新后，针对特定观察的新预测得到了改进，误差变小了。此时，我们需要做的就是将其应用于整个数据集，从上一步获得的θ值开始，逐步更新每次观察的θ值:

```
let stochastic rate (theta0,theta1) =
    data
    |> Seq.fold (fun (t0,t1) obs ->
        printfn "%.4f,%.4f" t0 t1
        update rate (t0,t1) obs) (theta0,theta1)
```

我们这里用的是 Seq.fold，相当于 C#的可枚举。LINQ 的集合方法。fold 只接受一个初始值和一个值序列，并通过对序列的每个元素应用相同的函数来更新累加器。一个简单的例子可能有助于更好地说明这个想法:

```
let data = [0;1;2;3;4]
let sum = data |> Seq.fold (fun total x -> total + x) 0
```

fold 跟踪一个总值，该值最初为 0，然后将每个值 x 加到总数上，直到序列中没有任何值，这时将返回总值。

在我们的例子中，我们遵循相同的模式:我们用两个值——θ0 和θ1——初始化累加器，并为数据集中的每个观察值更新它们，直到没有观察值，然后返回θ的最终值。

我们快完成了。唯一剩下的问题是校准学习率α，取一个足够小的值以避免过度调整，又足够大以至于我们的算法不会进展太慢。这很容易做到，因为我们可以测试不同的 Alpha 值(0.1，0.01，0.001...)并在数据集上更新一遍后比较结果的质量，从θ=(0.0，0.0)开始:

```
let tune_rate =
    [ for r in 1 .. 20 ->
        (pown 0.1 r), stochastic (pown 0.1 r) (0.,0.) |> model |> overallCost ]
```

如果你打印出学习率α的每一个值的成本，你会得到:

```
Alpha                  Cost
0.1                  : NaN
0.01                 : NaN
0.001                : NaN
0.000,1              : NaN
0.000,01             : Infinity
0.000,001            : 94,007
0.000,000,1          : 93,189
0.000,000,01         : 59,367
0.000,000,001        : 107,780
0.000,000,000,1      : 129,677
0.000,000,000,01     : 132,263
0.000,000,000,001    : 132,526
0.000,000,000,000,1  : 132,552
0.000,000,000,000,01 : 132,555
0.000,000,000,000,001: 132,555
```

结果相当典型。对于较大的α值，调整过于激进，以至于θ值变化很大，永远不会稳定下来。相反，对于较小的值，调整是如此之小，以至于它们几乎不能改善拟合。α的最有效值看起来是 1e-8，或 0.000，000，01，对于约(0.03，8.21)的θ值，给我们的成本约为 59，367；如果我们画出结果线，我们会得到:

```
let rate = pown 0.1 8
let model2 = model (stochastic rate (0.0,0.0))

Chart.Combine [
    Chart.Line count
    Chart.Line [ for obs in data -> model2 obs ] ]
```

![9781430267676_Fig04-06.jpg](../images/00069.jpeg)

 。使用随机梯度下降的最佳拟合曲线

![Image](../images/00011.jpeg) **注意**我们发现了α的合理有效值，但这里还有另一个微妙的问题。我们曲线的斜率看起来很好，但是它的初始值θ0 看起来太低了。问题是我们的变量在非常不同的尺度上:常数是 1，但是瞬间的范围是从 0 到大约 700。结果，调整主要由具有较大幅度θ1 的特征驱动。为了更快地调整θ0，我们需要准备数据集，以便所有输入都处于可比较的范围内，这一过程称为重新调整。我们暂时忽略这个问题，但是会在第五章中更详细地讨论特性缩放。

分析模型改进

现在怎么办？下一步是不断重复这个过程，对数据集进行多次遍历，直到我们观察到没有显著的改进。然而，与其这样做，不如让我们看一下算法的行为，特别是,预测误差如何随着我们的前进而演变。代码的细节并不太重要——我们在这里所做的是取一个非常激进的α值(比我们之前确定的“最佳点”值高 10 倍),并在每次调整后绘制总误差:

```
let hiRate = 10.0 * rate
let error_eval =
    data
    |> Seq.scan (fun (t0,t1) obs -> update hiRate (t0,t1) obs) (0.0,0.0)
    |> Seq.map (model >> overallCost)
    |> Chart.Line
```

这很耐人寻味。缓慢开始后，误差开始稳定下降，中间速度减慢，最后开始回升。这是怎么回事？用我们积极的学习率α，我们得到了图 4-7 中的图表。

![9781430267676_Fig04-07.jpg](../images/00070.jpeg)

[图 4-7](#_Fig7) 。使用随机梯度下降的逐步误差

实际情况是，因为我们的数据集中有季节效应在起作用(可以推测，在隆冬骑自行车去上班远不如春天有吸引力)，多次连续观察显示了相同的趋势，要么高于，要么低于理想线。结果，一次只查看一个观测值的更新过程进行一系列都向同一方向倾斜的校正(因为误差都向同一方向倾斜),并开始偏离，暂时增加了整个模型的误差。

这有几个含义。首先，除了梯度下降，如果我们使用交叉验证来评估我们的工作质量，我们需要非常小心。比方说，如果我们将最后 200 个观察值设置为验证样本，我们将得到一个非常有偏的预测误差估计值，因为在特定的时间跨度内，曲线遵循的下降趋势(大概)只是暂时的，当然不能代表数据的整体形状。

这表明在我们的随机下降方法中有一个潜在的限制。以一种非常非正式的方式，我们试图实现的是一个一致的估计，但是我们的过程本质上是一个接一个地探测每一个观察，并遵循最新观察所指示的方向。这可能会导致一些不必要的波动，例如，当我们遇到一个非常不典型的观察，或一系列的观察。公平地说，这可以在一定程度上得到缓解——通过以随机顺序而不是顺序挑选观察值(这将限制连续观察值出现类似偏差的风险)，以及通过逐步降低比率α(这将降低给予任何单个观察值的权重并减慢更新过程)——但即使如此，事实仍然是这种方法可能会波动。

解决这个问题的一种方法是避免依赖于单独的观察，而是基于整个数据集执行更新。这是批量梯度下降的基础，我们将在接下来的操作中看到。与此同时，随机梯度下降不应被完全抛弃。它相当容易实现，并且适用于广泛的情况。更有趣的是，它允许所谓的“在线学习”该算法不需要预先学习整个数据集；如果您要随着时间的推移获得新的数据，您的模型可以简单地更新，只使用您的当前模型和最新到达的观察。

批量梯度下降

那么，我们如何着手调整我们的模型，以基于整个数据集而不是基于单个观察来执行更新呢？事实证明，这样做相当简单。在随机下降法中，我们的目标是最小化单次观察的误差，由成本函数定义:

![9781430267676_Eq4-19.jpg](../images/00071.jpeg)

与其进行单次观察，我们还不如关注整个数据集的平均误差，也就是所有单个误差的平均值。最终的成本函数如下所示，每个观察值的索引从 1 到 N:

![9781430267676_Eq4-20.jpg](../images/00072.jpeg)

按照与之前相同的想法，我们需要计算成本相对于其自变量θ0 和θ1 的导数。这其实很简单，如果你记得 f(x) + g(x)的导数是它们各自导数的和；总误差的导数是每个单独观测值的导数之和。θ1 的更新简单地变成如下:

![9781430267676_Eq4-21.jpg](../images/00073.jpeg)

解释此函数的一种方式是，算法现在将计算对每个观测误差执行的调整，然后进行平均校正，而不是基于单个观测误差进行更新。这可以很容易地扩展到所有的θ，就像我们对随机下降所做的那样。因为我们喜欢保存树，所以这次我们不会遍历每个步骤，并调用经典的“我们将把它作为读者解决细节的练习”，直接跳到更新算法的一个可能的实现:

```
let batchUpdate rate (theta0, theta1) (data:Obs seq) =
    let updates =
        data
        |> Seq.map (update rate (theta0, theta1))
    let theta0' = updates |> Seq.averageBy fst
    let theta1' = updates |> Seq.averageBy snd
    theta0', theta1'
```

我们在这里所做的是获取每个观察值，计算我们仅基于该观察值对θ所做的调整，然后取所有调整值的平均值。然后，我们可以重复更新过程，逐步改进我们对θ的估计，在一定次数的迭代之后停止，或者可能在搜索已经稳定并且θ的值从一次迭代到下一次迭代变化不大时停止:

```
let batch rate iters =
    let rec search (t0,t1) i =
         if i = 0 then (t0,t1)
        else
            search (batchUpdate rate (t0,t1) data) (i-1)
    search (0.0,0.0) iters
```

我们只需找到一个合适的学习率α值，遵循与之前相同的步骤。对于α= 0.000，001 的值，如果我们将误差绘制为迭代的函数，我们会得到一条漂亮而平滑的曲线，它显示了误差的规则下降，没有任何起伏。请参见以下内容:

```
let batched_error rate =
    Seq.unfold (fun (t0,t1) ->
         let (t0',t1') = batchUpdate rate (t0,t1) data
         let err = model (t0,t1) |> overallCost
         Some(err, (t0',t1'))) (0.0,0.0)
    |> Seq.take 100
    |> Seq.toList
    |> Chart.Line

batched_error 0.000001
```

![9781430267676_Fig04-08.jpg](../images/00074.jpeg)

 。使用批量梯度下降的逐步误差

这样好多了。一方面，我们现在一次使用整个数据集，这在我们的情况下不是问题，但如果它大得多，可能会有问题。另一方面，我们的过程现在稳定地向最优值收敛，一路上没有任何颠簸:等待的时间越长，估计就越好。这里好的一点是，同样的方法也适用于数据拟合直线之外的情况。本质上，只要你的成本函数是可微分的，并且是凸的，就可以采取同样的一般方法。

然而，我们的方法仍然有一些小缺点。首先，有不得不手动调整学习率 alpha 的烦恼——如果不需要的话就好了。然后，如果你运行该算法进行大量迭代，你会注意到成本不断降低，参数θ0 不断增加，直到很长时间后才稳定下来。这是由于我们之前提到的问题:θ0 和θ1 在非常不同的尺度上，并且考虑到缓慢的学习速率，算法需要相当大量的迭代才能达到“正确”的值。在下一节中，我们将看到，对于手头的具体问题(没有任何正则化惩罚项的线性回归)，我们可以使用一点线性代数来简化我们的生活，并使我们的算法快得多。

拯救线性代数

到目前为止，我们遵循的方向是坚持我们的领域，将数据集表示为具有命名属性的观察值的集合，使用特定于当前领域的地图和折叠来应用变换。然而，如果你考虑一下，一旦我们开始应用我们的更新过程，领域的细节就不再重要了。如果我们试图根据汽车的引擎大小和行驶里程来预测汽车的价格，而不是根据风速来预测自行车的使用量，这不会有什么不同。最后，在这两种情况下，我们最终都将把观察值减少到成排的双精度值，并使用它们来尝试预测另一个双精度值。

在这种情况下，与其关注领域的细节(这在当时并不重要),不如关注数据的结构，将领域转换为通用结构，并应用相同的通用算法来解决共享相同结构的问题。这就是代数派上用场的地方:它为我们提供了一种完善的语言来描述数字行，以及对它们执行各种操作的方法。

线性代数复习者

线性代数的两个基本元素是向量和矩阵。大小为 n 的向量只是 n 个元素的集合，索引从 1 到 n，比如:

![9781430267676_Eq4-22.jpg](../images/00075.jpeg)

矩阵由它的维度(或大小)m x n 来描述，其中 m 指它的行，n 指它的列。例如，M 是一个 3 × 2 矩阵:

![9781430267676_Eq4-23.jpg](../images/00076.jpeg)

作为一个松散的类比，我们可以将向量和矩阵描述为数组和 2D 数组的数学等价物。

向量可以表示为行向量或列向量；在处理矩阵时，这种区别非常重要。矩阵可以看作是行向量或列向量的集合，对向量的大多数操作都可以看作是矩阵操作的特例。

我们需要的四个核心运算是加法、标量乘法、转置和乘法。添加矩阵(或向量)只需要通过索引添加元素，并且它要求两个矩阵大小相同:

![9781430267676_Eq4-24.jpg](../images/00077.jpeg)

标量乘法描述的是将一个矩阵(或向量)乘以一个单一的数，即标量；每个元素乘以标量:

![9781430267676_Eq4-25.jpg](../images/00078.jpeg)

用 T 表示的矩阵转置包括重新定向矩阵，使行变成列，反之亦然:

![9781430267676_Eq4-26.jpg](../images/00079.jpeg)

最后，矩阵乘法计算为第一个矩阵的每个行向量与第二个矩阵的每个列向量的点积(或内积)。两个向量的点积是一个单一的数字，计算方法是每个向量的乘积之和:

![9781430267676_Eq4-27.jpg](../images/00080.jpeg)

当应用于乘法矩阵时，我们得到的是:

![9781430267676_Eq4-28.jpg](../images/00081.jpeg)

请注意，为了正确定义，该方法要求第一个矩阵的列数与第二个矩阵的行数相同。

亲爱的，我把配方缩小了！

让我们看看如何用代数来重新表述我们的问题。之前，我们这样定义了我们的预测模型:

![9781430267676_Eq4-29.jpg](../images/00082.jpeg)

如果我们定义 Y 为 obs.cnt，θ和 X 为两个向量，用

![9781430267676_Eq4-30.jpg](../images/00083.jpeg)

![9781430267676_Eq4-31.jpg](../images/00084.jpeg)

...然后，我们的预测模型可以以更紧凑的形式重新表述为两个向量的乘积:

![9781430267676_Eq4-32.jpg](../images/00085.jpeg)

类似地，我们试图在批量梯度下降算法中最小化的成本函数是每个观察值的误差平方和，在 N 个观察值上取平均值:

![9781430267676_Eq4-33.jpg](../images/00086.jpeg)

同样，通过用 Yn 表示第 n 次观察的输出值(即 Y 向量的第 n 个元素)，用 Xn 表示描述第 n 次观察的数据行，可以更简洁地重申这一点:

![9781430267676_Eq4-34.jpg](../images/00087.jpeg)

或者，更好的是，如果我们将所有的观察值堆叠在一起，我们得到一个矩阵 X，其中矩阵的第 n 行 Xn 包含第 n 个观察值:

![9781430267676_Eq4-35.jpg](../images/00088.jpeg)

我们可以继续这样做，将整个批量梯度下降算法重写为一系列向量和矩阵运算。在进入那个之前，让我们首先确定这实际上是可行的，并且当我们使用线性代数时，我们的结果确实与原始公式相同。

Math.NET 线性代数

虽然存在其他选择，但是“做代数”的最佳起点是。网是 Math.NET。Math.NET 是一个维护得很好的开源库，在它存在的这些年里已经经受了相当多的考验。让我们开始一个新的脚本，并通过 NuGet 导入 Math.NET 和它的 F#扩展，以及我们目前使用的其他包，而不是在我们目前使用的同一个脚本中塞进更多的代码。首先，我们来看看如何用数学来执行基础代数。NET:

```
#I @"packages\"
#r @"FSharp.Data.2.2.1\lib\net40\FSharp.Data.dll"
#load @"FSharp.Charting.0.90.10\FSharp.Charting.fsx"
#r @"MathNet.Numerics.Signed.3.6.0\lib\net40\MathNet.Numerics.dll"
#r @"MathNet.Numerics.FSharp.Signed.3.6.0\lib\net40\MathNet.Numerics.FSharp.dll"

open FSharp.Charting
open FSharp.Data
open MathNet
open MathNet.Numerics.LinearAlgebra
open MathNet.Numerics.LinearAlgebra.Double
```

为了便于说明，让我们创建一个向量 A 和一个矩阵 B，并对它们执行一些操作:

```
let A = vector [ 1.; 2.; 3\. ]
let B = matrix [ [ 1.; 2\. ]
                 [ 3.; 4\. ]
                 [ 5.; 6\. ] ]

let C = A * A
let D = A * B
let E = A * B.Column(1)
```

一切看起来都像我们预期的那样工作，并且，作为一个很好的奖励，代码看起来非常类似于你打开数学书时看到的数学公式。现在，让我们重写我们的预测和成本函数，“代数风格”:

```
type Data = CsvProvider<"day.csv">
let dataset = Data.Load("day.csv")
let data = dataset.Rows

type Vec = Vector<float>
type Mat = Matrix<float>

let cost (theta:Vec) (Y:Vec) (X:Mat) =
    let ps = Y - (theta * X.Transpose())
    ps * ps |> sqrt

let predict (theta:Vec) (v:Vec) = theta * v

let X = matrix [ for obs in data -> [ 1.; float obs.Instant ]]
let Y = vector [ for obs in data -> float obs.Cnt ]
```

![Image](../images/00011.jpeg) **提示**为了方便，我们创建两种类型，Vec 和 Mat，分别对应向量<浮点>和矩阵<浮点>。这对于简化我们的代码非常方便:Math.NET 支持除 float/double 之外的数字类型上的代数，这非常有用，但也会使代码变得混乱，需要一些额外的类型注释来澄清具体是什么类型。鉴于我们将专门对浮动进行操作，保持所有内容的通用性不会增加任何东西，并且会产生一些噪音，因此进行了简化。

现在，我们可以使用相同的θ值，将以前获得的预测和成本与我们新公式的结果进行比较:

```
let theta = vector [6000.; -4.5]

predict theta (X.Row(0))
cost theta Y X
```

一切都吻合。我们在安全的地面上，所以让我们继续估算θ。

正规形式

我们可以重写我们的批量梯度下降“代数风格”然而，我们将采取不同的方法来强调使用代数的一些好处。碰巧的是，我们试图解决的问题(找到一个使成本最小化的θ值)有数学人所说的封闭形式的解；也就是说，一个显式的，精确的解决方案，可以直接从输入计算，而不需要任何数值近似。

解释如何得出结果会超出本书的范围，并且不会增加理解机器学习部分的价值，所以我将简单地“按原样”陈述结果；如果你对此感到好奇，可以在你最喜欢的搜索引擎中搜索“范式回归”没有任何正当理由，问题

![9781430267676_Eq4-36.jpg](../images/00089.jpeg)

有一个解θ，就是

![9781430267676_Eq4-37.jpg](../images/00090.jpeg)

在这种情况下，拥有一个可用的线性代数库会带来巨大的回报。矩阵转置和求逆为我们实现了，我们的算法现在变成了一行程序。更好的是，我们不必经历调整学习率或运行数千次昂贵迭代的漫长过程，同时希望我们接近正确的值——我们一次就得到正确的解决方案。请参见以下内容:

```
let estimate (Y:Vec) (X:Mat) =
    (X.Transpose() * X).Inverse() * X.Transpose() * Y
```

这是不是让我们之前所有关于梯度下降的工作都成了浪费时间？当然不是！除了练习的教学价值(有望阐明一般过程)之外，梯度下降是一种更通用的算法，适用于例如我们决定使用不同的非线性成本函数的情况，前提是对导数进行适当的修改。相比之下，范式是一个非常具体的方程的解，并不概括开箱即用。也就是说，它解决的问题非常广泛。

和 MKL 一起踩到底

到目前为止，我们强调的使用以代数为中心的方法的主要优点是，对于合适的问题，它可以实现更紧凑的公式。代数在机器学习中如此重要还有另外一个完全不同的原因，那就是性能。

如果只是用于机器学习，代数在软件中可能不是什么大事。幸运的是，还有一个领域是开发者非常关心代数的，那就是游戏行业。3D 图形都是关于移动多边形，这从根本上说是一个代数问题。因此，由于对更快、更详细的游戏世界的不懈推动，人们已经付出了巨大努力来实现高性能代数，包括在硬件层面。该领域最明显的例子是 GPGPU(图形处理单元上的通用计算)，它通过使用最初设计时考虑到矢量图形的硬件，可以实现某些类型计算的巨大加速。

使用 GPGPUs 是强大的，但也需要一些工作。然而，Math.NET 为您提供了利用类似东西的选择，几乎不涉及任何工作。该库支持 MKL 提供者，这种机制本质上允许你将计算发送到 **MKL** 、**数学内核库**，这是一组内置于英特尔处理器中的高度优化的数学库。

在 Math.NET 中使用 MKL 是相当简单的。一旦您将相应的 NuGet 包添加到项目中，就会出现两个库:MathNet.Numerics.MKL.dll 和 libiomp5md.dll。右键单击 MKL dll，在“属性”中，将“复制到输出目录”设置为“总是复制”

提示 MKL 有多种版本:32 位对 64 位，Windows 对 Linux。安装 NuGet 包时，请确保选择适合您的机器的包。

此时，将计算发送到 MKL 就像这样简单:

```
System.Environment.CurrentDirectory <- __SOURCE_DIRECTORY__

open MathNet.Numerics
open MathNet.Numerics.Providers.LinearAlgebra.Mkl
Control.LinearAlgebraProvider <- MklLinearAlgebraProvider()
```

有些难看的第一行的目的是将当前目录设置为我们的源代码所在的目录；这是脚本获取位于该目录中的 MKL dll 所必需的。一旦完成，只需用 MKL 等价函数替换默认的 linear 代数提供者，就万事大吉了。

那么，这对我们到底有什么好处呢？根据你所使用的计算机和你所处理的问题的大小，里程会有所不同。通常，在更小的矩阵上(就像我们现在正在处理的矩阵)，这种好处即使看得见，也是微不足道的。然而，如果在更大的数据集上运行计算，十倍的加速并不罕见。

快速发展和验证模型

我们从一个有梯度下降的小锤子变成了一个有正常形式的非常强大的锤子。将模型拟合到我们的数据集突然变得相当简单和普通:只要我们可以将观察值转换为双精度向量，我们就成功了。在这一节中，我们将重新编写代码，以从我们的通用算法中获益。我们的目标是尽可能容易地改变我们使用数据进行预测的方式，并通过添加或删除特征来试验不同的模型。然而，如果我们要创建多个模型，我们首先需要的是一个比较它们质量的过程。让我们从把它放在适当的位置开始，一旦我们处于安全的位置，我们将探索如何快速发展和改进模型。

交叉验证和过度拟合

我们将交叉验证作为模型比较的基础，这并不奇怪。我们将留出一部分数据集用于验证，并只根据 70%的数据训练我们的模型，保留 30%用于验证目的。

请注意，这在我们的案例中尤为重要。考虑一下下面的情况:假设您正在考虑使用一组特性[theta 0；θ1；...θn]和替代模型[θ0；θ1；...ThetaNThetaM]—也就是说，相同的模型，但增加了一个额外的功能。第二次选择产生的误差会比第一次选择产生的误差更大吗？

答案是，第二个特征集包含第一个特征集的所有特征和一个附加特征集，其误差总是比第一个特征集小。作为一个非正式的证明，想想这个:在最坏的情况下，ThetaM 的值可能是 0，在这种情况下，两个模型是等价的，显然有相同的误差。所以，至少，在两种情况下，搜索最低可能成本的算法可以达到相同的水平。然而，在第二种情况下，一旦达到该值，存在将θm 从 0 改变为另一个值的选项，潜在地改变其他θ，以获得甚至更低的成本。

换句话说，我们添加的功能越多，成本就越低，不管这些额外的功能是好是坏。如果我机械地添加一个完全随机的特征，比如说，东京一磅芜菁的每日价格，我会得到一个更好的拟合——或者至少是一个同样好的拟合。

结论很简单:与我们的训练数据非常吻合并不总是一件好事。同样，我们追求的不是一个完全符合我们提供给它的数据的模型，而是一个根据它从未见过的新数据产生稳健预测的模型。

这里我们必须处理的一个额外的问题是，如果我们简单地切割数据集，使用前 70%的观察值进行训练，使用后 30%的观察值进行验证，我们的验证集将不会“代表”我们将来可能获得的任意新观察值。由于明显的季节模式，最后一大块把相似的观察结果组合在一起，完全忽略了某些情况。为了避免这种情况，我们将首先简单地对样本进行洗牌，以消除季节性影响，使用经典的 Fisher-Yates 随机洗牌算法，如下所示:

```
let seed = 314159
let rng = System.Random(seed)

// Fisher-Yates shuffle
let shuffle (arr:'a []) =
    let arr = Array.copy arr
    let l = arr.Length
    for i in (l-1) .. -1 .. 1 do
        let temp = arr.[i]
        let j = rng.Next(0,i+1)
        arr.[i] <- arr.[j]
        arr.[j] <- temp
    arr
```

shuffle 算法简单地获取一个通用数组并返回一个新数组，其中相同的元素以随机顺序重新排列:

```
let myArray = [| 1 .. 5 |]
myArray |> shuffle
>

val myArray : int [] = [|1; 2; 3; 4; 5|]
val it : int [] = [|4; 1; 5; 2; 3|]
```

![Image](../images/00011.jpeg) **提示**使用随机数发生器时，与系统一样。随机，指定种子通常是个好主意。这样，您可以将结果从一个会话复制到另一个会话，这有助于确保您的计算按照您期望的方式运行——当每次运行代码时结果都发生变化时，这是很难做到的！

我们现在可以创建一个训练和验证集，方法是简单地混合数据集并返回两个切片，前 70%作为训练，其余 30%作为验证:

```
let training,validation =
    let shuffled =
        data
        |> Seq.toArray
        |> shuffle
    let size =
        0.7 * float (Array.length shuffled) |> int
    shuffled.[..size],
    shuffled.[size+1..]
```

简化模型的创建

我们已经准备好了评估工具。在这一点上，我们想要的是通过包含或删除特性来轻松地创建各种模型，然后运行这些数字来看看什么有效。

我们需要的是一种将观察值转换成特征列表的方法，每个特征从观察值中提取一个浮点数，这样我们就可以将观察值转换成特征矩阵，并通过范式估计相应的θ。为了实现这一点，我们将创建一个新的类型 Featurizer，它将 Obs 转换成一个 floats 列表:

```
type Obs = Data.Row
type Model = Obs -> float
type Featurizer = Obs -> float list
```

使用这种方法，我们将能够通过列出我们希望如何使用“特征化器”从观察中提取数据来简化我们定义模型的方式作为一个例子，下面是我们如何重新创建简单的直线模型，用一个恒定的虚拟变量始终取值为 1.0，并从观察中提取瞬间:

```
let exampleFeaturizer (obs:Obs) =
  [   1.0;
      float obs.Instant; ]
```

为了方便起见，我们还创建一个预测函数，它将接受一个特征和一个向量θ，并返回一个将单个观察值转换为预测值的函数:

```
let predictor (f:Featurizer) (theta:Vec) =
    f >> vector >> (*) theta
```

当我们这样做的时候，让我们也创建一个函数来评估给定数据集上的特定模型的质量。这样，我们将能够在训练集和验证集上相互比较模型。我们肯定可以为此使用成本函数，但是，我们将使用另一个经典指标，平均绝对误差(又称 MAE)，即平均绝对预测误差，因为它更容易掌握。平均误差指示我们的预测平均偏离多少，如下所示:

```
let evaluate (model:Model) (data:Obs seq) =
    data
    |> Seq.averageBy (fun obs ->
        abs (model obs - float obs.Cnt))
```

现在让我们把这些放在一起:

```
let model (f:Featurizer) (data:Obs seq) =
    let Yt, Xt =
        data
        |> Seq.toList
        |> List.map (fun obs -> float obs.Cnt, f obs)
        |> List.unzip
    let theta = estimate (vector Yt) (matrix Xt)
    let predict = predictor f theta
    theta,predict
```

model 函数接受一个 featurizer f，描述我们希望从观察中提取什么特征，以及一个我们将用来训练模型的数据集。首先，我们从数据集中提取输出 Yt 和矩阵 Xt。然后，我们使用 Yt 和 Xt 估计θ，并返回向量θ，与相应的预测函数捆绑在一起，现在可以使用了。

向模型中添加连续特征

让我们在简单的直线模型上测试一下，我们称之为模型 0:

```
let featurizer0 (obs:Obs) =
    [   1.;
        float obs.Instant; ]

let (theta0,model0) = model featurizer0 training
```

很好很容易！现在，我们可以在训练集和验证集上评估模型 model0 的质量:

```
evaluate model0 training |> printfn "Training: %.0f"
evaluate model0 validation |> printfn "Validation: %.0f"
>
Training: 1258
Validation: 1167
```

将我们的模型结果与真实数据可视化(见[图 4-9](#Fig9) )显示出一条漂亮的直线，看起来确实非常合理:

```
Chart.Combine [
    Chart.Line [ for obs in data -> float obs.Cnt ]
    Chart.Line [ for obs in data -> model0 obs ] ]
```

![9781430267676_Fig04-09.jpg](../images/00091.jpeg)

[图 4-9](#_Fig9) 。直线回归的可视化

现在，我们可以开始向我们的模型添加特征，从可用的连续变量中进行选择，即具有数字意义的变量，如温度或风速，并将它们添加到新的特征:

```
let featurizer1 (obs:Obs) =
    [   1.
        obs.Instant |> float
        obs.Atemp |> float
        obs.Hum |> float
        obs.Temp |> float
        obs.Windspeed |> float
    ]

let (theta1,model1) = model featurizer1 training

evaluate model1 training |> printfn "Training: %.0f"
evaluate model1 validation |> printfn "Validation: %.0f"
>
Training: 732
Validation: 697
```

显然，我们正在做正确的事情:MAE 显著下降，在训练集(这是预料中的)和验证集(这是重要的)上都是如此。以前，我们平均减少了大约 1，200 台，现在减少到大约 750 台。不完美，但有明显的改善！如果我们绘制新的预测曲线，这是显而易见的——新的预测曲线开始更加接近目标(见[图 4-10](#Fig10) )。

![9781430267676_Fig04-10.jpg](../images/00092.jpeg)

[图 4-10](#_Fig10) 。包括更连续的特征

让我们从稍微不同的角度来看数据，并绘制实际值与预测值的对比图:

```
Chart.Point [ for obs in data -> float obs.Cnt, model1 obs ]
```

图 4-11 证实了我们的模型开始看起来像真的了。虽然散点图并不完美，但它大致遵循一条 45 度的直线，顶部有一些噪声。

![9781430267676_Fig04-11.jpg](../images/00093.jpeg)

[图 4-11](#_Fig11) 。预测与实际散点图

使用更多特征优化预测

在这一点上，我们已经建立了一个结构，它允许我们通过简单地添加或删除模型中的数据，以最少的代码变化，非常自由地进行实验。这很好，但是数据集中还有很多数据我们还没有利用。在这一节中，我们将探索如何更进一步，在模型中包含更多的特性，以改进我们的结果。

处理分类特征

我们已经在数据集中包含了大部分我们可以使用的数据，但是我们也遗漏了一些。比如说，假期怎么样？假设周末对自行车的使用有影响也不是没有道理。那么一周中的每一天呢——没有理由人们不会有不同的行为，比如说，在周一和周五。

然而，这里我们有一个问题:这些变量是**分类的**。而工作日被编码为 0，1，...6 对于周日到周一，这些数字没有数值意义。一个星期三抵不上一个星期一的三倍，而且将自行车的使用建模为工作日代码的倍数也是没有意义的。

然而，我们可以做的是，使用同样的技巧，来表示方程中的常数项。如果我们为一周中的每一天创建一个特征，例如，为星期一创建一个特征，将观察值标记为 1.0，否则为 0.0，那么θ的估计值应该能够捕捉到星期一的影响。让我们试试这个:

```
let featurizer2 (obs:Obs) =
    [   1.
        obs.Instant |> float
        obs.Hum |> float
        obs.Temp |> float
        obs.Windspeed |> float
        (if obs.Weekday = 0 then 1.0 else 0.0)
        (if obs.Weekday = 1 then 1.0 else 0.0)
        (if obs.Weekday = 2 then 1.0 else 0.0)
        (if obs.Weekday = 3 then 1.0 else 0.0)
        (if obs.Weekday = 4 then 1.0 else 0.0)
        (if obs.Weekday = 5 then 1.0 else 0.0)
        (if obs.Weekday = 6 then 1.0 else 0.0)
    ]

let (theta2,model2) = model featurizer2 training
```

和...呃？我们对θ的估计返回为 seq[nan；南；南；南；...].这是怎么回事？我们面临的问题的专业术语是**共线性**。在探讨为什么我们会遇到这个问题之前，让我们先来看一个简单的案例，它说明了这个问题的根源。还记得我们的第一个模型吗，我们将特性定义如下:

*和*=*<sub class="calibre17">【0】</sub>+*<sub class="calibre17">*备注。即时〔t11〕*</sub>**

 **现在想象一下，我们修改了特性并选择了以下规范:

*和*=*<sub class="calibre17">【0】</sub>+*<sub class="calibre17">*备注。即时*+*【ηt13】*×*备注。即时〔t17〕***</sub>**

 *当然，这可能是一个愚蠢的规范，但是让我们先看一下。在这种情况下，会发生什么？我们会有一点问题，就是会发生什么。对于θ1 和θ2，有无限多种可行的组合。如果我们的θ1 的原始最优值(在初始模型中)是，比如说，10.0，那么θ1 和θ2 的总和为 10.0 的任何组合对于修改后的模型也将是最优的。10.0 和 0.0、5.0 和 5.0、1000.0 和-990.0 将同样适用，我们没有办法在其中任何一个之间做出决定。这本质上是算法面临的相同问题:它不能为θ找到唯一的最优值，因此悲惨地失败了。

那么，为什么我们会有共线性的问题呢？如果你想一想，如果我们的任何一个特征都可以表示为其他特征的线性组合，我们就会遇到这个问题。任何一天都必须是现有的七个工作日中的一天，所以星期一+星期二+...+星期日= 1.0。但 1.0 也是我们常数项的值，它映射到θ0。对此我们能做些什么？

我们可以做两件事。首先，我们可以从列表中删除一个工作日。在这种情况下，实际上我们要做的是将那一天设置为“参考日”，分配给剩余六天中每一天的 Thetas 将捕捉与那一天相比的差异损益。另一种方法是从我们的方程中去掉常数项；在这种情况下，我们将最终得到一周七天中每一天的直接估计值。

让我们继续并删除星期日，这将是我们的参考点。评估现在运行得像冠军一样，我们甚至得到了一个小的质量改进:

```
let featurizer2 (obs:Obs) =
    [   1.
        obs.Instant |> float
        obs.Hum |> float
        obs.Temp |> float
        obs.Windspeed |> float
        (if obs.Weekday = 1 then 1.0 else 0.0)
        (if obs.Weekday = 2 then 1.0 else 0.0)
        (if obs.Weekday = 3 then 1.0 else 0.0)
        (if obs.Weekday = 4 then 1.0 else 0.0)
        (if obs.Weekday = 5 then 1.0 else 0.0)
        (if obs.Weekday = 6 then 1.0 else 0.0)
    ]

let (theta2,model2) = model featurizer2 training

evaluate model2 training |> printfn "Training: %.0f"
evaluate model2 validation |> printfn "Validation: %.0f"

>
Training: 721
Validation: 737
```

如果我们稍微检查一下θ，将最后六个系数与相应的日子匹配，似乎周六是骑自行车最受欢迎的日子，而周日最好做点别的事情。

我们不会进一步深入到我们的模型中可能包含更多的分类特性，但是请随意使用它，看看您能在其中获得多少改进！我们想在这里提出的两个要点是，第一，通过将类别分成单独的特征，并使用指示变量(1 表示存在，0 表示不存在)来表示它们的激活，以最小的努力在回归模型中使用分类变量是可能的。然后，我们讨论了共线性及其如何在模型中造成破坏，以及出现问题时的处理方法。

非线性特征

我们包括了几乎所有的东西和厨房水槽。难道没有别的方法可以用来改进我们的预测吗？

为了回答这个问题，让我们首先再看一下数据，并绘制自行车使用与温度的关系图(见[图 4-12](#Fig12) ):

```
Chart.Point [ for obs in data -> obs.Temp, obs.Cnt ]
```

![9781430267676_Fig04-12.jpg](../images/00094.jpeg)

[图 4-12](#_Fig12) 。自行车使用与温度散点图

数据相当混乱，但总体来说，似乎只有在某一点上，使用量会随着温度上升，之后开始下降。这不是没有道理的:当天气非常冷或非常热时，骑自行车会失去一些吸引力。不幸的是，我们目前的模型不太适合捕捉这一点。所有的效应都表示为线性函数；也就是说，它们的形式是θx 特征值。如果θ为正，则所有因素保持不变，较高的特征值将始终导致较高的预测值；相反，如果θ为负值，则较高的特征值将始终产生较低的预测值。

这并不能满足我们想要捕捉的温度。我们预计，当温度变得非常低或非常高时，使用量都会下降。那么，我们能做什么呢？

如果一个线性关系是不够的，我们可以使用其他东西，例如，高阶多项式。最简单可行的方法是在混合中加入一个平方项。我们没有将温度的影响建模为

![9781430267676_Eq4-38.jpg](../images/00095.jpeg)

我们可以去

![9781430267676_Eq4-39.jpg](../images/00096.jpeg)

让我们尝试一下这个想法，暂时忽略所有其他特征，来演示从这个等式得出什么曲线:

```
let squareTempFeaturizer (obs:Obs) =
    [   1.
        obs.Temp |> float
        obs.Temp * obs.Temp |> float ]

let (_,squareTempModel) = model squareTempFeaturizer data

Chart.Combine [
    Chart.Point [ for obs in data -> obs.Temp, obs.Cnt ]
    Chart.Point [ for obs in data -> obs.Temp, squareTempModel obs ] ]
```

![9781430267676_Fig04-13.jpg](../images/00097.jpeg)

 。拟合方形温度

我们得到的不是一条直线，而是一条略微弯曲的曲线。曲线增加达到最大值，然后再次降低。由该方程生成的曲线类被称为抛物线，它总是遵循相同的一般形状(围绕最小值或最大值对称)，这使得它成为捕捉模型中的效果的自然候选，其中一个特征对于某个值达到峰值影响。

现在让我们将这一新特性整合到我们的模型中，看看是否有所改进:

```
let featurizer3 (obs:Obs) =
    [   1.
        obs.Instant |> float
        obs.Hum |> float
        obs.Temp |> float
        obs.Windspeed |> float
        obs.Temp * obs.Temp |> float
        (if obs.Weekday = 1 then 1.0 else 0.0)
        (if obs.Weekday = 2 then 1.0 else 0.0)
        (if obs.Weekday = 3 then 1.0 else 0.0)
        (if obs.Weekday = 4 then 1.0 else 0.0)
        (if obs.Weekday = 5 then 1.0 else 0.0)
        (if obs.Weekday = 6 then 1.0 else 0.0)
    ]

let (theta3,model3) = model featurizer3 training

evaluate model3 training |> printfn "Training: %.0f"
evaluate model3 validation |> printfn "Validation: %.0f"
 >
Training: 668
Validation: 645
```

不错！我们的误差从 750 降到了 670 以下，这是一个非常大的进步。更好的是，我们现在有了一个尝试进一步完善模型的大方向。我们可以引入新的特征——例如，其他变量的平方，或者两个变量的乘积——来捕捉模型中其他潜在的非线性效应。我们甚至可以更进一步，扩展到更高层次的多项式，如立方或其他函数形式，比如指数或对数。

这种方法的一个小缺点是对结果的解释变得更加棘手。在线性模型中，理解每个θ系数的含义相当简单。可以理解为乘数:将一个特性的值增加 1.0，输出将增加(或减少)θ。

这就产生了一个难题。首先，这引入了一个实际问题。随着我们扩展要考虑的潜在特性的数量，手动检查哪些特性应该被省略或包含将变得相当麻烦。例如，如果我们有 N 个变量，一个线性模型将包含 N + 1 个特征(每个变量和一个常数项)。包括每个特征和它们的正方形之间的相互作用增加了 N x (N + 1) / 2，并且要考虑可能惊人数量的组合。对于具有中等数量变量的数据集，手动检查每个组合仍然是可能的，但随着数据集宽度的增加，这将不起作用。

正规化

更大的问题当然是我们之前讨论过的那一点。如果我们能把所有的功能和厨房水槽都扔给模特，我们会做得很好。但是，正如我们之前看到的，盲目地这样做会机械地提高训练集的质量，而不一定会产生健壮的预测模型。这个问题被称为**过度拟合**:搜索算法使用它可以用来拟合数据集的所有特征，在这个过程中，最终在那个任务中做得太好了，潜在地捕获了训练数据集中不会泛化的工件。

那么，我们如何避免这个问题呢？一种常用的方法被称为**正则化**。想法如下:过度拟合的症状之一是，在试图用特征拟合训练集时，算法将开始为一些θ系数产生“野生”值，而在“合理”的模型中，系数将被期望保持在某个范围内，强制输入的变化不会引起疯狂的波动。

我们不会讨论正则化的整个例子，而是简单地概括出你将如何去做。

如果目标是防止每个参数θ波动太大，一种可能性是引入惩罚，使得高的个体值将比小的个体值更昂贵。例如，我们可以在成本函数中引入以下惩罚:

![9781430267676_Eq4-40.jpg](../images/00098.jpeg)

为什么这是一个好的候选人？正如我们之前在原始成本函数的上下文中所讨论的，因为平方函数随着值远离零而增加得更快，所以每个单独θ的更大值将受到更重的惩罚。因此，在默认情况下，这种惩罚倾向于支持更加平衡的值，而不是高度分化的值。例如，θ为[1.0；1.0]的成本为 2.0，而[0.0；2.0]将产生 4.0 的开销。

将这些放在一起，我们现在可以用下面的内容替换我们原来的成本函数:

![9781430267676_Eq4-41.jpg](../images/00099.jpeg)

这个公式有一个相当简单的解释。模型的成本包括两个因素:拟合的误差有多大，参数有多小。换句话说，现在存在通过增加参数θ来改进拟合的成本，并且算法将“偏好”使用更平衡的参数的拟合改进。

在研究如何在实际算法中使用它之前，有几个重要的技术问题需要考虑。

第一个是，为了实现这一点，每个功能都需要有一个可比较的规模。这很重要，因为我们引入的惩罚项假设系数θ的类似变化可以相互比较。我们试图通过这种惩罚来实现的一种方式是“当您可以通过增加特征的系数来提高拟合质量时，最好增加具有最小系数的特征。”这是一个很好的标准，只要特征本身具有可比性。例如，在我们的案例中，obs。瞬间从 1 到 731，而 obs。风速从大约 0 到 0.5。将瞬时系数增加 1.0 会导致输出在 1 和 731 之间变化，而风速的相同变化最多影响 0.5。

用来解决这个问题的方法叫做**规范化**。标准化简单地获取模型中的所有特征，并重新调整它们的比例，使它们具有可比性。有几种方法可以做到这一点；最简单的方法是通过应用以下变换将每个要素放在 0 到 1 的范围内:normalized = (x - xmin) / (xmax - xmin)。

值得注意的第二点是正则化罚项中省略了θ0。这不是错别字。首先，我们很难找到一种方法来使它正常化，因为该特性的每一个观察值都是 1.0。然后，θ0 扮演偏移的角色:对于每一个被归一化的其他特征，θ0 设置模型的基线，该基线可以是任何值。

第三个也是最后一个注意事项是罚项前面的参数 lambda。这个参数可以解释为一个权重；它决定了正则化惩罚在成本函数中所占的比重。该参数需要手动调整，以确定哪种级别的正则化效果最好。这个过程是一个习惯性的过程:尝试不同的级别，选择在验证集上最有效的 lambda 值；即误差最小的值。

假设数据集已经预先归一化，我们可以使用这个新的公式，用下面的更新规则，很容易地更新我们原来的梯度下降算法:

![9781430267676_Eq4-42.jpg](../images/00100.jpeg)

那么，我们学到了什么？

让我们面对它，这是一个密集的章节。我们讨论了很多想法，并花了相当多的时间以一种相当抽象的数学方式讨论问题。让我们看看是否可以重新分组并总结本章的主要学习内容。

利用梯度下降最小化成本

我们在本章中讨论的最重要的技术适用于机器学习的多个领域，而不仅仅是回归。我们提出了梯度下降算法，这是一种识别使函数值最小化的参数的通用方法，通过沿着梯度(从当前状态开始的最陡下降方向)迭代调整参数。

梯度下降一开始可能看起来有点吓人，因为它附带了大量的微积分，但是一旦通过了最初的障碍，它就是一个相当简单的算法。更有趣的是，它适用于广泛的情况；本质上，如果存在最小值，且函数可微(我们确实需要导数！)而且不要太病态，你身材很好。事实证明，这使它成为解决大多数机器学习问题的一个很好的工具:如果你试图找到一个最适合数据集的模型，你就是在最小化某种形式的距离或成本函数，梯度下降可能会帮助你找到它。

我们还看到了算法的两种变体，随机和批量梯度下降，每种都有自己的优点和局限性。随机梯度下降一次更新一个观测值的参数。它不需要一次对整个数据集进行操作，这使得它便于在线学习。如果您的数据集随着时间的推移而扩展，并且随着时间的推移您会收到新的观察值，那么您可以只使用最新到达的数据点来更新您的估计值，并保持逐步学习。正如我们在例子中看到的，缺点是它相对较慢，并且可能不太稳定:虽然算法总体上保持正确的方向，但每个单独的步骤都不能保证产生改进。相比之下，批量梯度下降一次对整个数据集进行操作，在每一步产生一个改进的模型。缺点是每一步的计算量都更大；此外，使用整个数据集可能不切实际，尤其是当数据集很大时。

用回归预测一个数

我们在这一章中追求的主要目标是使用我们在某一天可用的任何数据来预测某一天自行车共享服务的用户数量。这就是所谓的回归模型。与分类模型不同，分类模型的目标是预测一个观察值属于哪个类别，在这里，我们试图通过找到一个函数来预测一个数值，该函数结合了各种特征并尽可能地拟合数据。

从最简单的模型开始，通过数据拟合一条直线，我们逐步建立了一个更复杂的模型，包括更多的特征，并说明了一些简单的技术。特别是，我们演示了如何超越连续特征，通过将分类特征分解为多个特征，每个特征用 1 或 0 表示哪个状态是活动的，从而将分类特征包含在模型中。我们还看到了如何将非线性项(如平方或高次多项式)包含在内，从而对线性模型无法正确捕捉的输入和输出之间的更复杂关系进行建模。

向模型中添加更复杂的特征是一把双刃剑，更好预测的潜力是有代价的。首先，它造成了特性数量的爆炸，管理包括或不包括哪些特性本身就成了一个问题。然后，它引入了一个更深层次的问题:添加任何功能都会机械地提高模型与训练集的拟合程度，不管它是否与手头的问题有任何关系。这个问题被称为过度拟合:更多的特征给了算法更多的“自由”，以创造性的、扭曲的方式使用它们，更好的拟合不一定转化为更好的预测。

我们讨论了使用交叉验证和规范化来缓解这个问题的技术解决方案。然而，作为最后一点，考虑到这里存在一种内在的紧张关系可能是有用的。一个模型有两个明显的用处:理解一个现象，或者做出预测。添加更多数据通常有助于做出更好的预测，但也会带来复杂性。相反，更简单的模型可能不太准确，但有助于理解变量如何一起工作，并可以验证事情是否有意义。在建立一个模型之前，问问你自己，你是更喜欢一个非常精确但不透明的黑盒模型，还是一个不太精确但可以向同事解释的模型。这可能有助于指导你一路走来的决定！

有用的链接

*   FSharp。图表是一个稳定的库，在探索来自 http://fslab.org/FSharp.Charting/ FSI 的数据时方便创建基本图表:[T1】](http://fslab.org/FSharp.Charting/)
*   Math.NET 是一个开源库，涵盖了广泛的数学和数值分析工具。特别是它包含了基本的线性代数算法:[http://www.mathdotnet.com/](http://www.mathdotnet.com/)
*   如果您需要算法的更高性能，即使它需要更复杂的实现，quantalea 也提供了一个非常有趣的。NET-to-GPGPU 库，哪个对 F#特别友好:[http://www.quantalea.net/](http://www.quantalea.net/)
*   在撰写本文时，DiffSharp 仍在发展，它是一个很有前途的自动微分库，可以大大简化梯度计算:[http://gbaydin.github.io/DiffSharp/](http://gbaydin.github.io/DiffSharp/)***