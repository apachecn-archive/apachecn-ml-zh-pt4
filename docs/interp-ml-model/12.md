# 12.详述图像可解释性方法

这一章着重于图像模型的解释方法。深度学习已经非常成功，尤其是在涉及图像和文本的任务中，比如图像分类和语言翻译。深度神经网络的成功故事始于 2012 年，当时一种深度学习方法赢得了 ImageNet 图像分类挑战。从那时起，我们见证了深度神经网络架构的爆炸式发展，趋势是更深层次的网络具有越来越多的权重参数。

为了用神经网络进行图像预测，数据输入通过与学习的权重相乘的许多层，并通过非线性变换。根据神经网络的结构，单个预测可能涉及数百万次数学运算。我们人类不可能遵循从数据输入到预测的精确映射。我们必须考虑数百万个以复杂方式相互作用的权重，才能理解神经网络的预测。

为了解释神经网络的行为和预测，我们需要特定的解释方法。这本书假设你熟悉深度学习，包括卷积神经网络。你当然可以使用模型无关的方法，比如局部模型或者部分依赖图。尽管如此，有两个原因说明为什么考虑专门为神经网络开发的解释方法是有意义的。首先，神经网络学习其隐藏层中的特征和概念，我们需要特殊的工具来揭示它们。第二，梯度可用于实现比“从外部”观察模型的模型不可知方法在计算上更有效的解释方法此外，本书中的大多数其他方法旨在解释表格数据的模型。图像和文本数据需要不同的方法。

## 使用石灰的图像解释

让我们从模型可解释性的 LIME 方法开始。这是一种流行的方法，因为它可以用于表格、图像和文本数据。用一个例子，我们来探讨一下 LIME 是如何解读图像的。问题陈述如图 [12-1](#Fig1) 所示。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig1_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig1_HTML.jpg)

图 12-1

图像解释的问题描述

图像的 LIME 与表格数据和文本的 LIME 工作方式不同。直觉上，干扰单个像素没有多大意义，因为一个以上的像素对一个类有贡献。随机改变单个像素可能不会改变预测太多。因此，通过将图像分割成超像素并关闭或打开它们来创建图像的变体。超像素是具有相似颜色的互连像素，可以通过用用户定义的颜色(如灰色)替换每个像素来关闭。用户还可以指定在每个排列中关闭超像素的概率。

让我们使用下面的示例代码来学习图像解释的概念。首先，读取图像并使用 Keras 中提供的预训练的 InceptionV3 模型来预测图像类。

```
# Read Image

import numpy as np
import skimage
Xi = skimage.io.imread("https://arteagac.github.io/blog/lime_image/img/cat-and-dog.jpg")
Xi = skimage.transform.resize(Xi, (299,299))
Xi = (Xi - 0.5)*2 #Inception pre-processing

skimage.io.imshow(Xi/2+0.5) # Show image before inception preprocessing

#Predict class for image using InceptionV3
import keras
from keras.applications.imagenet_utils import decode_predictions
np.random.seed(222)
inceptionV3_model = keras.applications.inception_v3.InceptionV3() #Load pretrained model
preds = inceptionV3_model.predict(Xi[np.newaxis,:,:,:])
top_pred_classes = preds[0].argsort()[-5:][::-1] # Save ids of top 5 classes
decode_predictions(preds)[0] #Print top 5 classes

```

该脚本将输入图像加载到 Xi 变量中，并打印图像的前五个类别(和概率)，如图 [12-2](#Fig2) 所示。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig2_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig2_HTML.jpg)

图 12-2

用于问题的图像

*   拉布拉多寻回犬(82.2%)

*   金毛寻回犬(1.5%)

*   美国斯塔福德郡梗(0.9%)

*   公牛獒犬 （0.8%）

*   大丹犬(0.7%)

有了这些信息、输入图像和预训练的 InceptionV3 模型，我们可以用 LIME 生成解释。这个例子为拉布拉多猎犬类 ***生成解释。**T3】*

LIME 通过围绕被解释的实例生成随机扰动(及其各自的预测)的新数据集，然后拟合加权局部代理模型来创建解释。这种局部模型通常是具有内在可解释性的更简单的模型，例如线性回归模型。

### 第一步。为输入图像生成随机扰动

对于图像，LIME 通过打开和关闭图像中的一些超像素来产生扰动。以下脚本使用快速移动分割算法来计算图像中的超像素。此外，它还生成了一个由 150 个扰动组成的阵列，其中每个扰动都是一个由 0 和 1 组成的向量，代表超像素是开还是关。

```
#Generate segmentation for image
import skimage.segmentation
superpixels = skimage.segmentation.quickshift(Xi, kernel_size=4,max_dist=200, ratio=0.2)
num_superpixels = np.unique(superpixels).shape[0]
skimage.io.imshow(skimage.segmentation.mark_boundaries(Xi/2+0.5, superpixels))

#Generate perturbations
num_perturb = 150
perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))

#Create function to apply perturbations to images
import copy
def perturb_image(img,perturbation,segments):
  active_pixels = np.where(perturbation == 1)[0]
  mask = np.zeros(segments.shape)
  for active in active_pixels:
      mask[segments == active] = 1
           perturbed_image = copy.deepcopy(img)
           perturbed_image = perturbed_image*mask[:,:,np.newaxis]
  return perturbed_image

#Show example of perturbations
print(perturbations[0])
skimage.io.imshow(perturb_image(Xi/2+0.5,perturbations[0],superpixels))

```

计算图像中的超像素的结果如图 [12-3](#Fig3) 所示。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig3_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig3_HTML.jpg)

图 12-3

超像素输出

图 [12-4](#Fig4) 显示了扰动向量和扰动图像的例子。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig4_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig4_HTML.jpg)

图 12-4

扰动图像的输出

### 第二步。预测扰动的类别

下面的脚本使用`inceptionV3_model`来预测每个扰动图像的类别。预测的形状是(150，1000)，这意味着对于 150 个图像中的每一个，都有属于 InceptionV3 中的 1，000 个类的概率。在这 1000 个类中，我们在后续步骤中仅使用 Labrador 类，因为它是我们想要解释的预测。在这个例子中，使用了 150 次扰动。然而，在实际应用中，大量的扰动会产生更可靠的解释。

```
predictions = []
for pert in perturbations:
  perturbed_img = perturb_image(Xi,pert,superpixels)
  pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])
  predictions.append(pred)
predictions = np.array(predictions)
print(predictions.shape)

```

现在我们有了一切来拟合一个线性模型，使用扰动作为输入特征`X`和拉布拉多的预测`predictions[labrador]`作为输出`y`。然而，在我们拟合线性模型之前，LIME 需要给与被解释图像更接近的图像更多的权重(重要性)。

### 第三步。计算扰动的权重(重要性)

我们使用距离度量来评估每个扰动离原始图像有多远。原始图像是所有超像素都有效(所有元素合二为一)的扰动。假设扰动是多维向量，余弦距离是可用于此目的的度量。计算余弦距离后，核函数将距离转换为 0 到 1 之间的值(权重)。在这个过程的最后，我们对数据集中的每个扰动进行加权(重要性)。

```
#Compute distances to original image
import sklearn.metrics
original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled
distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()
print(distances.shape)

#Transform distances to a value between 0 an 1 (weights) using a kernel function
kernel_width = 0.25
weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function
print(weights.shape)

```

### 第四步。使用扰动、预测和权重拟合一个可解释的线性模型

我们使用前面步骤中获得的信息拟合加权线性模型。我们为图像中的每个超像素获得一个系数，该系数表示超像素在拉布拉多寻回犬预测中的影响有多强。

```
#Estimate linear model
from sklearn.linear_model import LinearRegression
class_to_explain = top_pred_classes[0] #Labrador class
simpler_model = LinearRegression()
simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)
coeff = simpler_model.coef_[0]

#Use coefficients from linear model to extract top features
num_top_features = 4
top_features = np.argsort(coeff)[-num_top_features:]

#Show only the superpixels corresponding to the top features
mask = np.zeros(num_superpixels)
mask[top_features]= True #Activate top superpixels
skimage.io.imshow(perturb_image(Xi/2+0.5,mask,superpixels))

```

我们需要对这些系数进行排序，以确定拉布拉多寻回犬预测中最重要的超像素(`top_features`)。尽管我们使用系数的大小来确定最重要的特征，但是诸如向前或向后消除之类的其他选择也可以用于特征重要性选择。图 [12-5](#Fig5) 显示了计算后的顶级超像素。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig5_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig5_HTML.jpg)

图 12-5

输入图像的顶部超像素

这是 LIME 返回的解释。图像中与拉布拉多猎犬预测有较强关联的区域(超像素)。这一解释表明，预训练的 InceptionV3 模型在预测给定图像的拉布拉多猎犬类别方面做得很好。这个例子显示了 LIME 如何通过理解为什么返回某些预测来增加机器学习模型中的信心。

## 使用像素属性(显著图)的图像解释

像素属性方法通过神经网络突出与特定图像分类相关的像素。图 [12-6](#Fig6) 是一个解释的例子。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig6_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig6_HTML.jpg)

图 12-6

像素属性样本图像

可以在各种名称下找到像素属性方法:灵敏度图、显著图、像素属性图、基于梯度的属性方法、特征相关性、特征属性和特征贡献。

像素属性是图像特征属性的特例。要素归因通过根据输入要素如何改变预测(负面或正面)来归因每个输入要素来解释各个预测。这些要素可以是输入像素、表格数据或文字。SHAP 值、沙普利值和莱姆值是一般特征归属方法的示例。我们考虑输出长度为 C 的向量作为预测的神经网络，它包括回归，其中 C=1。用于图像 I 的神经网络的输出称为 S(I)=[S1(I)，…，SC(I)]。所有这些方法都将具有 p 个特征的 x∈Rp(可以是图像像素、表格数据、单词等)作为输入，并将 p 个输入特征中的每一个的相关性值作为解释输出:Rc=[Rc1，…，Rcp]。c 表示第 c 个输出 SC(I)的相关性。

有大量令人困惑的像素归属方法。这有助于理解有两种不同类型的归因方法。

*   **基于遮挡或扰动的**:像 SHAP 和莱姆这样的方法处理部分图像来生成解释(模型不可知)。

*   **基于梯度的**:许多方法计算关于输入特征的预测梯度(或分类分数)。基于梯度的方法(有许多种)最大的不同在于梯度是如何计算的。

这两种方法的共同点是解释与输入图像的大小相同(或者可以有意义地投射到输入图像上)。它们为每个像素分配一个值，该值可以被解释为该像素与该图像的预测或分类的相关性。

像素归属方法的另一个有用的分类是基线问题。

*纯梯度方法*判断像素的变化是否会改变预测。普通渐变和 Grad-CAM 就是两个例子。仅梯度属性的解释是:如果我们要改变这个像素，预测的类概率将上升(对于正梯度)或下降(对于负梯度)。梯度的绝对值越大，该像素的变化效果越强。

接下来的几个部分解释了激活图的图像解释。

## 使用类别激活图的图像解释

*类激活图* (CAM)在论文“学习深度特征用于区分性定位”中使用 CNN 中的全局平均池进行了介绍。特定类别中的 CAM 表示 CNN 用来识别类别的区分区域。

已经观察到，卷积神经网络的各层的卷积单元充当对象检测器，即使在为分类任务训练网络时没有提供关于对象位置的这种先验。尽管卷积具有这种显著的属性，但当使用完全连接的图层执行分类任务时，它会丢失。为了避免使用完全连接的网络，一些架构如网络中的网络(NiN)和 GoogLeNet 是完全卷积的神经网络。global*average pooling*(GAP)是这类架构中常用的层。它主要用作调整剂，以防止训练时过度拟合。类别激活图简单地指示图像中的区分区域，CNN 使用该区分区域将该图像分类到特定类别中。对于此技术，网络由 ConvNet 组成，并且就在 softmax 图层(用于多类分类)之前，对卷积要素地图执行全局平均池。该图层的输出被用作生成所需分类输出的全连接图层的要素。给定这个简单的连通性结构，我们可以通过将输出图层的权重投影回卷积特征图上来识别图像区域的重要性。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig7_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig7_HTML.jpg)

图 12-7

CAM 的理想网络架构示例

实现这一点的方法将在下面的步骤中详细介绍。

### 第一步。修改模型

假设您已经用 Conv 块和几个完全连接的层构建了深度分类器。您必须修改这个体系结构，以便没有任何完全连接的层。在输出层(softmax/sigmoid)和最后一个卷积块之间使用 GlobalAveragePooling2D 层。CAM 模型为猫狗分类器提供了所需的修改。这里我们使用预训练的 VGG16 模型来模拟训练好的分类器。

```
def CAMmodel():
      ## Simulating my pretrained dog and cat classifier.
      vgg = VGG16(include_top=False, weights='imagenet')
      vgg.trainable = False
      ## Flatten the layer so that it's not nested in the sequential model.
      vgg_flat = flatten_model(vgg)
      ## Insert GAP
      vgg_flat.append(keras.layers.GlobalAveragePooling2D())
      vgg_flat.append(keras.layers.Dense(1, activation='sigmoid'))

      model = keras.models.Sequential(vgg_flat)
      return model

```

一个简单的工具 flatten_model 返回我的预训练模型中的层列表。这样做是为了在使用顺序模型修改时各层不会嵌套，并且可以访问最后一个卷积层并将其用作输出。我们在 flatten_model 返回的数组中追加了 GlobalAveragePooling2D 和 Dense。最后，返回顺序模型。

```
def flatten_model(model_nested):
      '''
      Utility to flatten pretrained model
      '''
      layers_flat = []
      for layer in model_nested.layers:
           try:
                layers_flat.extend(layer.layers)
           except AttributeError:
                layers_flat.append(layer)

      return layers_flat

```

接下来，我们用适当的模型输入形状调用`model.build()`。

```
keras.backend.clear_session()
model = CAMmodel()
model.build((None, None, None, 3)) # Note
model.summary()

```

### 第二步。用 CAMLogger 回调重新训练模型

由于引入了新的层，我们必须重新训练模型。但是我们不需要重新训练整个模型。我们可以通过使用`vgg.trainable=False`来冻结卷积块。当我们检查模型性能时，我们看到模型性能在训练和验证准确性方面都有所下降。因此，为了实施 CAM，我们必须修改我们的架构，从而降低模型性能。

### 第三步。使用 CAMLogger 查看类激活图

在`CAM`类的`__init__`中，我们初始化`cammodel`。注意这个 CAM 模型有两个输出。

最后一个卷积层的输出(`block5_conv3`)。模型预测(softmax/sigmoid)。

```
class CAM:
  def __init__(self, model, layerName):
    self.model = model
    self.layerName = layerName
    ## Prepare cammodel
    last_conv_layer = self.model.get_layer(self.layerName).output
    self.cammodel = keras.models.Model(inputs=self.model.input,
                                       outputs=[last_conv_layer, self.model.output])

  def compute_heatmap(self, image, classIdx):
    ## Get the output of last conv layer and model prediction
    [conv_outputs, predictions] = self.cammodel.predict(image)
    conv_outputs = conv_outputs[0, :, :, :]
    conv_outputs = np.rollaxis(conv_outputs, 2)
    ## Get class weights between
    class_weights = self.model.layers[-1].get_weights()[0]
    ## Create the class activation map.
    caml = np.zeros(shape = conv_outputs.shape[1:3], dtype=np.float32)
    for i, w in enumerate(class_weights[:]):
      caml += w * conv_outputs[i, :, :]    caml /= np.max(caml)
    caml = cv2.resize(caml, (image.shape[1], image.shape[2]))
    ## Prepare heatmap

    heatmap = cv2.applyColorMap(np.uint8(255*caml), cv2.COLORMAP_JET)
    heatmap[np.where(caml < 0.2)] = 0    return heatmap

def overlay_heatmap(self, heatmap, image):
    img = heatmap*0.5 + image
    img = img*255
    img = img.astype('uint8')
    return (heatmap, img)

```

compute_heatmap 方法负责生成热图，这是 CNN 用来识别类别(图像类别)的区分区域。

*   输入图像上的 cammodel.predict()给出了形状(1，7，7，512)的最后一个卷积层的特征图。

*   我们还提取形状(512，1)的输出层的权重。

*   计算从最终层提取的权重和特征图的点积，以产生类别激活图。

现在让我们把所有的东西都放在回调中。CAMLogger 回调集成了 wandb.log()方法，将生成的激活映射记录到 W&B 运行页面。通过调用 overlay_heatmap()方法，最终将 CAM 返回的热图叠加在原始图像上。

### 第四步。从 CAM 中得出结论

从图 [12-8](#Fig8) 所示的曲线图中可以得出很多结论。如果预测得分大于 0.5，则网络将图像分类为狗；否则，它被归类为猫。CAM 图表有其相应的类激活映射。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig8_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig8_HTML.jpg)

图 12-8

查看狗的脸和猫的胡须、爪子和耳朵的 CAM 模型的输出。在错误分类的图像中，模型没有看它应该看的地方。因此，通过使用 CAM，我们能够解释这种错误分类背后的原因，这真的很酷

让我们来看一些观察。

*   该模型通过查看图像中的面部区域将图像分类为狗。在一些图像中，它能够看到整个身体，除了爪子。

*   该模型通过观察耳朵、爪子和胡须将图像分类为猫。

## 使用梯度加权类别激活图的图像解释

Grad-CAM 是一种用于可视化卷积神经网络模型的流行技术。Grad-CAM 是特定于类别的，这意味着它可以为图像中出现的每个类别生成单独的可视化效果，如图 [12-9](#Fig9) 所示。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig9_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig9_HTML.jpg)

图 12-9

Grad-CAM 图像示例

Grad-CAM 是一种用于可视化卷积神经网络模型的流行技术。它是特定于类的，这意味着它可以为图像中出现的每个类生成单独的可视化效果。Grad-CAM 可用于弱监督定位，即使用仅在完整图像标签而非显式位置注释上训练的模型来确定对象的位置。它也可以用于弱监督分割。该模型预测属于特定对象的所有像素，而不需要用于训练的像素级标签。最后，Grad-CAM 可用于更好地理解模型，例如，通过提供对模型故障的洞察。以下是 Grad-CAM 值得注意的几点。

*   Grad-CAM as **事后关注** : Grad-CAM 是事后关注的一种形式，这意味着它是一种产生热图的方法，这些热图应用于经过训练的神经网络，并且参数是固定的。这与可训练的注意力不同，可训练的注意力涉及通过学习参数来学习如何在训练期间产生注意力图(热图)。

*   Grad-CAM 作为 CAM 的**推广**: Grad-CAM 不需要特定的 CNN 架构。Grad-CAM 是 CAM 的推广，是一种需要使用特定架构的方法。CAM 需要一个将 GAP 应用于最终卷积特征图的架构，然后是一个产生预测的全连接层。

Grad-CAM 背后的基本思想与 CAM 背后的基本思想相同:我们希望利用通过卷积层保存的空间信息来了解输入图像的哪些部分对于分类决策是重要的。与 CAM 类似，Grad-CAM 使用 CNN 最后一个卷积层产生的特征图。Grad-CAM 的作者认为，“我们可以预期最后的卷积层在高级语义和详细的空间信息之间具有最佳的折衷。”

图 [12-10](#Fig10) 显示了与 Grad-CAM 相关的神经网络模型的部分。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig10_HTML.png](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig10_HTML.png)

图 12-10

Grad-CAM 流程图

CNN 由一些卷积层组成(在草图中显示为“conv”)。最终卷积层生成的特征图显示为 A1、A2 和 A3，与 CAM 草图中的相同。此时，对于 CAM，我们需要进行全局平均池化，然后是一个完全连接的层。对于 Grad-CAM，我们可以做任何事情——例如，多个全连接层，也就是草图中的“任何神经网络层”。唯一的要求是，我们在 A1、A2 和 A3 之后插入的层必须是可微分的，以获得梯度。最后，还有对*飞机*、*狗*、*猫*、*人*等等的分类输出。

CAM 和 Grad-CAM 之间的区别在于如何对特征图 A1、A2 和 A3 进行加权以生成最终热图。在 CAM 中，我们使用取自网络最后一个完全连接层的权重对这些特征地图进行加权。在 Grad-CAM 中，我们使用基于梯度计算的“alpha 值”对特征图进行加权。因此，Grad-CAM 不需要特定的架构，因为我们可以通过任何我们想要的神经网络层来计算梯度。Grad-CAM 中的 *Grad* 代表*渐变*。

Grad-CAM 的输出是一个*区分类别的定位图*，这是一个热点部分对应于特定类别的热图。Grad-CAM 使用任何目标概念的梯度(例如，*狗*的逻辑或甚至一个标题)，流入最终卷积层，以产生一个粗略的定位图，突出图像中的重要区域，用于预测概念。

使用 Grad-CAM，您可以直观地验证您的网络正在查看哪里，验证它确实在查看图像中的正确模式，并在这些模式周围激活。

如果网络在图像中的适当模式/对象周围*而不是*激活，那么你知道以下内容。

*   你的网络没有正确地学习你的数据集中的潜在模式。

*   你的训练程序需要重新审视。

*   您可能需要收集额外的数据。

*   最重要的是，*您的模型还没有准备好进行部署。*

Grad-CAM 是一个应该在任何深度学习实践者的工具箱中的工具。深度学习模型经常被批评为黑盒算法，我们不知道引擎盖下发生了什么。使用梯度相机(即 Grad-CAM)，深度学习实践者可以使用 Keras/TensorFlow 可视化 CNN 层激活热图。像这样的可视化可以让你看到“黑盒”在做什么。

要使用 Grad-CAM 实施，您需要用几个软件包配置您的系统，包括 TensorFlow(推荐 2.0)、OpenCV 或 Imutils。

现在我们来讨论 Grad-CAM 的实现。

首先，我们需要一个模型来运行向前传递。我们使用在 ImageNet 上预训练的 VGG16。您可以使用任何模型，因为 Grad-CAM 与 CAM 不同，不需要特定的架构，并且与任何卷积神经网络兼容。

```
model = VGG16(weights='imagenet')

```

定义模型后，我们加载一个样本图像(见图 [12-11](#Fig11) )并对其进行预处理，使其与模型兼容。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig11_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig11_HTML.jpg)

图 12-11

Grad-CAM 的输入图像

```
def preprocess(img):
    img = img_to_array(img)
    img = np.expand_dims(img,axis=0)
    img = preprocess_input(img)
    return img
image_1 = preprocess(image)

```

然后，我们使用该模型对样本图像进行预测，并解码前三个预测。正如您在下图中看到的，我们正在考虑模型中的前三个预测。顶级模特预测是给拳击手的。

```
predict = model.predict(image_1)
print(decode_predictions(predict,top=3))
target_class = np.argmax(predict[0])
print("Target Class = %d"%target_class)

```

![../images/511613_1_En_12_Chapter/511613_1_En_12_Figa_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Figa_HTML.jpg)

在下一步中，我们找到目标类得分 y <sub>c</sub> 相对于最后一个卷积层的特征图 A <sub>k</sub> 的梯度。直观地讲，它告诉我们每个渠道对目标阶层有多重要。grads 变量返回一个张量，用于以下步骤。

```
last_conv = model.get_layer('block5_conv3')
grads = K.gradients(model.output[:,242],last_conv.output)[0]

```

这样获得的梯度然后被全局平均汇集，以获得对应于目标类别的神经元重要权重 a <sub>k</sub> 。这将返回一个传递给 Keras 函数的张量，该函数将图像作为输入，并返回 pooled_grads 以及来自最后一个卷积层的激活贴图。

```
pooled_grads = K.mean(grads,axis=(0,1,2))
iterate = K.function([model.input],[pooled_grads,last_conv.output[0]])
pooled_grads_value,conv_layer_output = iterate([image_1])

```

之后，我们将每个激活图与相应的混合梯度相乘，该混合梯度作为权重来确定每个通道对于目标类有多重要。然后，我们取通道上所有激活图的平均值，得到的结果就是最终的类别区分显著图(见图 [12-12](#Fig12) )。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig12_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig12_HTML.jpg)

图 12-12

类别区分图

```
for i in range(512):
    conv_layer_output[:,:,i] *= pooled_grads_value[i]
heatmap = np.mean(conv_layer_output,axis=-1)

```

然后，我们在生成的热图上应用 ReLU，只保留对输出图有积极影响的要素。但是我们看到热图中没有太多的负强度，因此在应用 ReLU 后热图没有太大的变化(见图 [12-13](#Fig13) )。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig13_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig13_HTML.jpg)

图 12-13

ReLU 后的类判别图

```
for x in range(heatmap.shape[0]):
    for y in range(heatmap.shape[1]):
        heatmap[x,y] = np.max(heatmap[x,y],0)

```

然后，我们将热图的每个强度值除以最大强度值，以标准化热图，使所有值都落在 0 和 1 之间(参见图 [12-14](#Fig14) )。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig14_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig14_HTML.jpg)

图 12-14

归一化后的类别区分显著图

```
heatmap = np.maximum(heatmap,0)
heatmap /= np.max(heatmap)
plt.imshow(heatmap)

```

最后，我们对生成的热图进行上采样，以匹配输入图像的尺寸，并将其覆盖在输入图像上，以便查看结果，如图 [12-15](#Fig15) 所示。

![../images/511613_1_En_12_Chapter/511613_1_En_12_Fig15_HTML.jpg](../images/511613_1_En_12_Chapter/511613_1_En_12_Fig15_HTML.jpg)

图 12-15

以牛头犬为目标类别的最终显著图

```
upsample = resize(heatmap, (224,224),preserve_range=True)
plt.imshow(image)
plt.imshow(upsample,alpha=0.5)
plt.show()

```

## 摘要

在本章中，您学习了一种解释卷积神经网络的新技术，这是一种最先进的体系结构，尤其适用于与图像相关的任务。Grad-CAM 改进了其前身 CAM，并提供了更好的定位和清晰的类别区分显著图，这些图指导并揭示了类黑盒模型背后的复杂性。可解释机器学习的研究正在以更快的速度前进，并被证明在建立客户信任和改进模型方面至关重要。您还学习了如何使用 LIME 等基本技术解读图像。

下一章讨论文本解读方法。