# 5.模型可解释性中的人为因素

前几章讨论了可解释性方法的属性和评估标准；但是，请记住，解释是为人类设计的。因此，设计解释时人的因素非常重要。

解释系统及其实现的几个主要组成部分包括目标、策略和过程。这一章从人类的角度涵盖了所有这些。我们已经介绍了一些由可解释性的实际实践者引用的例子。 <sup>[1](#Fn1)</sup>

像任何需要影响业务的机器学习练习一样，可解释性也需要公司内不同角色的人之间的合作和头脑风暴。这建立了对可解释系统的信任。这种信任不仅限于人与模型之间的信任，定期的交互也建立了组织内部人与人之间的信任。

让我们从谈论可解释性角色开始。

## 可解释性角色

可解释性实践的涉众可以分为三个主要类别:技术专家构建者、领域知识评审者和模型消费者。

### 技术专家构建者

这些角色通常由组织内的数据科学家或 ML 工程师组成。这些人开发、设计或测试模型，并将系统集成到组织的数据基础设施中。

### 领域知识审阅者

评论者是一个有趣的分类。就重要性而言，这些人拥有领域知识，坐在技术专家构建者旁边。他们与专家一起工作，向他们提供关于模型的反馈，并确保模型符合预期的目标和行为。这些人可能有也可能没有关于 ML 的技术知识。评审者提供不同的观点来解决潜在的问题。审阅者类别可以非常广泛；因此，将它分解成更多的角色是有益的。

*   **领域专家**:领域专家对模型需要捕捉的真实世界现象有深入的了解。例如，医疗保健产品中的医生或信用评估模型中的分析师。

*   **产品经理**:产品经理负责组织需要的实际产品的开发。需求和业务目标是产品经理的主要沟通议程，它们也确保所有的目标都能及时实现。

*   **审核员**:审核员从法律角度审查模型。他们的目标是从法规遵从性的角度确保模型满足法律要求。

### 利益相关者或最终用户

涉众是模型所采用的信息和决策的最终用户。这个小组由像这样的个人组成

*   研究死亡率和再入院模型的医生

*   银行代表，他们使用贷款批准模型来决定是否批准贷款

*   必须预测机器何时会出现故障或输出模式会是什么样的工程师

*   一个想了解药物作用的生物学家

这些人可能属于也可能不属于技术专家或评审人员所属的公司。这取决于组织的商业模式。对于向其他组织提供 ML 服务的服务公司，利益相关者通常属于客户组织。对于做内部 ML 咨询的公司，利益相关者属于同一组织内的不同团队。

关于这些角色，需要记住的一个重要方面是，它们不一定定义一个人。一个人可以涵盖多个角色，一个角色可能对一个人或整个团队有效。

## 可解释性阶段

在模型和产品开发的不同阶段，可解释性扮演着重要的角色。

让我们从人类的角度来讨论各个阶段，以及可解释性如何适应这些阶段。

### 构思和概念化阶段

甚至在实际的模型开发出来之前，可解释性就扮演了重要的角色。在建立模型之前，个人花费大量时间来探索策略和度量标准。

技术专家可能会让领域评审人员参与进来，帮助他们在特性集的设计阶段选择从领域知识的角度来看有意义的特性。例如，财务专业人员知道在定义信用评估模型的特征时，尽早拥有业务分析师的观点的重要性。

类似地，一些技术专家和审计员在过程的早期阶段一起工作(特别是在严格监管的行业),以定义一个框架来解释每个功能，它是如何设计的，以及为什么使用它不会有违反法律的风险。一家大银行的数据工程师解释说，与法规遵从性团队一起设计功能可以更容易地在验证和确认阶段赢得他们的信任。

### 构建和验证阶段

在这个阶段，技术专家构建并验证模型。他们部署各种可解释性技术来理解为什么模型会给出特定的输出。这是模型可解释性分类法中使用量最大的地方。技术专家选择一种可解释的方法来解释特定的观察结果或整体模型行为。此外，在构建模型和验证步骤时，选择特定于模型或与模型无关的方法。

作为对这一活动的补充，技术专家识别边缘案例，通常称为*异常*或*角落案例*——即模型特别不确定或可能出现意外行为的案例。即使范围和完整性有限，围绕案例的推理也是专家解释和测试黑盒模型的方式。

此外，专家还研究了模型中使用的特征，作为可解释性透镜的一部分，以更好地理解模型。专家们主要通过研究什么特征驱动大多数决策来考察一个模型的可持续性和似是而非程度。这通常是通过按重要性排列特性来完成的。在这种情况下，检查不太重要的功能与检查更重要的功能一样有益。

关于特性重要性的信息通常与其他涉众共享，并帮助他们推断模型的行为。技术专家还强调了这样一个事实，即通过可解释性来比较多个模型变得非常容易。可解释性适合于理解一个单一的模型，并且可以开发一组可以比较和改进的模型。虽然，对于许多用户来说，模型可解释性是首选的模型比较方法，但没有这样做的既定方法。

在这个阶段，有两个可解释性需求。

*   **获得自信**。技术专家需要对他们建立的模型的可靠性和有效性有信心。

*   **获得利益相关者的信任**。当模型是黑盒，并且逻辑过于复杂时，模型涉众有时会犹豫是否使用模型进行最终决策。他们很难建立信任。这个问题在高风险环境中非常严重，在这种环境中，即使是一个小小的错误都会造成代价高昂的后果。

一位致力于重症监护室(ICU)模型的高级数据科学家评论道:“我不认为我们可以提供一个黑盒模型。我不认为那会像人们说的那样管用。如果你不向他们(模型消费者)解释，我认为他们永远不会想使用它。”

除了模型分析和验证需求之外，模型技术专家还非常需要与其他风险承担者交流模型的行为方式、信任程度以及在什么情况下可能会失败。沟通的质量很重要；它鼓励可以用来改进模型和获得组织信任的洞察力。不同涉众之间的交流通常是迭代的，并且可能激发进一步的建模，以获取关于模型如何运行的洞察力。

模型技术专家描述的最强烈的需求之一是设计方法和工具来与模型评审者互动和交流。让评审人员使用他们能理解的语言和表述参与验证过程是一个主要的挑战。

许多人也相信“知识的诅咒”模型技术专家通常很难识别其他涉众不知道的东西。即使他们掌握了利益相关者的知识水平，决定如何传递洞察力似乎也很困难，即使是可视化。

### 部署、维护和使用阶段

一旦技术专家在多次审查和增强之后最终确定了模型，就可以通过将模型集成到实际的数据馈送中来部署模型。在集成阶段，可解释性问题不仅限于理解模型行为，还扩展到围绕模型的整个基础设施。

围绕可解释性的目标可能会有一点混乱，即它是为了理解模型如何工作还是整个模型管道如何工作。但是，模型存在的生态系统和模型本身一样重要。在这些场景中，由于模型的黑盒性质和作为软件一部分的复杂管道，可能会出现可解释性问题。模型框架中信息流的复杂性也会导致可解释性问题。

在 ML 系统的生命周期中，可解释性的挑战在模型投入生产后出现。一些相关的问题是破译如何检查实例/预测模式，不反映最先进的行为，并检查为什么模式作出了不可接受的错误，同时支持高风险的决策。负责监控模型的团队会发现这些问题。一旦错误被识别出来，我们就需要一个可解释性的方法来确定错误的原因。这种类型的“根本原因分析”尤其具有挑战性。

数据科学家通常需要构建一个更好的 ML 模型版本，因为从以前版本中学习的新版本可能会提高性能。在这种情况下，模型比较成为可解释性的一部分。在复杂系统中部署多个模型的场景中，不同模型之间的比较通常需要工具的支持。由于模型变化会影响可解释性，模型技术专家需要密切关注模型的参数。有时，专家保持模型输入不变，以了解变化对不同模型的可解释性的影响。

在两种主要情况下，解释是至关重要的。在高风险的情况下，解释起着非常重要的作用。例如，当医生使用模型来支持关于患者的决策时，他们需要理解模型如何“推理”以使其与自己的知识相一致。

该公司的高级数据科学家表示，有时仅仅解释预测是不够的，医生或内科医生倾向于了解可以操纵哪些关于患者的信息来使模型表现不同。当模型的用户同意模型的决策时，这些解释可以用作支持决策的证据，或者他们可以在模型与人类的预测想法不一致的情况下找到新的见解。在人类不同意模型预测的情况下，可解释性起着非常重要的作用。ML 模型背后的整个想法是，模型应该帮助人类变得更加智能和高效。但是，当突然之间，比人类所能做出的更好的预测无法向人类解释时，人类就需要学习这些场景，进而改进他们自己的预测或知识。有时候，当一个模型的决策与现实生活中的预期目标相违背时，解释就变得至关重要了。

这种场景的一个很好的例子是银行使用模型来支持代表与客户会面。

一家银行负责商业信用评估的人工智能主管提到了两种主要情况，其中解释在组织中至关重要。首先，当客户的贷款被拒绝时，代表需要说明理由(解释的权利是许多国家法律的一部分)。第二，代表们对发放更多贷款有强烈的兴趣；因此，他们总是在寻找他们可以采取行动的解释，比如那些建议如何将拒绝的贷款变成接受的贷款的解释。

## 可解释性目标

上一节提到了不同的涉众在模型开发的不同阶段可能具有的特定目标。本节总结了我们遇到的将目标更明确地连接到角色和过程的一组目标。我们确定了三大类可解释性目标。

### 模型验证和改进的可解释性

模型开发过程中最重要的活动之一是识别模型的问题，并找到解决方案来修复它们。模型技术专家和评审人员应该在模型开发过程中实现这个目标。然而，他们有时也需要投入来有效地实现这一目标。例如，审计员和法律团队可以帮助专家和评审员在解决任何问题时确定一套严格的法律要求。验证模型的方法之一是检查模型输出的汇总统计数据；然而，我们需要可解释性工具和方法。

有时，模型可以高精度地学习输入和输出之间的非本质和无意义的关系。这可能需要领域专家的参与来正确解释结果。识别不相关的相关性和因果关系的错误含义可能需要了解模型及其机制的专家的参与。这个过程的特点是将模型行为与涉众对数据和模型预测的实际意义的理解进行对比。

从法律角度来看，透明度是必要的，以确保模型不违反法律法规，如公平交易法或一般数据保护条例，旨在防止歧视性做法。当一个模型出错时，解释为什么做出一个给定的决定是至关重要的。遵从性是需要可解释性的主要原因之一。

### 决策和知识发现的可解释性

可解释性背后的整个想法是帮助决策。对于高风险问题，决策者不能相信或使用不为其建议提供理由的预测。类似地，解释需要符合专家的需求和领域的限制。围绕可解释性的一个重要主题是可操作性。

因此，到目前为止，可操作性是可解释系统最重要的新兴需求。每一种可解释性方法都有一定的解释；然而，并不是所有的解释都是可行的。一些非常常见的方法给出了很好的解释，可能只会增加关于模型及其行为的知识。然而，可解释性的一个主要优势应该是它提供行动或建议的能力。模型专家需要多次迭代才能得出可操作的特性。这有时也需要与评审者和涉众进行多次讨论。

工具解释其决策的潜力是学习或知识发现的催化剂。例如，在决策支持环境中，当决策者和模型不一致时，模型的可解释性变得至关重要。在这些情况下，学习是可解释性的一个重要结果。如果不解释为什么模型会做出预测，人类就无法重新考虑决策或从模型中学习。在有些情况下，模型不一定是为决策或自动化而构建的，而主要是作为一种工具来生成对数据所描述的现象的见解，并在未来理解 KPI(关键绩效指标)。目标是了解哪些特性驱动性能，而不是预测未来的性能。

根据过去使用的系统功能，用户保留模型可以显示用户再次使用该服务的可能性。

### 获得信心和信任的可解释性

每个模型专家都同意需要获得对模型的信任。调试对于获得模型正常工作的信心是必不可少的。这种个人层面的信任是迄今为止 ML 可解释性研究的焦点。即使在客户或利益相关方不要求透明的情况下，数据科学家也应该彻底调查他们的模型，以建立理解和信任，这是必要的尽职调查的一部分。然而，说服他人一个模型可以被完全信任通常在可解释性工作中优先考虑。缺乏对模型构建企业的信任，或者对开发这些模型的团队的工作的信任，可能会阻碍在组织中使用 ML 的进展。因此，模型技术专家需要获得对他们的模型和过程的信任，并且可能依靠可解释性来说服组织中不同角色的其他人，他们的工作提供了价值。

能够简明有效地解释一个模型是如何工作的以及为什么它应该被信任是不容易的，特别是因为一些涉众可能需要在很高的抽象层次和很少的技术细节上理解这一点。可视化方法可以在可解释性的交流阶段发挥重要作用。模型专家将建立信任作为建模目标的高度优先性意味着他们认为需要建立透明的模型或使用模型提取方法向他人解释给定模型是如何工作的。

## 以可解释性工作为特征的人性化主题

关于产生新工具或见解的可解释性的研究主要集中在个体和模型之间的交互上。可解释性被描述为模型类的属性，基于其与人类期望或约束的关系，例如单调性或可加性，或者来自领域知识的人类驱动的约束，或者使人类能够理解为什么标签被应用于实例(例如，莱姆或 SHAP)的特定模型结果的呈现。

我们总结了四个不同于前几章提到的技术特征的主题。这些特征更加以人类为中心，并讨论了人类如何与可解释性一起工作，以使模型更加有用。

### 可解释性是合作的

组织中的可解释性意味着涉众角色之间的价值和知识的协作和协调。这个主题主要是通过提到与其他涉众团体的讨论来定义的，比如领域专家。这些讨论经常发生在构思、模型构建和验证期间，但也发生在部署之后。围绕可解释性的协作对于改进业务推理和使涉众相信已经建立的模型将带来价值是很重要的。

可解释性在建立和维护组织中人与人之间的信任方面的作用是另一个清楚的方式，它的社会性质是显而易见的。

人们对用例及轶事的反应比他们对数学的反应更好。因此，在与客户沟通时，应采纳证据和可解释性需求。通常是解释系统的操作，然后给出一些答案。

当公共关系、技术转移或其他远离模型开发的团队通过可解释性获得信任时，模型上的限制或约束往往更容易被模型接受。

内部信任将团队从他们可以使用的模型类型或所需的监控级别的约束中解放出来，因此模型可解释性被视为获得某种状态的手段。

有时候，可解释性的重要性并不在于它可以解释任何决策，而是仅仅包括它的行为表明了其他利益相关者或最终用户感到欣慰的“尽职调查”。

### 可解释性是一个过程

现有的可解释性的特征，例如测量人类心理模型和 ML 模型之间的不匹配，很少评论可解释性及时发生的可能性。可解释性是一个持久的问题。虽然可解释性可能在不同阶段表现不同(例如，在管道早期的功能选择与部署后对现有决策管道的模型结果的识别)，但它自然与一组不同的实践和意图相关联，这些实践和意图可能会发展或同时发生，但都有助于理解可解释性的含义。

可解释性可以促进人类用户和模型之间的“对话”,这被认为是在模型的生命周期中为了学习和继续使用而必须保持的。

### 可解释性是一种心理模型比较

可解释性被认为在组织中带来的学习和意义形成的类型通常通过不同利益相关者持有的心智模型之间的比较而发生。这与可解释性定义形成对比，可解释性定义将可解释性框架为可由个体心智模型和 ML 模型之间的相互作用来确定。虽然在许多参与者关于可解释性的描述中隐含了人工智能模型与人类的比较，但人工智能专家自己对问题的理解对他们如何追求可解释性有中介影响是显而易见的。

理解终端用户的心智模型以指导他们的工作需要付出相当大的努力。反思人类决策的各个方面可以提炼并正确看待可解释性目标。

### 可解释性依赖于上下文

团队构建的可解释性解决方案强烈地受到他们所针对的用户组的需求和用例的影响。可解释性工作的影响与复杂的决策基础设施密切相关。

## 可解释性挑战的设计机会

本节讨论了从人的角度实现可解释性时遇到的挑战。

### 识别、表示和整合人类期望

为了识别和整合人类对可解释性的期望，理解和识别边缘案例是非常重要的。这种边缘案例识别可以由使用该模型的一组人或者由个人来进行

以下是在模型构建练习中集成可解释性的一些好处。

*   它帮助人们在可解释性任务中用预测来表达他们的期望。

*   它帮助人们有效地认识到他们对系统的期望和实际表现之间的差距。

*   它帮助人们根据他们的发现调试/改变模型行为。

数据科学家经常参与这样的协作工作。获取利益相关者对其他不同类型模型的响应是模型开发和构建阶段的一个重要组成部分。

一个有益的方法是开发交互式应用程序，涉众或最终用户可以使用它来有效地提供关于模型使用的反馈。研究人员已经开发了平台，以发现预测模型所犯的错误，从而收集一群人的反馈。交互式应用程序和方法可以使领域专家和 ML 专家参与模型并分享关于决策过程的知识。

### 交流和总结模型行为

由于模型在构建阶段是合作性和社会性的，大量的知识转移发生在模型移交阶段。为了识别部署后发生的模型错误，拥有能够捕捉模型和接收模型的人之间的关系的可视化是非常有益的。简而言之，一个好的模型行为的总结为涉众和最终用户理解模型的工作提供了很大的帮助，并使开发人员能够快速解决模型错误。

这些可以与识别边缘情况的算法一起开发。

一个重要的关注点是改进输出与人类行为或感知不一致的方法。

### 可扩展和可集成的可解释性工具

对于许多可解释性方法来说，关于可解释性工具集成的最佳实践或指南并不存在，因为它们是在学术环境中开发的，行业需要赶上可解释性用于生产级模型工作的趋势。

工具很难与现有平台集成，因为它们不能适应特定的环境。在某些情况下，问题是可解释性工具不能适应数据大小。

另一个挑战是可解释性可能是比较模型的一个很好的工具；然而，没有多少方法提供模型比较的见解。模型比较的例子包括比较参数、选择特征、时间戳等等。

### 部署后支持

在培训后阶段，一旦部署了模型，就需要更好的工具来监控它们。

可视化工具是监控模型行为的好方法。除了识别潜在的令人烦恼的行为之外，自动化异常检测可能是有用的，给定应该保持的关键期望的适当规范，例如从领域专家那里学到的规范。

## 摘要

本章描述了不同的可解释性角色。我们描述了每个角色如何在建模生态系统中工作，并对可解释性做出贡献。接下来，我们描述了可解释性或可解释性练习的各个阶段。我们谈到了每个角色是如何与构建和应用可解释方法的不同阶段相联系的。最后，我们回顾了可解释性方法的不同目标。我们讨论了如何将这些目标与方法开发和应用的角色和阶段结合起来。本章最后讨论了各种方法主题，以及如何使用以人为中心的方法来定义它们。

下一章描述了如何基于几个属性和因素来评估各种可解释性方法。第 6 章为理解为你的建模任务建立或选择一个可解释性或可解释性方法的基本要求奠定了基础。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

请参考纽约大学的 Sungsoo Ray Hong、Enrico Bertini 和西北大学的杰西卡·胡尔曼撰写的“模型可解释性中的人为因素:行业实践、挑战和需求”。

 </aside>