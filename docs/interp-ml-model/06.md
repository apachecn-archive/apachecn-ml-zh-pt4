# 六、可解释事实：系统评估可解释方法的框架

在前面的章节中，你学习了解释的属性和人类对解释方法的理解。从这一章开始，我们慢慢过渡到可解释的方法和解释的框架。可解释的系统可以从五个关键维度进行评估。

*   功能的

*   操作的

*   可用性

*   安全

*   确认

使用本章的知识，你可以比较各种可解释性方法，理解它们的能力，并确定它们的差异、理论品质和实现属性。本章介绍了一个基于功能和操作需求评估方法的框架。我们认为理解设计可解释性方法框架的属性和过程对你在本书后面彻底理解每个方法是非常重要的。每一个为预测系统设计的可解释方法都应该伴随着一个评估其功能和操作需求的框架。解释的质量应该基于可用性标准来评估，以便从用户的角度来理解它。在设计方法之前，还应该考虑它们的安全性、隐私性和脆弱性。因此，跨越所有五个维度的可解释性属性的标准化列表便于对不同的可解释性方法进行评估和比较。

## 可解释事实列表维度

可解释性的事实列表应该在多个维度上评估可解释的系统。

功能要求考虑算法参数，例如

*   解释适用的问题类型

*   解释是为数据、模型还是预测而设计的

*   哪种技术使用了解释(即本地与全局)

*   与预测系统的关系(即，事后或内在)。

这在第 [3](03.html) 章中有所涉及。

操作需求包括与最终用户的交互，准确性与可解释性的权衡，以及用户背景知识，以充分利用可解释能力。

可用性需求包括列出解释的属性，使其更加人性化和自然，从而使输出易于理解。其中一些属性是我们在第 4 章中讨论过的。

可解释方法的健壮性和安全性是两个安全需求。安全需求包括可解释方法的健壮性和安全性。例如，它包括有多少信息被泄露给底层模型的可解释性方法。此外，通过使用相同的方法，它检查来自单个数据点的解释在不同的模型中是否是恒定的。

现在让我们详细讨论一下这些要求。

## 功能需求

设计可解释方法的功能需求与第 3 章中讨论的可解释方法的分类密切相关。数据科学家可以使用功能需求列表(F1–F9)来确定和部署最适合特定用例的算法。这些将在下面的章节中讨论。

### F1:问题监督级别

可解释方法可能与监督问题相关，并适用于非监督学习方法和一些半监督学习方法。

### F2:问题类型

机器学习中有不同类型的问题，可以为这些问题设计不同的解释。例如，我们有一个分类(二进制、多类、多标签等。)、回归、聚类、排名等推荐算法以及协同过滤。一旦你基于提到的方法或者其他方法定义和识别了一个问题，你就可以很容易地识别出适合需求的可解释方法。

### F3:解释目标

任何机器学习流水线都有三个主要组成部分:数据点、模型和预测。

解释数据点不属于机器学习解释或可解释性的范畴，因为还有其他方法，如汇总统计、类别比率和特征相关性。可解释性方法的目标通常是潜在的模型及其预测。通过理解模型的行为和预测，我们可以很容易地决定选择哪种解释方法。对模型的解释是基于它的一般功能和概念行为。

### F4:解释广度/范围

范围是挑选或设计任何可解释模型的主要评估标准之一。该范围类似于第 3 章中讨论的分类范围。根据需求，您可以将范围定义为局部(任何单个解释)或全局(全部数据或数据中的一组解释)。

### F5:计算复杂性

另一个重要的评估标准是关于解释算法中模块的计算复杂度。复杂性可以通过一些指标来衡量，比如生成结果的时间和计算解释所需的内存。这通常应该与提到的解释范围相结合。设计者应该考虑随着范围从局部到全局的增加，时间和计算资源是否呈指数增长。应该建立各种优化框架来计算优化时间和资源的解释。

### F6:适用的模型等级

模型类评估对于设计可解释算法非常重要。这也和我们在第 [3](03.html) 章讨论的范围密切相关。模型类意味着确定这些方法是应用于特定的模型还是适用于所有的模型(即，模型类特定的方法还是模型不可知的方法)。

### F7:与预测系统的关系

在可解释的系统和建模流水线之间存在两个主要的关系。可解释的系统可以是事后的。它们被设计成在现有模型之上工作，并利用模型文件或预测模型函数；例如，石灰或 SHAP 技术将在接下来的章节中讨论。在固有技术中，模型本身用于预测和解释(例如，用其特征权重解释线性回归)。事后方法的例子是局部代理解释器。

另一个额外的关系可以是代理系统的形式，它使用一个简单的模型来模拟复杂模型的模式。代理需要仔细设计，以避免引入与原始模型相同的复杂程度。

### F8:兼容的要素类型

设计每种方法时，都应该牢记不同类型特性的指导原则。兼容的要素类型可能是分类的、数字的或顺序的。以优化为基础的算法不能很好地处理分类特征。一些选定的模型实现需要对分类特征进行预处理，例如，一键编码，从而使它们与一些可解释性方法不兼容。每种方法都应该描述特征类型，以判断它们是否属于特定的层次结构，以及它们如何对模型有所帮助。

### F9:警告和假设

最后，任何不属于前面提到的范畴的可解释性方面都应该包含在一个单独的函数类中。特征独立性假设、相关特征对解释质量的影响或解释权重时的简单要求(如特征标准化)应适当地划分成类别。

## 操作要求

操作要求(O1–O9)是功能要求之后需要考虑的另一个重要方面，将在以下章节中讨论。这些描述了可解释系统如何与用户交互，以及期望什么。从部署和自动化的角度来看，这些都很重要。

### O1:解释系列

解释族是根据操作需求对解释方法进行的第一个评估。在评估解释方法时，我们应该考虑它们所属的家族，可以有多种类型，如下所述。

*   解释可以属于模型内部之间的关系，或者特征之间的关系，特征和预测器之间的关系；例如，一些解释可以基于各种特征的特征重要性。

*   解释也可以属于一个对比和差异的家族(用例子)；例如，原型和批评(相似和相异)和类对比反事实陈述。

*   解释可以属于代表完全因果模型的类型。

### O2:解释性介质

不同的解释系统使用不同的媒介来表达他们试图解释的模型的工作原理。解释系统表达结果最常见的媒介是可视化，以文本的形式或者两者的混合。一些方法也可能以系数、汇总统计和类似 PDP(部分相关图(见第 [9](09.html) )和 ICE(个人条件期望(见第 [9](09.html) )的图)的形式表示模型的汇总。在文本形式中，解释有一个变量和一个简单的文本来解释变量在模型上下文中的含义。一个有明确框架的解释系统可以给听者提供一个反驳的机会，或者在完全了解的情况下接受它。

有时，情节和自然语言表达的混合可能是必要的，因为可视化被限制在三维空间，所以包含文本可以防止解释系统受到维度的诅咒。

### O3:系统交互

系统交互在决定用户如何与可解释的系统交互方面起着重要的作用。通信框架可以是动态的或静态的(即，有或没有反馈)。在静态系统交互中，可解释的方法根据技术专家预先定义的协议给出输出。在这种情况下，输出可能不总是满足用户的期望。一个例子可以总是在用户可能对不同的特征集感兴趣时输出最重要的特征。另一种类型的系统交互可以是动态交互。这使得用户能够探索关于特定解释的所有细节。这种系统交互的例子是 web 界面。这种系统的创建者应该清楚地指出反馈如何影响可解释的系统。

### O4:解释领域

解释领域可以与模型领域非常不同。例如，模型可以将图像数据作为输入；然而，解释可以是放置在图像之上的文本而不是显著性图。类似地，该模型可以是表格数据模型；然而，解释系统输出可能是基于规则的文本段落。评估任何方法解释领域都是非常重要的，因为它对于定义模型构建者和涉众之间的一致性是至关重要的。

### O5:数据和模型透明度

对可解释方法的操作评估的另一个要求是潜在的透明性要求。有时，进入模型的数据会被转换。此外，在使用解释系统之前，用户需要评估他们对模型内部工作原理的了解。用户还应该检查这些特性是否是人类可以理解的。例如，在预测室温时，可以在输出中包含一个简单的室温变量，也可以是室温和房间高度的平方和。当输入不容易理解时，系统设计者可能希望提供一个数据转换列表。

### O6:解释受众

任何解释方法的受众可能不同，从领域专家到不了解模型和可解释性领域的人。领域专家中有两种类型的受众:一种具有 AI/ML 知识，另一种仅具有业务领域知识。因此，对理解方法的受众的背景知识的水平和类型保持警惕变得非常重要。此外，功能(或模型)的透明性应该根据目标受众来考虑。例如，考虑一个使用自然语言语句解释其预测的系统。给定接收者的语言技能，系统可以使用不同语法和词汇复杂性的句子来促进更容易的理解；例如，向医生解释疾病诊断，而不是向病人或他们的家属解释。

### O7:解释的功能

对可解释方法进行适当评估的一个操作要求是，每种方法都应该附有一个其实际应用的列表。

大多数方法都是为透明性而设计的(即向最终用户解释 ML 流水线的组件)。不同的组件可以是这种方法是支持系统还是比较模型。向用户提供经过验证的部署上下文是很重要的，以防止其误用，从而在高风险应用程序或自治系统中部署时导致意外伤害。

### O8:因果关系与可诉性

解释通常不具有因果性。这个属性需要传达给用户，这样他们就不会得出错误的结论。对于可操作的而非因果的解释，如果用户倾向于假设洞察是因果的，这可能会导致非常不准确的洞察，就像 SHAP 等流行的方法一样。同样，从完全因果模型中得出的解释应该被宣传为因果的，并充分发挥其潜力。

### O9:信任与绩效

可解释性方法的一个重要评估标准围绕着性能。详细讨论性能与可解释性的权衡非常重要。可解释性的主要工作是建立用户对预测系统的信任。然而，有时为了使模型更容易解释，性能会下降。用户需要选择不会大幅降低准确性或其他性能相关指标的方法。

## 可用性要求

在这里，我们讨论从听者的角度来看很重要的解释的属性。其中许多都基于社会科学研究；因此，只要适用，让算法解释对最终用户来说感觉更自然，不管他们的背景知识和以前对这种类型的系统或技术的经验。此外，我们可能会发现解释的可用性方面和第 4 章中讨论的解释的属性之间有很多相似之处。

### U1:稳健

这个属性衡量一个解释的真实性。如果解释是内在的，则此属性无效，因为预测和解释是从同一个模型中派生的。在即席解释中，此属性度量由于解释方法而引入的错误量。

这可以通过计算预测和解释模型的输出之间的选定性能度量来完成。任何选定指标的高值意味着解释与潜在的预测一致。

### U2:完整性

解释应该远远超出某一点。完整性衡量解释覆盖潜在预测模型的程度。通常通过测量多个数据点解释的正确性来检查。

### U3:上下文完整性

解释应附带所有必要的条件，以便用户正确理解限制。Contextfullness 使局部解释显式地为局部的，或者指示它可以被视为全局的，尽管它是为单个预测导出的。这个特性的一个特例叫做*代表性*，它测量一个解释在一个数据集中包含的实例数量。语境的另一个方面是对解释有贡献的每个因素的重要程度。例如，如果三个原因支持一个解释，那么它们各自的重要性如何。

### U4:互动性

考虑到受众广泛的经验和背景知识，一种解释无法满足广泛的期望。因此，为了提高整体用户体验，解释过程应该是可控的。例如，它应该是可逆的(以防用户输入错误的答案)，尊重用户的偏好和反馈，是社会性的(双向沟通比单向信息卸载更可取)，允许调整解释的粒度，并且是交互式的。这意味着，只要有可能，用户应该能够定制他们得到的解释，以满足他们的需求。例如，如果系统用反事实的陈述来解释它的决定，并且在这样的陈述中使用的箔片不包含用户感兴趣的信息，他们应该能够请求包含该箔片的解释(如果存在的话)。

### U5:可操作性

可操作性是解释属性中一个非常重要的概念。所有的解释都应该以一种用户能够理解决策背后的原因的方式来准备。此外，输出应该是这样的，以便可以根据解释采取一些可取的行动。例如，就银行贷款问题的反事实解释而言，有效贷款的数量比客户的年龄更有意义。

### U6:新奇

应该避免向用户提供平凡的或预期的解释。解释应包含发生概率低的令人惊讶或异常的特征(例如，罕见的特征值)，以将用户的注意力引向感兴趣的领域。然而，这一目标需要平衡与听者心智模式的一致性、新颖性和整体合理性之间的权衡。例如，考虑一个 ML 系统，其中的解释有助于更好地理解给定的现象。在这种情况下，应该避免向用户提供强调他们已经知道的关系的解释

### U7:复杂性

解释的复杂性应该根据使用解释的用户来调整。假设系统不允许用户调整解释复杂度。在这种情况下，默认情况下应该尽可能简单(除非听者明确要求一个更复杂的)。例如，自动医疗诊断解释应该使用可观察到的症状，而不是导致疾病的潜在生物过程。

### U8:个性化

调整对用户的解释需要解释能力技术来模拟他们的背景知识和心智模型。这在调整解释的复杂性、新颖性和连贯性时尤为重要。可以通过交互在线个性化解释，或者通过将必要的信息并入模型(例如，参数化)或数据离线个性化解释。个性化的解释与合作交流的另一个规则有关:关系准则。根据这一规则，在任何给定的时间点，通信只应传递相关和必要的信息。因此，一个可解释的系统必须“知道”用户所知道和期望的，以确定解释的内容。

## 安全要求

可解释性方法揭示了用于训练预测模型的数据集的部分信息，模型的内部机制或参数，以及它们的预测边界(参见以下章节中的 S1-S3 安全要求)。因此，我们应该考虑可解释性对预测系统的健壮性、安全性和隐私性方面的影响，它们是建立在解释本身的健壮性之上的。

### S1:信息泄露

每一种可解释的方法都应该伴随着对其隐私和安全含义的批判性评估，以及关于减轻这些因素的讨论。重要的是要考虑解释揭示了多少关于基础模型及其定型数据的信息。例如，考虑应用于逻辑机器学习模型的反事实解释；鉴于该模型系列对数据特征应用精确的阈值，这种类型的解释很可能会泄漏它们。另一方面，对 k-最近邻模型的解释可以揭示训练数据点，并且对于支持向量机，这些可以是支持向量上的数据点。

### S2:解释误用

考虑到信息泄漏，人们可以问需要多少解释和不同的数据点才能收集足够的见解来窃取或利用潜在的预测模型。这可能是一个主要问题，尤其是如果预测模型是商业秘密的话。此外，对手可以利用解释来玩弄模型；考虑这样一种情况，恶意用户可以通过检查模型的解释来发现模型中的错误，因此现在能够利用它。这一观察表明了解释和敌对攻击之间的密切关系。

### S3:解释不变性

给定由预测系统建模的现象，我们收集的数据是量化其观察到的效果的一种方式。预测系统的目标应该是引出对潜在现象的洞察，而解释是在人类可理解的背景下促进理解的媒介。理想情况下，解释应该基于潜在现象的属性，而不是依赖于预测模型的人为因素。给定相同的输入(模型和/或数据点)，可解释性方法应该提供相同的解释。这可以通过调查一个可解释性算法的多次执行的解释的方差来衡量。理想情况下，一种方法产生的解释应该与使用另一种可解释技术产生的解释相当(给定固定的训练数据)。

## 验证要求

最后，可解释的系统应该通过*用户研究*或*合成实验*在类似于预期部署场景的环境中进行验证。一个设计良好的用户研究也可以为前面章节中列出的一些(定性的)解释属性提供一个清晰的答案。例如，它可以评估某个解释对特定受众的有效性，评估从某个解释中受益所需的背景知识，或者检查使用该解释所需的技术技能水平，因为不是每个人都会对某个特定的解释媒介感到舒适。标准化的用户研究并不是灵丹妙药。然而，应该有一个协议(如医学科学中的随机对照试验),因为它们是验证新方法解释能力的最可接受的方法。

提出了一套不同的、主要是合成的验证方法，使用具有已知特征的模拟数据来验证解释的正确性，并测试解释的稳定性和一致性

*   **模拟能力**衡量人类根据系统提供的解释重新创建或重复(模拟)计算过程的能力。

*   **算法透明度**衡量人类能够完全理解一个预测算法的程度:它的训练过程，它的参数来源，以及管理它的预测的过程。

*   **可分解性**量化收听者理解预测模型的各个部分(及其功能)的能力:理解输入的特征、模型的参数(例如，特征之一的单调关系)以及模型的输出。

为一个预测系统开发一个可解释的方法，并基于属性列表对其进行评估时，会出现多个问题。它们都同等重要吗？它们是相互兼容的，还是有些不一致？在实际应用中，很多是不能同时实现的，其重要性往往取决于应用领域。此外，虽然选择的可解释性方法类别(例如，反事实)在理论上可能足够灵活以符合大多数属性，但是由于算法选择，它们中的一些可能在实现中丢失。

虽然功能和操作需求是特定解释方法及其实现的属性，但可用性需求是一般属性。任何方法都应该以满足所有这些需求为目标。例如，使可解释性技术与模型无关迫使它成为一种事后(或模仿)的方法，并阻止它利用特定模型实现的细节。此外，这种方法在预测模型的基础上又增加了一层复杂性(相对于事前技术而言),这对解释的保真度是有害的:在完整性和可靠性之间进行权衡，这在模型不可知的方法中很常见。更简单的解释——也就是说，原因更少，更普遍，更连贯——通常对人类更有吸引力。然而，根据应用程序和目标受众的不同，这可能并不总是理想的。

这些属性中的一些可以用来实现不止一个目标。例如，通过上下文解释可以部分实现完整性。如果一个可解释性系统本质上不是交互式的，那么这个需求可以通过将它部署在一个交互式平台中来实现，比如一个用于自然语言解释的对话系统或者一个用于可视化的交互式网页。可操作性通常是特定于数据集的，可以通过手动注释可操作的特征并对时间敏感的特征进行排序来实现。

## 摘要

这结束了基于多重标准的可解释性方法的评估这一章。当您进一步了解不同的方法时，这些信息会很有用。从第 [8](08.html) 章开始，您可以将这些属性和评估标准映射到不同的方法。这是一个很好的练习，看看哪些方法满足一些或所有的属性，并根据它们的用例评估不同的方法。