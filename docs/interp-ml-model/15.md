# 15.可解释方法的八个陷阱

与模型无关的模型解释技术，如置换重要性图、置换特征重要性和 Shapley 值，为模型解释提供了许多见解，但如果使用不当，往往会导致错误的结论。

本章强调了模型解释的许多常见缺陷，例如在错误的上下文中使用解释技术，或者使用不能很好概括的模型。有时人们会忽略高维数据集中的特征依赖、交互和不确定性估计。这一章集中在使用可解释性技术时可能出现的常见陷阱。

由于许多可解释性技术都是基于操纵数据和发现模型的相同原理工作的，所以它们也有许多共同的缺点。大多数模型通常具有非线性效应和高阶相互作用。不管模型在测试数据上的表现如何，都可以应用解释方法，但是只有当数据能够很好地概括时，我们才能得出可靠的结论。在某些情况下，一个简单的模型足以完成期望的任务，因此复杂的模型和模型可解释性技术可能会增加不必要的复杂性。像这样的缺点很容易被忽略，但它们在许多机器学习练习中相当频繁地存在。

## 假设一劳永逸的可解释性

有很多可解释的方法，从业者可以选择来回答模型为什么做出某个决定的问题。太多的选项使得实践者很难为他们的需求选择一种方法。如果一个单一的可解释性方法可以适用于多个用例，这是一个非常危险的假设。例如，有许多全局特征重要性方法，如排列特征重要性和 SHAP。两者提供的输出可能完全不同，它们的使用目标也可能不同。例如，如果模型构建者希望深入了解与模型概化误差相关的要素相关性(例如，数据科学家选择模型要素)，则应使用基于损失的方法，如置换要素重要性。当模型构建者希望展示模型预测所依赖的特征时，则应使用 Shap 特征重要性(例如，业务分析师分析模型以呈现给利益相关者)。因此，模型构建者必须调查他们对可解释方法的选择，以符合他们的目标，并产生有意义的分析。

## 模型泛化不良

当模型欠拟合或过拟合时，可能会导致误导的特征重要性分数，因为输出与基础模型不够匹配。

大多数可解释性方法被设计用来解释模型，而不是创建关于数据生成过程的推论。当一个模型足够接近数据生成过程时，它的解释揭示了对潜在过程的洞察。训练数据永远不应该评估模型的性能，因为这样模型可能会过度拟合训练数据，从而导致乐观或夸大的估计。应当使用基于抽样程序的样本外验证，因为这些方法中有许多是容易获得的，并且经过了充分的研究和实践。在计算模型选择和超参数调整中，嵌套采样是首选。

## 复杂模型的不必要使用

在使用可解释性技术时，一个常见的陷阱是当一个简单的自解释模型足够时，使用一个不透明的复杂模型。尽管模型不可知的可解释性技术揭示了复杂黑盒模型的行为和机制，但有时可解释的模型提供了高度的透明性。

尽管模型不可知的方法可以阐明复杂的 ML 模型的行为，但是内在可解释的模型仍然提供了更高程度的透明性，并且考虑它们增加了发现真正的数据生成函数的机会。人们普遍认为，就准确性而言，复杂的最大似然模型总是优于更易解释的模型，因此应该是优选的。然而，有几个例子表明，可解释的模型是严重的竞争对手。

我们建议从简单、可解释的模型开始，如线性回归模型和决策树。GAM 或广义加法模型可以在简单的线性模型和更复杂的机器学习模型之间逐渐过渡。gam 具有理想的特性，可对平滑的非线性效果进行额外建模，并提供现成的 PDP，但没有掩蔽交互的潜在缺陷。在拟合模型之前指定 GAM 的附加模型结构，以便仅估计预先指定的特征或交互作用效果。特征之间的交互可以手动或通过算法来添加(例如，通过前向贪婪搜索)。

只有当精度提高显著且相关时，才应首选复杂模型。但是，从简单的模型开始，然后慢慢往上爬，尝试更复杂的模型，这是非常可取的。

## 忽略特征相关性

在应用解释方法之前，从业者应该检查数据中特征之间的依赖性。让我们来看看依赖特性可能导致的各种问题。

### 外推解释

每当 ML 模型依赖于特征相互作用(即，特征是依赖的)时，在训练数据中推断的诸如置换特征重要性、部分依赖图和 shapely 值的方法通常会导致误导性的解释。这些外推或扰动产生用于模型预测的人工数据点，并被进一步聚合以产生全局解释。特征值通常受到随机二次抽样值的干扰。

因此，每当您应用模型解释方法时，您应该总是通过描述性统计或依赖性度量来检查特性中的依赖性。如果在模型中包含从属特征，还应该提供有关从属强度和关系类型的附加信息。

### 混淆线性相关性和一般相关性

有时，当特征之间的相关系数为零时，特征之间仍然存在相关性，当将其放入模型中时，会导致误导性的模型解释。“特征之间的独立性意味着皮尔逊相关系数将为零”的反义词一般为假。该系数通常跟踪线性关系，并且经常受到异常值的影响。

当我们的特征数量较少时，我们可以使用散点图。但是，对于高维度的图，我们可以使用其他的依赖度。如果相关性是单调的，那么可以使用 Spearman 的秩相关。我们应该对分类或混合特征使用不同的度量。

### 误解条件解释

解释技术的条件变量避免了外推，但需要不同的解释。独立于其他要素干扰要素的解释方法在相关要素下进行外推，但提供了对模型机制的深入了解。解释技术的条件变量避免了外推，但需要不同的解释。因此，这些方法被称为对模型真实，但对数据不真实。对于部分相关图等特征影响方法，该图可解释为特征对预测的孤立平均影响。对于置换特征重要性，重要性可以被解释为当特征的信息被破坏时性能的下降。当特征是高度依赖和条件效应，并且使用重要性分数时，从业者必须意识到不同的解释。

## 由于功能交互导致的误导性解释

当使用各种方法生成解释时，有时特征交互会导致很多混乱。让我们来研究特性交互可能导致的各种问题。

### 聚合导致的误导性特征效果

全局解释方法，如部分相关图或累积局部效应图，将模型预测的平均效应可视化。但是，当功能交互时，它们可能会产生误导性的解释。

对于部分相关图，我们建议额外考虑相应的个体条件期望曲线。ICE 曲线直接显示了个体预测之间的异质性，而部分相关图和累积局部效应平均了交互效应。通过将这些单独的条件期望曲线聚集成一个整体的边际效应曲线，比如部分相关图，你不会丢失很多信息。然而，当特征相互作用时，不同观测的边际效应曲线可能不会显示对目标的类似效应。

### 未能将主效应与交互效应分开

对于模型解释方法来说，将相互作用的影响与主要影响分开是非常重要的。例如，排列特征重要性包括特征的重要性和与其他特征的相互作用。像 SHAP 和莱姆这样的方法提供了附加的解释，但是没有把主效应和交互效应分开。函数方差分析可能是将联合分布分解成主效应和交互效应的最流行的方法。使用相同的思想，H 统计量通过将二维 PDP 分解成单变量分量来量化两个特征之间或一个特征与所有其他特征之间的相互作用强度。在非交互特征中，二维部分相关函数等于两个基础单变量部分相关函数之和。

## 忽略模型和近似不确定性

许多解释方法仅提供了一个平均估计值，但没有量化不确定性。模型训练和解释计算都受到不确定性的影响。该模型是根据(随机)数据训练的，因此应被视为随机变量。解释方法通常根据对数据的期望值(PFI，PDP，Shapley 值，...)但是使用蒙特卡罗积分来近似。忽略这两个不确定性来源会导致对噪音和不可靠结果的解释。一旦在多个模型拟合中取平均，这种效应就可以抵消。单个 PDP 可能会产生误导，因为它没有显示由于 PDP 估计(第二个图)和模型拟合而产生的方差。如果我们对学习特定的模型不感兴趣，而是对特征 X1 和目标之间的关系感兴趣，我们应该考虑模型方差。

通过用给定的模型但用不同的排列或自举样本重复计算 PDP 和 PFI，估计的不确定性可以被量化，例如，以置信区间的形式。对于 PFI，置信区间和假设检验的框架是存在的，但是它们假设一个固定的模型。如果从业者希望以建模过程为条件进行分析，并捕捉过程的变化，而不是以固定模型为条件，PDP 和 PFI 应在多个模型拟合上计算。

## 无法扩展到高维设置

在生成或使用解释时，有时会忽略高维设置。让我们调查一下这可能导致的问题。

### 高维 IML 输出的人类可理解性

将可解释性方法天真地应用于高维数据集(例如，可视化特征效果或计算特征级别的重要性分数)会导致压倒性的高维可解释性输出，这阻碍了人类分析。尤其是基于可视化的解释方法，使得高维设置中的从业者很难专注于最重要的洞察。

一种自然的方法是在应用任何 IML 方法之前降低维数。这是否有助于理解取决于所得到的缩减特征空间的可能语义可解释性，因为可以通过线性或非线性变换来选择特征或减少维度。如果用户希望在原始特征空间中解释，可以使用许多特征选择技术，从而产生更稀疏和更容易解释的模型。包装器选择方法是模型不可知的，并且像贪婪正向选择或子集选择过程这样的算法从空模型开始，并且如果需要的话迭代地添加相关特征(的子集)，甚至允许测量特征的相关性以获得预测性能。另一种方法是直接使用隐式执行特征选择的模型，如 LASSO 或组件式增强，因为它们可以生成具有较少特征的稀疏模型。

### 计算工作量

一些解释方法不与特征的数量成线性比例。例如，对于精确 Shapley 值的计算，可能联合的数量，或者对于(完全)功能 ANOVA 分解，成分的数量(主要效应加上所有相互作用)与 O (2p)成比例。

对于功能性方差分析，一个常见的解决方案是保持对主要效应和选定的双向相互作用的分析(类似于 PDP 和 ALE)。有趣的双向交互可以通过另一种方法来选择，例如 H 统计。然而，双向交互的选择需要额外的计算工作。相互作用强度通常随着相互作用大小的增加而迅速降低，只有当所有的(d 1)方向相互作用都很显著时，才应该考虑 d 方向相互作用。对于基于 Shapley 的方法，存在基于随机采样和评估特征排序的有效近似，直到估计收敛。估计的方差在 O (1m)中减少，其中 m 是评估排序的数量。

## 不合理的因果解释

技术数据科学专家通常对底层数据生成机制的因果洞察感兴趣，而模型可解释性方法通常不提供这种洞察。常见的因果问题包括确定原因和影响，预测干预措施的效果，以及回答反事实问题。例如，医学研究人员可能想要识别风险因素或预测平均和个体治疗效果。因此，研究者可能倾向于从因果的角度来解释可解释性方法的结果。预测模型的因果解释通常是不可能的。标准的监督 ML 模型不是设计来模拟因果关系的，而仅仅是利用关联。模型可能依赖于目标变量的原因和结果，以及重建对 Y 的未观察到的影响的变量(例如，结果的原因)。因此，变量是否与预测模型相关(例如由 PFI > 0 表示)的问题并不直接表明变量是原因、结果还是与目标变量没有任何因果关系。

## 摘要

这一章回顾了解释技术的许多缺陷，例如不良的模型概括、依赖特征、特征之间的相互作用或因果解释。我们希望鼓励在实践中解释 ML 模型时采取更加谨慎的方法，为从业者指出已经(部分)可用的解决方案。赌注很高:ML 算法越来越多地用于社会相关的决策，模型解释在决策中发挥着重要作用。因此，我们相信你可以从 IML 技术的属性、危险和问题的具体指导中受益。我们需要努力开发一套推荐的、易于理解的工具，这需要更仔细的研究。