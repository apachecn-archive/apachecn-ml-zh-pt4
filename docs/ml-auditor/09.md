# 9.机器学习基础

在这一章中，将详细描述机器学习过程的基础。有监督和无监督的机器学习将被进一步探索。将详细描述一些最常用的机器学习模型和算法。这包括降维。本章还将介绍在特征工程阶段需要记住的一些事情。它还讨论了模型拟合，特别是过拟合和欠拟合以及如何避免它们。

## 监督学习

监督机器学习算法由两种不同类型的预测组成——分类器和回归。受监督的机器学习涉及对结果的每次观察的特定目标变量的预测。例如，考虑对观察是否构成欺诈付款的预测。用于训练机器学习算法的特征可以是支付日期、支付金额、供应商细节等。这些特征代表非因变量，对付款是否为欺诈付款的预测表示要预测的因变量或目标变量。一旦这些特征被“清除”，这些特征和训练观察可以由应付账款团队手动分类为确认的欺诈性支付。这些特征和欺诈性支付指示符然后可以用于训练机器学习分类器模型。

在用另一组数据点对模型进行训练和验证之后，可以将其部署到生产中来预测欺诈性付款。这个例子基于这样一个事实，即被预测的输出或目标变量只具有两种可能的类别之一——“欺诈”或“非欺诈”。可以设置相同的示例来显示我们对给定观察可能是欺诈性支付的置信度。这可以通过机器学习回归模型(例如神经网络)来完成。在这种情况下，预测输出可以采用从 0 到 1 的十进制值，0.35 表示给定观察发生的概率为 35%。通常，如果被预测的输出是分类变量，则使用分类器。并且如果目标变量是可量化的数字，则使用回归模型。

通常，如果被预测的输出是分类变量，则使用分类器。并且如果目标变量是可量化的数字，则使用回归模型。

在本书中，我们将探索解决风险管理和审计问题所需的关键机器学习算法的高级工作方式。可能没有必要详细介绍这些算法是如何工作的。然而，对算法做什么以及何时使用它们的高层次理解可能足以以最实际的方式应用机器学习方法。作者认为，这对于确保最终用户在行业环境中采用机器学习至关重要。

### 分类器

如前所述，分类器是监督学习的重要组成部分，因为它们可以在预测分类变量时使用。构建分类器模型的步骤包括:

*   用训练数据(特征和预期输出)训练分类器。

*   分类器使用这些数据观察来识别观察如何映射到预期输出的关系和模式。

*   在训练分类器之后，生成分类器模型，并通过混淆矩阵和 ROC 曲线评估其准确性。接收器操作特性(ROC)曲线是显示分类器系统性能的曲线图。

*   最终的模型随后被部署到生产中。当模型处理新的观察值时，模型将根据特征预测预期输出。

分类器通常以两种不同的方式使用——提供对底层数据的洞察和将观察结果分类到正确的类别。第二种应用更常用。前面关于欺诈付款分类器的例子是第二个应用的极好例子。前面的例子可以与分类器一起使用的另一种方式是理解哪些模式导致欺诈。在前面的示例中，在执行分类后，可以进一步分析已确认的欺诈组，以找到创建更有利于欺诈的环境的特征组合。大多数欺诈付款是否发生在特定的一天(也许是星期五)？)并带有编号的公司(149324 省有限公司)？这种类型的问题可以提供对数据的额外见解。

### 决策树

决策树是一种易于理解的分类器。它遵循具有节点和叶子的树结构(在最低层)。每个节点由一个关于一个特性的问题组成。如果特性(在节点级别)是分类的，那么将有不同的子节点对应于每个类别。另一方面，如果特征是连续的，则使用阈值来获得相应子节点的“是”或“否”响应。在最低层，每个叶显示了被预测的目标变量中分裂的比例。

虽然不一定要了解创建决策树的确切步骤，但还是会给出一个简要的解释供参考。以下是步骤:

*   找到将数据划分到预期类别的最佳功能。例如，在前面的例子中，类可能是“未存活”和“存活”这将成为树根。

*   递归地，通过划分将数据划分到类中的最佳特征来训练每个子节点。

*   一旦所有特征用尽或达到最大深度(参数)，该算法停止离开具有分区类及其比例的叶节点。

信息增益或基尼系数最有可能被用来确定每一级树的比例。这些是可以进一步阅读的细节，但在此不做讨论。

Python 库 Sklearn 可以用来创建决策树分类器。以下是创建决策树分类器并基于 Python 中的样本 iris 数据集对其进行训练的示例代码:

```
from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
x, y = iris.data, iris.target
clf = tree.DecisionTreeClassifier()
clf = clf.fit(x, y)

```

在前面的代码中，x 表示定型数据，y 表示预期输出。在训练决策树分类器之后，我们可以使用以下代码绘制决策树:

```
tree.plot_tree(clf)

```

我们可以使用经过训练的决策树分类器进行预测，代码如下:

```
print(clf.predict([[1., 2., 3., 4.]]))

```

图 [9-1](#Fig1) 显示了为 Python 中提供的样本 iris 数据集创建的决策树。这里，X[0]是第一个特征，X[1]是第二个特征，以此类推。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig1_HTML.jpg)

图 9-1

虹膜数据集的决策树

决策树根据节点中的条件集将每个节点上的数据集分成两部分。例如图 [9-1](#Fig1) 中的根节点检查第四个特征是否≤ 0.8。数据是根据这一条件划分的。

### 随机森林

随机森林在其算法中利用决策树。随机森林是一种集成学习方法，它通过在训练时构建大量决策树来进行分类、回归和其他机器学习任务。对于分类任务，每个单独决策树的分类分数被平均，以获得训练的随机森林模型的分类分数。随机森林通常比决策树执行得更好，因为分类器不像单个决策树那样被“修剪”，并且包含来自随机决策树的各种模式的混合。修剪通过降低最终分类器的复杂性来提高预测准确性。随机森林最重要的特征是，它有助于了解给定数据集中特征的重要性。例如，假设我们使用上一章的欺诈付款分析示例。考虑发票日期、发票号、供应商国家和采购订单号等功能，以及其他十个变量。使用随机森林，我们可以确定这些特征中哪些比其他变量更重要。例如，也许供应商所在的国家/地区决定在发现某项付款是欺诈性付款方面拥有最大的发言权。随机森林通过将特征随机插入决策树来确定其对分类分数的影响，从而发现特征的重要性。

以下代码可用于训练随机森林，并在 Python 中使用它进行预测:

```
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
iris = load_iris()
x, y = iris.data, iris.target

clf = RandomForestClassifier()
clf = clf.fit(x, y)
print(clf.predict([[1., 2., 3., 4.]]))

```

### 支持向量机

支持向量机(SVM)是一种基于线性可分性概念的分类器算法。对于 SVM，存在由平面(3 维)或线(2 维)分开的两个类，使得属于第一类的所有数据点与属于第二类的数据点分开。如果数据可以在逻辑上分成两类，那么 SVM 就是有效的。然而，现实中并非所有数据集都是如此。如果数据集确实可以用一个平面(或一条线)清晰地分割，那么这是一个有效的算法。在高维空间中(当有许多特征时)，可以利用降维算法来降低数据集的维数以供 SVM 使用。

图 9-2 显示了 SVM 算法使用的线性可分性概念。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig2_HTML.jpg)

图 9-2

线性可分性

以下代码可用于训练 SVM 分类器:

```
from sklearn.linear_model import SGDClassifier
from sklearn.datasets import make_blobs
X, Y = make_blobs(n_samples=50, centers=2, random_state=0)
clf = SGDClassifier(alpha=0.01, max_iter=100)
clf = clf.fit(x, y)

```

### 逻辑回归

逻辑回归分类器模型的工作方式类似于 SVM 模型，但它给出的是非二进制输出。分数是根据这些点离超平面有多远来分配的。超平面将 n 维空间分成两个不相连的部分。使用 sigmoid 函数来导出分数。该函数以这样一种方式建立，即在超平面处得分为 0.5，并且如果与超平面的距离减小或增加，则得分趋向于 0 或 1。该函数强制分数的最小值为 0，最大值为 1。

图 [9-3](#Fig3) 显示了 s 形函数。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig3_HTML.jpg)

图 9-3

Sigmoid 函数

以下代码可用于训练逻辑回归分类器:

```
from sklearn import linear_model
clf = linear_model.LogisticRegression()
clf.fit(x, y)

```

逻辑回归的一个问题是，大多数要素的权重(系数)可能很小，它们之间没有明显的区别。为了使分类器最佳地工作，我们需要更少的具有大权重的特征，这些特征可以提供对数据的一些洞察。套索回归有助于解决这一问题，它提供了一种将大多数要素权重映射为零的方法，只有重要的要素才具有非零权重。

### 朴素贝叶斯

朴素贝叶斯是一类基于贝叶斯定理的监督机器学习问题，假设特征具有条件独立性。如果两个事件是条件独立的，第一个事件不会影响第二个事件的结果。这里不讨论朴素贝叶斯分类器的细节。对于该分类器，值得注意的是，最初给每个类别分配一个置信度(例如，在二进制分类问题的情况下为 0 或 1)。随着更多信息变得可用，置信度随之改变。可以看出，作为该算法的结果，广泛的现实世界应用是如何成为可能的。即使存在特征之间的条件独立性的假设，该算法在真实世界的应用中仍然惊人地强大，具有高度的准确性。

以下代码可用于训练高斯朴素贝叶斯分类器。在该分类器中，假设特征的可能性是高斯分布(钟形曲线):

```
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)
gnb = GaussianNB()
y_pred = gnb.fit(x_train, y_train).predict(x_test)

```

图 [9-4](#Fig4) 显示了正态分布(高斯分布的一个例子)。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig4_HTML.jpg)

图 9-4

正态分布

### 深度学习

人工神经网络(ANN)或简称为神经网络，已广泛用于分类和回归问题的机器学习中。最近，深度学习(神经网络的一个子领域)因其在大型数据集上的扩展和表现更好的能力而变得更受欢迎。神经网络受到人脑中的生物神经网络的启发。神经网络由称为神经元的连接节点的集合组成，类似于神经元在人脑中的工作方式。有许多不同类型的神经网络。感知器是最简单的神经网络形式。感知器中的每个神经元接受多个输入，并给出一个输出。

图 9-5 显示了由互连节点组成的感知器。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig5_HTML.jpg)

图 9-5

感知器

在图 [9-5](#Fig5) 中，每个神经元都被一个叫做激活函数的函数激活。给定一个输入或一组输入，节点的*激活函数*定义该节点的输出。对于不同的应用有不同类型的激活功能。如果使用 sigmoid 激活函数，那么每个神经元可以充当逻辑回归函数，如前面的逻辑回归部分所示。每个神经元采用输入的加权组合，并应用激活函数来确定输出。

如图 [9-5](#Fig5) 所示，起始层将所有特征作为输入。因此，起始层代表输入层。底部的最后一层是输出层。中间的所有层都称为隐藏层。当有许多隐藏层时，神经网络被认为是深层网络。训练深度网络是术语深度学习发挥作用的地方。

以下代码可用于使用多层感知器分类器进行训练和预测:

```
from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(max_iter=100).fit(x_train, y_train)
clf.predict(x_test)

```

### 混淆矩阵

对数据进行分类的监督学习算法在前面的章节中已经讨论过了。在本节中，将探讨分类模型的评估。

建立机器学习模型的下一个逻辑步骤是找出模型对数据集的执行精度。需要识别假阳性和假阴性，以了解分类器的性能。当预测结果与实际结果不同时，就会出现假阳性和假阴性。对于二元分类器，当分类器预测结果为 1，但实际结果为 0 时，会出现假阳性。同样，当分类器预测结果为 0 而实际结果为 1 时，也会出现假阳性。基于手头的业务问题，确定更重要的错误情况(肯定或否定)。

当分类器设计用于检测癌症患者时，分类器需要识别尽可能多的癌症患者，因此，分类器需要更积极地标记 1，这样假阳性可能更容易接受。如果构建一个分类器来投资基于项目变量的项目，分类器将需要更保守地标记 1 以保持在预算内。

诸如精确度、召回率、真阳性率和假阳性率(以及其他)的度量可以用于测量分类器的性能。以下是这些指标的一些高级要点:

*   **精确。**这显示了最终正确的肯定识别的比例。

*   **回忆。**这显示了被正确标记的实际阳性的比例。

*   **真阳性率。**这个和 precision 一样。如果真阳性率为 1.0，则 100%标记的阳性是正确的。

*   **假阳性率。**这是不应被标记的阳性比例。假阳性率为 0 意味着所有阳性都被正确标记。

混淆矩阵可用于表示落入每个类别的数据点的数量:真阳性、真阴性、假阳性和假阴性。

图 [9-6](#Fig6) 显示了二元分类器的混淆矩阵，其中 0 或 1 是唯一的结果。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig6_HTML.jpg)

图 9-6

二元分类器的混淆矩阵

以下 Python 代码可用于构建混淆矩阵:

```
from sklearn.metrics import confusion_matrix
y_true = ["0", "1", "0", "0", "1", "1"]
y_pred = ["1", "1", "0", "1", "0", "0"]
tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=["0", "1"]).ravel()
(tn, fp, fn, tp)

```

在前面的代码中，tn、fp、fn 和 tp 分别表示真阴性、假阳性、假阴性和真阳性。

### ROC 曲线

**接收器操作特性(ROC)** 是示出分类器在所有阈值下对于真阳性率和假阳性率的性能的图表。

绘制了真阳性率与假阳性率的关系图，如图 [9-7](#Fig7) 所示。这里，具有较低假阳性率和较高真阳性率的分类器是优选的，因为这意味着分类器模型能够正确地预测更多的项目。两个轴的范围都是从 0 到 1。ROC 可用于查看多个分类器或单个分类器的多个类别的性能。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig7_HTML.jpg)

图 9-7

受试者工作特征曲线

在图 [9-7](#Fig7) 中，可以计算 ROC 曲线下的面积(AUC ),以获得跨所有阈值的综合绩效指标。图 [9-8](#Fig8) 显示了曲线下阴影部分的 AUC。AUC 范围从 0 到 1，0 表示 100%的预测是错误的，1 表示 100%的预测是正确的。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig8_HTML.jpg)

图 9-8

ROC 曲线下面积

以下代码可用于绘制 ROC 并计算 AUC:

```
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
fpr, tpr, _ = roc_curve(y_test, y_score)
plt.figure()
lw = 2
plt.plot(fpr, tpr, lw=lw,
      label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()

roc_auc = auc(fpr, tpr)

```

### 回归

**回归**是一组受监督的机器学习技术，用于根据一组输入特征预测连续的输出变量。在高层次上，确定输入特征的系数，以便可以根据输入特征显示输出变量。一旦求解出这些系数，就可以将它们代入一个方程，并用于预测其他输入变量集的输出。

### 线性回归

在线性回归中，假设要素与输出具有线性关系。线性回归方程可以定义为

y = b<sub>0</sub>+m<sub>1</sub>x<sub>1</sub>+m<sub>2</sub>x<sub>2</sub>+m<sub>3</sub>x<sub>3</sub>+…+m<sub>I</sub>x<sub>I</sub>+e

其中 y 是输出变量，

b <sub>0</sub> 为截距，

m <sub>i</sub> 是系数，

x <sub>i</sub> 是用于预测的输入特征，

e 是剩余误差。

一种称为普通最小二乘法(OLS)的方法可以用来计算系数。本书不涉及 OLS。

评估回归模型最流行的方法是使用均方根误差(RMSE)。均方误差(MSE)是预测输出(y <sub>i</sub> )和实际输出平方之间所有差异的平均值。RMSE 是通过取 MSE 的平方根获得的。

RMSE 越低，回归模型的表现越好。较高的 RMSE 意味着预测输出和实际输出之间的差异较大，这意味着回归模型表现不佳。

以下 Python 代码可用于构建线性回归模型并计算 RMSE:

```
from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(x, y)
reg.coef_
reg.intercept_
reg.predict(xtest)
print(np.sqrt(metrics.mean_squared_error(yobs, ypred)))

```

逻辑回归模型和线性回归模型的区别在于，逻辑回归模型是一个分类器，用于对二进制或分类输出进行分类(例如，付款是否具有欺诈性)。线性回归模型可用于计算连续产出(例如，预测对供应商的下一笔付款)。图 [9-9](#Fig9) 显示了逻辑回归和线性回归模型的输出变量如何变化。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig9_HTML.jpg)

图 9-9

逻辑回归与线性回归

## 无监督学习

**无监督学习**是一组机器学习算法，用于在不指定特征标签的情况下，从数据中导出隐藏的见解。聚类是无监督机器学习的流行应用之一。用于识别向供应商支付的欺诈性款项的相同示例可用于演示无监督学习。而不是识别特征(支付日期、支付金额、供应商细节等。)和目标输出(欺诈性支付与否)，我们可以将这些特征输入到聚类算法中。数据集不需要像监督机器学习那样分成训练集和测试集。

聚类算法将使用要素之间的欧几里德距离，根据数据点之间的距离得出聚类。集群的数量可以是基于业务环境和项目需求的任何数量。有一种方法可以计算最佳集群大小，我们将在后面详细讨论。然后，可以使用相应的功能对聚类进行分析，以深入了解数据集中的异常值或公司的支付行为。例如，聚类 1 可以包括对最近被添加到系统中的具有更高支付金额的供应商的支付。群组 2 可以包括对具有较高付款金额的旧供应商的供应商付款。如果过去存在支付欺诈，则可以分析欺诈性支付周围的条件，并与聚类进行交叉检查，以查看是否有任何聚类表现出相似的特征特性。如果存在匹配，则可以进一步分析支付集群，以确认是否发生了欺诈。如果没有欺诈的实例，已知的增加欺诈机会的特征，例如如果公司是一个编号的公司，或者如果公司只经营了很短时间，可以被用作查看是否有任何集群与这些特征共鸣的方法。

### 聚类算法

k-means 和层次或凝聚聚类是两种广泛使用的聚类算法。在本节中，我们将详细讨论这两种算法。

### k 均值聚类

k-means 聚类在数据集中查找具有相似特征的数据组，并将它们分配到最近的聚类质心。质心是数据集中所有点的算术平均值。指定质心的数量是为了让算法分割数据。客户行为细分、异常检测和库存分类是 k-means 聚类的一些常见用例。

可以在 Python 中使用以下代码创建一个包含 3 个聚类的 k-means 聚类，并用颜色绘制它:

```
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=0).fit(x)

```

一旦聚类被 k-means 算法识别出来，就可以将它们与原始数据集连接起来，并绘制成图，以查看聚类是如何跨变量分布的。

图 [9-10](#Fig10) 显示了一个样本 k 均值聚类，显示了聚类如何在两个变量之间分布。在这里，ASD 代表自闭症谱系障碍。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig10_HTML.jpg)

图 9-10

集群的分布

### 分层聚类

在聚集或分层聚类中，使用自底向上的方法来对数据点进行聚类。例如，假设要聚类的点如图 [9-11](#Fig11) 所示。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig11_HTML.jpg)

图 9-11

样本数据点

得到的聚类显示在一个分层的树状结构中，这就是所谓的树状图。前面数据点的树状图如图 [9-12](#Fig12) 所示。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig12_HTML.jpg)

图 9-12

系统树图

以下 Python 代码可用于执行分层聚类:

```
from sklearn.cluster import AgglomerativeClustering
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
model = model.fit(x)

```

为了评估聚类算法，可以使用轮廓分数方法作为一种方式来确定聚类彼此在几何上的不同程度。

### 剪影分数

Silohouette 分数测量每个星团的密度和彼此之间的距离。使用一个简单的公式来确定两个集群之间的轮廓系数:

![$$ s=\frac{\left(b-a\right)}{\left(a,b\right)\ } $$](../images/513842_1_En_9_Chapter/513842_1_En_9_Chapter_TeX_Equa.png)

其中 a 是一个聚类内各点之间的平均距离，b 是所有聚类之间的平均距离。

以下 Python 代码可用于计算轮廓得分:

```
from sklearn.metrics import silhouette_score
print(silhouette_score(x, label))

```

剪影系数在-1 和 1 之间。如果它更接近于 1，则该点更接近于它自己的簇中的点。接近 0 的分数意味着该点与两个聚类的距离大致相同。小于 0 的分数意味着该点不在正确的聚类中。

### 肘图

*肘图*用于确定 k 均值聚类算法的最优聚类数。对 k = 1 到 10 运行 k-means 聚类算法，并且对于每个 k，计算聚类中每个点距其指定中心的平方距离之和(称为失真)。然后将它们全部绘制出来，得到的图看起来像一个弯头。靠近直线急剧弯曲处(在肘部或附近)的聚类大小被认为是聚类的最佳数量。这是因为对于簇中的每个增量，在到达肘部之后，y 轴值(扭曲)的大小会有最小的减少。

图 [9-13](#Fig13) 显示了一个样本弯管图。如图所示，该图中的最佳聚类数为 3。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig13_HTML.jpg)

图 9-13

肘图

### 降维

维数约减是一类无监督学习技术，用于减少数据集的维度大小。降维的假设是，对于一个包含 d 维(特征)的数据集，在 k 维空间(其中 k 远小于 d)可以将两个或多个特征降维为一个特征。这对于处理声音、图像或视频文件特别有用，因为这些文件的特征数量可能非常大。例如，处理图像的一种常见方法是读取它们的像素数据以提取有意义的特征。每个像素可以由 0 或 1 组成，用于编码黑色和白色，或者更复杂，可以由一个数字组成，用于红色、绿色和蓝色。假设需要将人类肺部 x 光图像输入到机器学习算法中，以检测癌症。在这些应用中，能够减少特征的数量是有益的，使得相同的数据可以用更小的子集来表示，以提高算法对患者中检测到的癌症的性能。

主成分分析可用于执行维数缩减，并将在下文中详细讨论。

### 维度的诅咒

降低维度的一个主要原因是，在更高的维度中，大多数数据点彼此相距更远。随着更多的维度添加到数据集中，它们之间会产生更多的空间，从而导致添加的要素实际占用的空间更小。这就是所谓的维数灾难，也是应用降维算法作为初始步骤的主要原因。

### 主成分分析

在主成分分析(PCA)中，高维数据被假设为一个捕捉信息的大球体。PCA 将高维数据投影到低维空间中，以便可以在最佳拟合线中捕获数据，该最佳拟合线最小化点到该线的平均平方距离。如同本书中的其他类似概念一样，我们不会对此进行更深入的探讨。

图 [9-14](#Fig14) 显示了执行 PCA 的示例输出。在这种情况下，更高维度的数据用两个维度表示，X 和 y。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig14_HTML.jpg)

图 9-14

样本数据的主成分分析

以下代码可用于在 Python 中执行 PCA:

```
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(x)

```

### 屏幕图

Scree 图可用于查看在应用 PCA 之前，需要多少主成分才能从数据集中获取大部分信息。

图 [9-15](#Fig15) 显示了一个样板碎石图。从图中可以看出，第一个组件代表大约 30%的信息，第二个组件代表大约 15%的信息的捕获，依此类推。如果加上前四个主成分，大约可以捕捉到 70%的数据。本质上，我们丢失了 30%的信息。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig15_HTML.jpg)

图 9-15

碎石图

## 过拟合、欠拟合和特征提取

本节中的主题是机器学习模型拟合需要牢记的重要通用指南。过拟合和欠拟合是两个常见的监督学习问题，必须加以考虑。特征提取是一个专门的主题，它在特征被机器学习应用程序使用之前处理特征的转换。在许多情况下，需要以这样的方式转换特征，使得它们可以被机器学习算法适当地利用。

### 过度拟合

过度拟合一直是机器学习应用有效性的主要障碍。一般来说，当模型对训练数据有效但对测试数据表现不佳时，过度拟合发生在监督机器学习应用中。例如，假设我们训练一个受监督的机器学习模型(分类器)来检测欺诈性支付，并将供应商 ID 作为特征之一。分类器模型可能对训练数据中的供应商 id 表现良好，但可能无法识别测试数据中的新供应商 id。因此，它对训练数据表现良好，但不能概括学习，因此，分类器对测试数据表现不佳。有很多这样的例子，过度拟合可能是一个问题。

以下是克服过度拟合的一些方法:

*   在将数据集分为训练、测试和验证之前，对其进行随机化。这对于减少由于将交易的前半部分作为训练集而将交易的后半部分作为测试集而引入的偏差可能是令人惊讶的有效。在前面的示例中，他们在培训数据中有一些经常性支付，但在测试数据中没有。

*   许多有监督的机器学习模型不使用验证集。当多个模型被竞争并且最终模型被挑选时，这是特别重要的。当使用多个模型时，使用训练数据训练每个模型，并且为了评估模型，使用测试数据。最终选择的模型需要用以前从未见过的维持数据集进一步测试，以查看选择的模型在真实数据中的有效性。

*   k-fold 交叉验证是一种重采样技术，用于处理小数据集中的过拟合。它随机地将数据集分成 k 个分区。然后，每个分区被用作测试集，剩余的分区被用作训练集。然后记录 k 个分区中每个分区的评估分数，并在最后进行平均，以获得模型的最终性能分数。

图 [9-16](#Fig16) 说明了 k 倍交叉验证程序。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig16_HTML.jpg)

图 9-16

k 倍交叉验证图

### 欠拟合

当监督学习模型不能从训练集中学习时，会发生欠拟合，因此，模型不能泛化。在分类器的例子中，这意味着我们有很多假阳性和假阴性。

最佳模型拟合应该有一个平衡的模型，该模型学习的内容足以概括所使用的数据集。

图 [9-17](#Fig17) 显示了表示过拟合、欠拟合和平衡模型拟合的图表。线条代表与提供的数据(圆圈)相符合的模型。

![](../images/513842_1_En_9_Chapter/513842_1_En_9_Fig17_HTML.jpg)

图 9-17

模型拟合插图

### 特征抽出

数据集的要素通常需要进行转换才能在模型中使用。也就是说，它们不能以原始状态使用。特征生存性与被选择用于机器学习应用的模型一样重要。在本节中，我们将了解一些需要记住的常见特征提取情况以及如何使用它们。这些转换几乎总是在探索性数据分析阶段(EDA)执行。根据研究结果，也可以在其他阶段进行。例如，在模型评估阶段，您确定需要尝试一个您认为可能会提高性能的不同特性。

您可能希望创建仅捕获分类字段中特定类别的虚拟变量。某些字段(如付款类型)可能有大量类别。作为分析的一部分，可能只需要真正捕获某些支付类别。创建虚拟变量的另一个原因是将分类变量转换为数值变量，因为某些算法无法处理分类数据。实现这一点的一种方法是通过一键编码。在一键编码中，如果类别出现在分类字段中，则创建一个新的二进制变量来捕获“0”或“1”。例如，可以创建一个虚拟变量来标记支付类型是否是供应商支付类型。对于分类变量指示供应商付款的所有观察结果，新创建的二进制变量将指示“1”

在某些情况下，变量可能需要从连续变量转换为类别。例如，0-12 岁、13-21 岁、21-50 岁和大于 50 岁的年龄组可能是一次性编码年龄变量的有效方法。

大多数机器学习模型在每次观察只有一个客户信息及其相应特征时效果最佳。例如，考虑一个使用聚类的客户细分练习。如果算法使用每个客户包含多行的原始数据，它可能会将多行视为重复。这可能会给具有更多线路的客户的模型带来偏差。多行可以指示系统中对客户所做的每个更改。为了避免这种偏差，可以对原始表进行汇总(或分组),使其正好包含每个客户的一行，其余的数字列可以提供行数、总计、平均值、最小值、最大值等。这取决于柱的功能。

对于涉及数千个特征的复杂数据，在被机器学习模型使用之前，可能需要降低数据集的维度。例如，在寻找具有癌症风险的患者的机器学习应用中，需要使用患者的肺部 x 射线图像来训练分类器模型。如前所述，由于图像中的维数，原始数据将需要首先通过主成分分析(PCA)来运行，以确保在训练分类器模型时将主成分之一考虑在内。这可以极大地改进训练过程，并捕获训练模型所需的正确级别的信息。

### 全体

前面看到的随机森林算法是一个集成技术的例子，其中决策树的集合输出用于分类和回归问题。一般来说，与单独使用一种机器学习技术相比，集成使用多种机器学习技术来获得更好的预测性能。

一桶模型是一种集成方法，它使用许多模型(如随机森林、神经网络、朴素贝叶斯、支持向量机和决策树)来解决同一个预测问题。该算法以及相应的性能分数被制成表格，以便在问题发生变化时找出哪个算法表现良好。这可能是一个强大的工具，特别是对于欺诈检测应用程序，在这些应用程序中，问题环境会不断变化，需要找到评估的最佳算法来提高真正的阳性率。

## 结论

机器学习主要分为两种:有监督的和无监督的。在监督学习中，标签被提供给算法来训练模型。然而，在无监督学习中，标签并不给予算法。无监督学习帮助我们找到数据集中隐藏的结构。这两种类型的机器学习算法都有自己的一套性能测量指标。PCA 可用于将高维数据集(或特征)降低到低维，而不会丢失大部分信息。