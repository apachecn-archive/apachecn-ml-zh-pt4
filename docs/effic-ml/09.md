# 9.深度学习

Any fool can know. The point is to understand. —Albert Einstein

自诞生以来，人工神经网络(ann)的历史充满了高潮和低谷。在节点水平上，人工神经网络从高度简化的神经模型开始，如麦卡洛克-皮茨神经元(麦卡洛克和皮茨 1943)，然后演变成罗森布拉特的感知器(罗森布拉特 1957)和各种更复杂和复杂的计算单元。从单层和多层网络，到自递归 Hopfield 网络(Tank 和 Hopfield 1986)，到自组织映射(也称为 Kohonen 网络)(Kohonen 1982)，自适应共振理论和时间延迟神经网络以及其他建议，人工神经网络已经见证了许多结构迭代。这几代产品进行了不断的改进，有望解决前辈的局限性，达到更高的智能水平。尽管如此，这些“智能”网络的复合效应并没有能够捕捉到真正的人类智能(Guerriere 和 Detsky 1991 贝克尔和辛顿 1992)。因此，深度学习正在机器学习社区中兴起，因为传统的浅层学习架构已被证明不适合更具挑战性的机器学习和强人工智能(AI)任务。增强的计算能力的激增和广泛可用性(Misra 和 Saha 2010 年)，加上高效训练算法的创建和神经科学的进步，使得迄今为止不可能实现的深度学习原则得以实现。这些发展导致了深度架构算法的形成，这些算法着眼于认知神经科学，以提出生物启发的学习解决方案。本章介绍了脉冲神经网络(SNNs)和分级时间记忆(HTM)的概念，其相关技术是本书涵盖的技术中最不成熟的。

## 分层时间记忆综述

HTM 的目标是复制新大脑皮层的功能和结构特性。HTM 吸收了霍金斯《智力》一书(2007)中的一些观点，该书认为智力的关键是预测能力。它的框架被设计成新皮层的仿生模型，试图复制大脑的结构和算法属性，尽管是以一种简化的、面向功能的方式。因此，HTM 是分层组织的，如图 [9-1](#Fig1) 所示。所有层级及其子组件都执行一个通用的计算算法。

![A978-1-4302-5990-9_9_Fig1_HTML.gif](A978-1-4302-5990-9_9_Fig1_HTML.gif)

图 9-1。

An HTM network’s hierarchical structure

深度架构采用人类新皮层的分层结构，因为大脑中明显存在一种常见的计算算法，这种算法遍布新皮层区域，并使大脑以非常相似的方式处理感官信息——视觉、听觉、嗅觉等。大脑中的不同区域以层次结构连接，这样向上流动的信息合并，在每个连续的层次上建立更高和更复杂的感官刺激的抽象和表示。大脑的结构，特别是新皮层，进化成能够模拟它所感知的世界的结构。最简单的抽象来说，大脑可以被视为一个生物数据处理黑匣子，它可以在将大量数据强加于其输入(感官)的环境中发现外部原因。这种连续信息流的原因本质上是有层次的，在空间和时间上都是如此。这些原因就像一堆更小的积木，组合起来形成一幅更大的世界图景。例如，语音可以分解成句子，句子可以分解成单词发音，单词发音可以分解成音素，等等。对于数字图像，像素组合成边缘，边缘组合成轮廓，轮廓组合成形状，最后，形状组合成对象。世界上每一个被感知的物体都揭示了在不同粒度水平上感知的相似结构。新大脑皮层，因此也是 HTM，通过模仿，旨在塑造这个等级分明的世界。这种模式发生在 HTM 的各个层面。

在 HTM，网络的最低层节点被馈送感觉信息。根据网络执行的任务，该信息可以是原始的或预处理的。节点通过识别可重复的模式和这些模式出现的顺序，并通过本地存储器结构或通过连接性配置存储它们，来学习数据流的最基本特征。然后，这些基本模式和序列被用作更高层次的构建模块，以形成更复杂的感官原因表征。随着信息沿着层次结构向上传播，随着输入模式的抽象越来越高，使用了相同的学习机制。信息也可以沿着层级向下流动。这使得网络能够作为一个生成模型，在该模型中，较高的级别通过传达其内部状态来偏置较低的级别，以填充缺失的输入数据或解决歧义，或两者兼而有之(Hawkins 2007)。

## 分层时间记忆世代

HTM 在其发展过程中已经经历了两代人。称为 Zeta 1 的第一代计算算法的底层实现深深植根于贝叶斯信念传播(BBP ),并从该理论中借用了许多计算和收敛规则。在这个早期版本中，也就是现在的《日落》, HTM 使用了 BBP 的变体。BBP 用于贝叶斯网络中，由此，在某些拓扑约束下，确保网络在消息沿着最大路径长度穿过网络所花费的时间内达到最佳状态。因此，BBP 迫使网络中的所有节点达成相互一致的信念。这些节点的状态用概率术语编码，贝叶斯理论用于处理和融合信息。HTM 可以被认为是一个贝叶斯网络，增加了一些处理时间、自我训练和发现原因的功能。

创建第二代算法是为了使该框架在生物学上更加可行。从功能上来说，许多不变表示和空间与时间池的概念通过引用稀疏分布式表示(SDRs)和结构变化的原则得以延续。用更接近的具有生物学上真实的神经元模型的皮质柱类似物代替节点，并且改变连接性以允许强的侧向抑制(Edelman 和 Mountcastle 1978)。皮质柱是细胞的集合，其特征是共同的前馈连接和强烈的抑制性互连(Edelman 和 Mountcastle，1978)。

第二代算法——最初被称为固定密度分布式表示(FDRs)，现在简称为 HTM 皮层学习算法(CLAs)——取代了 Zeta 1。在更新的框架中，每一层都是一个连续的细胞区域，这些细胞堆叠成列，作为 Edelman 和 mount castle(1978)的皮质列的简化模型，取代了具有清晰感受野的“桶”层次。图 [9-2](#Fig2) 描绘了 HTM 的结构，细胞组织成列，列组织成级，级组织成皮层区域的层级。

![A978-1-4302-5990-9_9_Fig2_HTML.gif](A978-1-4302-5990-9_9_Fig2_HTML.gif)

图 9-2。

HTM structure

鉴于 Zeta 1 强烈植根于贝叶斯理论——特别是信念传播——CLA 建立在 SDR 的原则上。为了理解 CLA 的基本实现，有必要讨论一下特别提款权及其对 HTM 理论的重要性。

为第一代 HTM 模型建议的架构受到它实现的贝叶斯规则的强烈影响。它得益于新大脑皮层的结构特征，即它的等级组织；然而，节点与它们的生物学对应物不同。简而言之，功能建模是重点。在 CLA 中，HTM 抛弃了这些根源，更严格地遵循新皮质结构指导方针。结果是一个神经元模型，在这种情况下被称为 HTM 细胞。图 [9-3](#Fig3) 描述了霍金斯、艾哈迈德和杜宾斯基(2011)提出的模型。与传统人工神经网络中使用的细胞相比，这些细胞更加真实，在生物学上更加可靠。

![A978-1-4302-5990-9_9_Fig3_HTML.gif](A978-1-4302-5990-9_9_Fig3_HTML.gif)

图 9-3。

An HTM cell/neuron model

HTM 细胞有两种传入连接结构:近端树突和远端树突。这两种类型的树突片段都有突触将它们与其他邻近细胞连接起来。由于真实神经元的随机性质，这些突触连接是二元的。因为 Hawkins，Ahmad 和 Dubinsky (2011)假定任何旨在模拟大脑的算法都不能依赖于单个神经元的精度或保真度，所以 HTM 细胞被建模为具有二元、非加权突触；也就是说，它们要么有联系，要么没有联系。为了解释真实神经元收缩和延伸以形成连接的能力，HTM 细胞突触被赋予一个参数，称为持久性。持久性是一个介于 0 和 1 之间的标量值，根据突触对活动的贡献而递增或递减。当持久性超过预定的阈值时，突触就会连接起来。所以 CLA 中的所有突触都是潜在突触。它们是连通性的动态元素。

近端树突负责区域之间的前馈连接。这些树突由一组潜在的突触组成，这些突触与 HTM 区域的输入子集相关联。树突由一列的所有单元共享(共享前馈连接),并充当线性求和单元。远端树突负责单个区域的横向联系。几个节段与远端树突相关联。这些区段相当于一组临界符合侦测器，也就是说，当一次有足够多的连结活跃时，它们会在接收细胞中引发反应。一个片段被激活足以触发细胞(或门)中的响应。每个段将一个 HTM 单元连接到相邻单元的不同子集。这些细胞的活动受到监控，并允许接收细胞进入预测状态。这实质上是 HTM 网络中预测的根源:每个细胞持续监视周围细胞的活动，以预测自己的细胞。

最后，一个 HTM 单元有两个二进制输出。如果存在足够的激活，由于近端连接，第一个输出迫使细胞进入激活状态。如果存在足够的邻居活动，则由于远端连接，第二输出迫使细胞进入预测状态；该细胞有望很快被激活。这使得 HTM 能够以预测的方式行动，并对输入序列做出反应。最后，HTM 单元的输出是这两个输出的或。这是层次中较高区域接收的输入。图 [9-4](#Fig4) 显示了一组 HTM 列中细胞激活的例子:在任何一点，由于前馈输入(深灰色)，一些细胞将是活动的，而其他细胞，从活动细胞接收横向输入，将处于预测状态(浅灰色)。

![A978-1-4302-5990-9_9_Fig4_HTML.gif](A978-1-4302-5990-9_9_Fig4_HTML.gif)

图 9-4。

HTM columns

### 稀疏分布表示

CLA 借用了其生物类似物的许多操作原理。新大脑皮层由超过 10 个高度互联的神经元组成。然而，它仍然能够以相对稀疏的激活水平对刺激做出反应。大量的抑制性连接使这成为可能。抑制保证了在任一时刻只有少量的神经元被激活。此外，CLA 实现了与 SDR 相同的编码策略。使用横向突触连接，强烈刺激的神经柱抑制附近的活动，从而减少活动柱的数量，并产生输入模式或刺激的稀疏内部表示。这种内部表示也是分布式的，即在一个区域内展开。

因为有效位是稀疏的，所以与其他表示(如 ASCII 代码)相比，知道它们的子集仍然携带关于输入模式的信息。对于 ASCII 码，单个位是没有意义的；因此，特别提款权注入了个别激活的代表性质量。因为可能是大量神经元中只有极小一部分是活跃的，所以单个神经元获得的任何语义意义都是特定于有限数量的相似模式的。因此，即使是一个模式的活跃神经元的子集也可以很好地指示它。强化这种稀疏性导致的理论上的信息损失没有实际影响(Hawkins，Ahmad 和 Dubinsky 2011)。

### 算法实现

与第一代相反，学习阶段与推理阶段的分离并没有提供对 HTM 运算规则的更多洞察。在 CLA 中，学习、推理和——最重要的——预测和谐地发生。层级的每一层总是在预测。较低节点的学习可以在它们稳定时关闭；当被激活时，它与预测同时在线发生。

因此，最好从 CLA 的池函数的角度来考虑它的操作。以下两节讨论了第二代算法中空间和时间池背后的理论，并展示了它们如何在皮层网络中实现，如 Hawkins，Ahmad 和 Dubinsky (2011)所建议的。

### 空间池

空间池的角色在 CLA 和 Zeta 1 中是一样的。空间上相似的输入模式应该有一个共同的内部表示。该表示不仅应该对噪声鲁棒，而且应该稀疏以遵守 SDR 的原则。这些目标是通过加强皮质柱之间的竞争来实现的。当显示输入数据时，HTM 区域中的所有列将计算它们的前馈激活。被激活的列被允许禁止相邻的列。这样，只有一小组强活动列可以表示相似输入的集群。为了给其他列一个公平的激活机会，并确保所有列都被使用，增加了一个提升因子。这使得弱列能够更好地竞争。

对于每一个活动列，所有潜在突触的持久性值基于 Hebbian 学习规则被调整。与活动输入位对齐的突触的持久性值增加，而与非活动输入位对齐的突触的持久性值减少。图 [9-5](#Fig5) 显示了相关阶段的流程图。

![A978-1-4302-5990-9_9_Fig5_HTML.gif](A978-1-4302-5990-9_9_Fig5_HTML.gif)

图 9-5。

Spatial pooler flowchart

空间池操作如下:

*   阶段 0(对应于初始化):为每一列随机分配一组随机的输入(输入向量的 50%)，这被称为列的潜在池。这个池中的每个输入都由一个潜在的突触代表，并被分配一个随机的持久性值。永久性值的选择根据以下标准决定:
    *   值是从持久性阈值附近的一个小范围内选取的。这使得潜在的突触在少量的训练迭代后变得连接(或断开)。
    *   每一列在输入区域上都有一个自然中心，持久性值偏向这个中心，在中心附近有较高的值。
*   阶段 1(对应于重叠):每列的重叠被计算为具有活动输入的连接突触的数量乘以其增强。如果该值低于预定义的阈值(“minOverlap”)，则重叠分数设置为 0。
*   阶段 2(对应于抑制):将抑制的局部区域(列的邻域)中获胜列的数量设置为预定义值 n。如果一列的重叠分数大于其抑制半径内第 n 个最高列的分数，则该列是获胜者。这种抑制策略的一个计算要求低得多的变体是为层级的每一级挑选具有最高重叠分数的列。
*   阶段 3(对应于学习):在此阶段，根据需要更新所有突触的持久性值以及参数，如增强和抑制半径。对于获胜列，如果突触是活动的，则其持久性值递增；如果无效，则递减。有两种独立的增强机制来帮助列学习连接。如果一个列没有经常获胜，它的整体提升值会增加；或者，如果一个列的连接突触与任何输入的重叠不够频繁，它的持久性值就会增加。这个阶段随着更新抑制而终止。

### 时间池

通过由空间池程序计算出的获胜列，HTM 网络可以洞察其输入端可能看到的模式。它缺少的是背景。任何一个模式都可以作为大量序列的一部分出现，也就是说，出现在多个上下文中。HTM 理论的核心是通过学习序列来预测的能力。在 CLA 中，每列使用多个单元进行顺序学习。一列中的所有单元共享前馈激活，但是只有一个子集(通常是单个单元)被允许激活。这意味着由同一组列表示的同一模式可以由每列中的不同单元格表示，这取决于模式出现的上下文。在细胞水平上，细胞的每个树突远端片段都与同一区域的其他细胞有一系列连接，用于识别网络在某个时间点的状态。细胞可以通过观察它们的连接来预测它们何时变得活跃。一个特定的单元可以是几十或几百个时间转换的一部分。因此，每个细胞都有几个树突节段，而不仅仅是一个。

时态池包含三个阶段。在第一阶段，计算每个单元的活动状态。阶段 2 计算每个细胞的预测状态。在阶段 3，突触通过增加或减少它们的持久性值来更新。下面是推理和学习同时发生的一般情况，注意 CLA 中的学习阶段可以关闭。

*   阶段 1:对于每一个获胜的列，在这里计算它的每个单元的活动状态。此外，一个单元被指定为学习单元。如果任何单元由于其横向连接而处于预测状态，则其被置于活动状态。如果一个学习细胞有助于其横向激活，该细胞也被选为学习细胞。相比之下，如果列中没有单元处于预测状态，则所有单元都变为活动状态，以指示上下文不清楚，这一过程称为突发。此外，最佳匹配单元成为学习单元，并且新的远端段被添加到该单元。
*   阶段 2:一旦获胜列中的所有单元都被更新，它们的状态就可以用于其他列的单元中的预测。该区域中的每个单元计算其横向/远端连接。如果一个细胞的任何片段被激活，由于其他细胞的前馈激活，该细胞被置于预测状态。然后，该单元将下列更改排队:
    *   通过增加活动突触的永久值和减少非活动突触的值来加强当前活动片段
    *   加强可以预测这种激活的区段，即与前一时间步中的活动具有(潜在的弱)匹配的区段
*   阶段 3:这是通过决定提交哪些排队的更新来进行学习的阶段。一旦有了前馈输入并且选择了一个单元作为学习单元，就实施临时段更新。因此，只有当突触正确预测了细胞的前馈激活，你才能更新它们的持久性；否则，如果单元由于任何原因停止预测，则片段被负向强化。

## 相关著作

Zhituo，Ruan 和 Wang (2012)在基于内容的图像检索(CBIR)系统中使用了多个 htm，该系统利用查询图像的分类语义而不是低级图像特征来进行图像索引和检索。使用十个单独的 HTM 网络，每个网络具有一些大小为 50 的训练和测试数据集，对于所涉及的五个类别中的四个，召回率达到了 95%以上，对于第五个类别，召回率达到了 70%以上。

Bobier (2007)在 Numenta (Hawkins 的公司)报告的美国邮政署​数据库上重建了一个手写数字识别实验，达到了 95%的准确率。数字图像被二值化，并以不同的参数输入网络，以达到 96.26%的最大比率——作者指出，与其他分类器相比，如支持向量机(SVM)，它在一小部分计算时间内提供了更高的比率。

Kostavelis、Nalpantidis 和 Gasteratos (2012)提出了一种受生物启发的物体识别系统。显著图用于模拟视觉注视，仅显示图像的相关部分，从而减少呈现给分类器的冗余信息量。作者选择用基于相关性的替代方法来代替时态池，在顶部节点使用 ETH-80 和监督学习。在 SVM 和 k-NN 作为顶级主管的情况下，该系统的表现优于其他基于 HTM 的实施。

Sinkevicius、Simutis 和 Raudonis (2011)探索了在公共空间使用 HTM 进行人流量分析。设计了两个 HTM 网络:一个用于人体检测，另一个用于运动方向检测。使用安装在门口的高架摄像机进行实验，并使用不同难度的多个场景评估检测性能。行人检测的平均准确率为 80.94%，方向检测的平均准确率为 73.44%。

Boone 等人(2010)使用 HTM 作为传统计算机视觉技术的替代方法来治疗糖尿病视网膜病变。HTM 主要在视网膜图像上检测视神经。这些图像被分割成视神经大小的片段，用标签(0 或 1)呈现给 HTM。在监督训练后，HTM 网络能够正确分类 77.44%的视神经，这使作者得出结论，HTM 与传统技术相比没有竞争力，尽管它有前途。

Zhuo 等人(2012)补充了最新的图像分类技术，包括局部约束线性编码(LLC)、空间金字塔匹配(SPM)和用于特征池的 HTM。使用 LLC 提取和编码图像描述符。LLC 码然后被馈送到 HTM 和多尺度 SPM 以形成图像向量。该系统使用加州理工学院 101 数据集和 UIUC-Sport 数据集进行评估，线性 SVM 作为分类器。结果显示，与原始 LLC 模型相比，两个数据集的准确性都有所提高(分别为 73.5%对 71.2%和 86.7%对 84.2%)。

Gabrielsson，Konig 和 Johansson (2012)旨在利用 HTM 创建一个有利可图的金融市场交易软件代理。监督训练计划用于 HTM，日内分笔成交点数据用于 E-mini 标准和标准普尔 500 期货市场。调整后的模型被用作市场趋势的预测器，并且当对照人工神经网络进行评估时，至少显示出可比较的结果。

Note

大多数使用 HTM 的工作都是由 Numenta 建立的开发者社区完成的。一年一度的“黑客马拉松”已经产生了多个演示，其中 CLA 被用于交通预测、人体运动预测、井字游戏模拟、红外(IR)传感器预测等等。一个更令人印象深刻的演示，关于音乐分析和预测，展示了 MIDI 音符序列在训练中的应用；一段旋律学了 25 个时代( [`http://numenta.org/blog/2013/06/25/hackathon-outcome.html#jin-danny-stan`](http://numenta.org/blog/2013/06/25/hackathon-outcome.html#jin-danny-stan) )。目前，正在对用于自然语言处理(NLP)的 CLA 模型进行研究。GitHub 网站上有一个例子( [`https://github.com/chetan51/linguist`](https://github.com/chetan51/linguist) )。然而，需要更多的研究和验证来比较 HTM 的性能与该模型声称优于或类似的最先进的机器学习方法。据报道，HTM 的表现，社区还没有看到 HTM 占了上风。

## 脉冲神经网络综述

SNNs 是生物启发的网络，属于第三代 ann。对于人工神经网络来说，任何性能上的改进都应该基于神经元模型。第二代人工神经网络中的神经元模型是基于实际神经元的简化模型，该模型忽略了神经元之间编码信息的实际方式以及这种信息的类型。snn 类似于由一层或多层连接的神经元组成的 ann 结构，但是在神经元的模型和激活函数的类型上有所不同。与利用时间缺失连续激活函数的第二代人工神经网络相比，SNNs 在其学习和激活阶段依赖于尖峰定时。snn 努力模仿人类神经元，使用棘波来传输和学习时空数据(SSTD ),空间数据用突触的位置编码，时间数据用棘波时间活动编码。

SNNs 及其变体已在许多应用中使用，例如字符识别(Gupta 和 Long 2007)、手语识别(Schliebs、Hamed 和 Kasabov 2011)、视觉和听觉模式识别(Wysoski、Benuskova 和 Kasabov 2006、2007)、图像聚类(Meftah 等人 2008)、车祸识别(Kasabov 等人 2013)、人类行为识别(Meng、Jin 和 Yin 2011)、乳腺癌分类(O ' hallDemertzis 和 Illiadis 2014)、脑电图(EEG)空间/光谱时间模式识别(Kasabov 等人 2013)和味觉识别(Soltic 和 Kasabov 2010)。通常，由于计算成本高，SNNs 不如其他机器学习方法流行。

为了理解 SNNs 与 ann 的区别，有必要考察人类神经元最常见的模型:霍奇金-赫胥黎模型、积分-点火模型、泄漏积分-点火模型、伊兹基科维奇模型和索普模型。这些模型将在以下章节中介绍。

### 霍奇金-赫胥黎模型

霍奇金-赫胥黎模型阐述了神经元中动作电位的传播，并可被视为其他模型的基础。霍奇金和赫胥黎在鱿鱼的巨大轴突后模拟了天然神经元的电化学信息。该模型由四个微分方程组成，描述了作为电压(V<sub>m</sub>)和电流(I(t))的函数的神经元膜电容部分的电荷变化，

![A978-1-4302-5990-9_9_Figa_HTML.jpg](A978-1-4302-5990-9_9_Figa_HTML.jpg)

在哪里

I(t)是由突触前电位引起的输入电流

g <sub>Na</sub> ，g <sub>K</sub> ，g <sub>L</sub> 分别是钠和钾离子通道的电导参数和单位面积的泄漏

E <sub>Na</sub> ，E <sub>K</sub> ，E <sub>L</sub> 为平衡势

m，n，h 是无量纲变量，由另外三个微分方程控制

由于这些方程的复杂性(由数据的非线性和四维性引起),为实际实施提出了几种较简单的形式。我们将在下面的章节中讨论其中的一些提议。

### 集成发射模型

积分-点火模型是从霍奇金-赫胥黎模型推导出来的，但忽略了潜在作用的形状。这个模型假设所有的潜在行为都是一致的，只是发生的时间不同。作为前面简化的结果，所有尖峰都具有相同的特征，例如形状、宽度和幅度。膜电容和突触后电位(PSP)由以下方程给出

![A978-1-4302-5990-9_9_Figb_HTML.jpg](A978-1-4302-5990-9_9_Figb_HTML.jpg)

在哪里

u <sub>rest</sub> 是初始状态下神经元的膜电位

ϑ是神经元触发的阈值

t <sup>(f)</sup> 是尖峰发射时间

I(t)是输入电流，由突触前电位引起

### 泄漏集成发射模型

漏积分发射模型不同于积分发射模型，因为如果没有电位到达神经元，则神经元的膜电位随时间衰减。当神经元的膜电位 u(t)在时间 t 达到特定的阈值ϑ，称为锋电位时间 t <sup>(f)</sup> ，且 u(t)满足 u`(t <sup>(f)</sup> ) > 0 条件时，神经元立即发出锋电位。然后，神经元进入绝对不应期 u <sub>abs</sub> ，这意味着神经元将忽略在此期间到达的尖峰的任何影响。不应期持续特定时间 d<sub>ABS</sub>；在此期间，神经元的膜电位为

u(t)=–u<sub>ABS</sub>，

其中 u <sub>abs</sub> 是耐火度潜力。

当 d <sub>abs</sub> 到期时，膜电位回到 u <sub>静止</sub>值。膜电位由下式给出:

![A978-1-4302-5990-9_9_Figc_HTML.jpg](A978-1-4302-5990-9_9_Figc_HTML.jpg)

在哪里

τ <sub>m</sub> 是神经元膜的时间常数

u <sub>rest</sub> 是初始状态下神经元的膜电位

I(t)是输入电流，由突触前电位引起

r 是神经元模型的等效电阻。

### 伊兹克维奇模型

Izhikevich 模型是生物合理性和计算效率之间的折衷。该模型使用以下两个微分方程来表示膜电位的活动:

![A978-1-4302-5990-9_9_Fige_HTML.jpg](A978-1-4302-5990-9_9_Fige_HTML.jpg)

峰后作用由下面的术语描述，其中膜电位和恢复变量被重置

如果 u ≥ ϑ，则 u ← c，且 w ← w + d

这里，u 代表膜电位，w 代表向 u 提供–ve 反馈的膜恢复变量，a、b、c 和 d 是无量纲参数。由于这种模型的简单性，大量的神经元可以用计算机来模拟。

### 索普模型

索普的模型是“整合-发射”模型的一个变种，它考虑了脉冲到达神经元的顺序。索普的模型适用于许多应用，因为它使用了简单的数学表示:

![A978-1-4302-5990-9_9_Figg_HTML.jpg](A978-1-4302-5990-9_9_Figg_HTML.jpg)

在哪里

w <sub>ji</sub> 是神经元 j 和神经元 I 之间突触的权重或效率

mod 是一个调制系数![A978-1-4302-5990-9_9_Figv_HTML.jpg](A978-1-4302-5990-9_9_Figv_HTML.jpg) [0，1]

顺序 <sub>j</sub> 是突触前神经元 j 的放电顺序，其中 j ![A978-1-4302-5990-9_9_Figw_HTML.jpg](A978-1-4302-5990-9_9_Figw_HTML.jpg) [1，n–1]，n 是连接到神经元 I 的突触前神经元的数量

该模型中的权重根据以下内容进行更新

![A978-1-4302-5990-9_9_Figh_HTML.jpg](A978-1-4302-5990-9_9_Figh_HTML.jpg)

索普的模型与提前触发并到达当前神经元的连接神经元建立了更强的连接。每当 PSP <sub>i</sub> 达到阈值 PSP <sub>θi</sub> 时就会出现尖峰。在尖峰之后，PSP <sub>i</sub> 立即被设置为 0，使得:

![A978-1-4302-5990-9_9_Figi_HTML.jpg](A978-1-4302-5990-9_9_Figi_HTML.jpg)

这种方法可以快速实时地模拟大型网络。

### SNN 的信息编码

神经元中的信息编码长期以来一直是激烈辩论的主题，即神经元中的信息编码是速率编码还是尖峰编码。最近的研究表明，信息被编码为棘波编码，因为速率编码不足以代表神经元快速处理信息的能力。

秩编码非常高效，可以实现最高的信息编码容量。秩编码从使用高斯感受野将输入值转换成一系列尖峰开始。高斯感受野由 m 个感受野组成，用于将输入值 n 表示为尖峰。假设 n 取范围![A978-1-4302-5990-9_9_Figd_HTML.jpg](A978-1-4302-5990-9_9_Figd_HTML.jpg)中的值，神经元 I 的高斯感受野由其中心 u <sub>i</sub> 给出，

![A978-1-4302-5990-9_9_Figj_HTML.jpg](A978-1-4302-5990-9_9_Figj_HTML.jpg)

以及宽度σ，

![A978-1-4302-5990-9_9_Figk_HTML.jpg](A978-1-4302-5990-9_9_Figk_HTML.jpg)

其中β是控制感受野宽度的参数，1 ≤ β ≤ 2。

不像普通的学习方法依赖于尖峰的速率，SNNs 使用 Hebb 规则的变体来强调尖峰时间的影响。权重更新机制基于突触前和突触后神经元的放电时间之间的间隔。两种类型的神经元参与权重更新过程；当前神经元的突触前神经元(突触前神经元)和突触后神经元(突触后神经元)。如果突触后神经元紧接在突触后神经元之后触发，那么这两个神经元之间的连接被加强，使得神经元的权重由下式给出:

如果δt≥0，那么 w <sub>新</sub> ← w <sub>旧</sub>+δw，

其中δt 是点火时间差，等于后 t<sub>前</sub>t<sub>。</sub>

如果突触前神经元在突触后神经元之后立即触发，那么这两个神经元之间的连接被削弱，使得

![A978-1-4302-5990-9_9_Figl_HTML.jpg](A978-1-4302-5990-9_9_Figl_HTML.jpg)

当突触后神经元的触发时间不紧接在突触前神经元的触发时间之后出现时，权重不被更新。

前面的讨论提出了兴奋性连接。抑制性连接使用一个简单的过程，因为它没有考虑突触前和突触后神经元放电时间之间的间隔。

### 在 SNN 学习

为 SNN 开发的最流行的算法是 SpikeProp 和 Theta 学习规则。SpikeProp 类似于设计用于调整第二代神经网络中的权重的反向传播算法。Theta 学习规则使用二次积分和点火(QIF)神经元模型。这两种算法都对神经元模型的参数非常敏感，有时这些算法会遇到尖峰丢失问题。当神经元对于任何模式都不触发时，发生尖峰丢失，因此不能通过梯度方法恢复。另一种训练 SNNs 的方法是使用进化策略，这种策略不会受到调谐敏感性的影响，但是这种方法计算量大且成本高。

假设 H，I，J 分别是输入层，隐藏层，输出层。来自特定层的每个神经元由小写 I、h 和 j 表示。在神经元 I 之前的神经元集合由γ<sub>I</sub>表示，而在神经元 I 之后的神经元由γ<sup>I</sup>表示。连续层中两个神经元之间的每个连接由 m 个![A978-1-4302-5990-9_9_Figx_HTML.jpg](A978-1-4302-5990-9_9_Figx_HTML.jpg) {1..m}个子连接，其中每个子连接具有恒定的增量延迟 d <sub>k</sub> 和带有 k ![A978-1-4302-5990-9_9_Figy_HTML.jpg](A978-1-4302-5990-9_9_Figy_HTML.jpg)的权重![A978-1-4302-5990-9_9_Figf_HTML.jpg](A978-1-4302-5990-9_9_Figf_HTML.jpg)。t <sub>i</sub> ，t <sub>h</sub> 和 t <sub>j</sub> 代表各层神经元的尖峰时间，而![A978-1-4302-5990-9_9_Figt_HTML.jpg](A978-1-4302-5990-9_9_Figt_HTML.jpg)和![A978-1-4302-5990-9_9_Figu_HTML.jpg](A978-1-4302-5990-9_9_Figu_HTML.jpg)代表各层神经元的实际尖峰时间。

神经元 I 的响应函数由下式给出:

![A978-1-4302-5990-9_9_Figm_HTML.jpg](A978-1-4302-5990-9_9_Figm_HTML.jpg)

使用:

![A978-1-4302-5990-9_9_Fign_HTML.jpg](A978-1-4302-5990-9_9_Fign_HTML.jpg)

其中τ是膜常数。

从输出层到隐藏层的权重更新由下式给出

![A978-1-4302-5990-9_9_Figo_HTML.jpg](A978-1-4302-5990-9_9_Figo_HTML.jpg)

在哪里

![A978-1-4302-5990-9_9_Figp_HTML.jpg](A978-1-4302-5990-9_9_Figp_HTML.jpg)

从隐藏层到输入层的权重更新由下式给出

![A978-1-4302-5990-9_9_Figq_HTML.jpg](A978-1-4302-5990-9_9_Figq_HTML.jpg)

在哪里

![A978-1-4302-5990-9_9_Figr_HTML.jpg](A978-1-4302-5990-9_9_Figr_HTML.jpg)

SpikeProp 算法的缺点包括:

*   以固定的时间步长间隔计算神经元的膜电位。
*   没有选择初始权重和阈值的方法。
*   需要在 t=0 时出现尖峰的参考神经元。
*   由于尖峰响应函数不足，收敛受到损害。

### SNN 变体和扩展

SNNs 的变体和扩展将在下面的章节中讨论。

#### 进化脉冲神经网络

进化脉冲神经网络(eSNNs)是具有动态架构的 SNNs 的变体类别(Schliebs 和 Kasabov 2013)。eSNNs 使用 Thorpe 的模型和种群等级编码来编码信息。eSNNs 结合了不断发展的连接主义系统(ECoS) (Kasabov 2007)架构和 SNNs。与 SNNs 相比，eSNNs 有三个优点。首先，eSNNs 具有较低的计算成本，因为它们依赖于轻神经元模型，即 Thorpe 的模型。其次，eSNN 中的学习算法比 SNNs 中的学习算法更有效，SNNs 在 20%的情况下无法收敛(Thiruvarudchelvan、Crane 和 Bossomaier 2013)。第三，eSNNs 是在线学习模型，这使它们比其他技术有明显的优势。

#### 基于水库的进化脉冲神经网络

水库计算(RC)是一个随机和稀疏连接的节点(神经元)的框架，用于解决复杂的动态系统。RC 分为回声状态机(ESM) (Jaeger 2007)和液态状态机(LSM) (Maass，natschlger，和 Markram 2002Maass 2010)类型。

LSM 是一个实时计算模型，它接受连续的数据流并生成高维连续输出流。LSM 可以被视为动态 SVM 核函数。LSM 的结构由液体和读出功能组成。液体可以被视为具有训练有素的架构的过滤器，并且用于通用问题解决，例如从同一视频流中识别不同种类的对象。相反，读出函数是专用的，使得不同的读出函数被用于从相同的液体中识别不同的对象。读出函数应该是线性判别和无记忆函数。

#### 动态突触进化脉冲神经网络

动态突触进化脉冲神经网络(deSNNs)是一类在整个学习过程中使用动态权重调整的 esnn(Kasabov 等人，2013 年)。权重的动态更新使得模型在捕捉问题的复杂模式时更有效。在 deSNN 中，根据到达连接的尖峰，权重继续轻微变化，这与 eSNN 相反，在 eSNN 中，权重更新一次。

#### 概率脉冲神经网络

概率脉冲神经网络(pSNN)的神经模型(见图 [9-6](#Fig6) )使用三种概率(Kasabov 2010):

![A978-1-4302-5990-9_9_Fig6_HTML.gif](A978-1-4302-5990-9_9_Fig6_HTML.gif)

图 9-6。

The pSNN model

P <sub>cj，i</sub> (t)，神经元 n <sub>j</sub> 发出的一个尖峰在时刻 t 通过 n <sub>j</sub> 和 n <sub>i</sub> 之间的连线 cj，I 到达神经元 n <sub>i</sub> 的概率。

P <sub>sj，i</sub> (t)，突触 s <sub>j，i</sub> 从神经元 n <sub>j</sub> 接收到一个锋电位后对 PSP <sub>i</sub> (t)做出贡献的概率。

P <sub>i</sub> (t)，神经元 n <sub>i</sub> 在其 PSP 达到发射阈值后发射尖峰的概率。

神经元 n <sub>i</sub> 的 PSP 由下式给出:

![A978-1-4302-5990-9_9_Figs_HTML.jpg](A978-1-4302-5990-9_9_Figs_HTML.jpg)

在哪里

如果从神经元 n <sub>j</sub> 发出尖峰脉冲，则 e <sub>j</sub> 为 1，否则为 0

g(P <sub>cj，I</sub>(t–P))以概率 P <sub>cj，i</sub> (t)为 1，否则为 0

g(P <sub>sj，I</sub>(t–P))以概率 P <sub>sj，i</sub> (t)为 1，否则为 0

## 结论

这一章和前两章已经涵盖了与深度学习相关的四种学习算法:深度神经网络(DNNs)、皮层算法(CAs)、分层时间记忆(HTM)和脉冲神经网络(SNNs)。DNN 是一种从传统人工智能角度发展起来的成熟技术，并在许多应用中显示出稳健的结果。CA、SNN 和 HTM 是生物启发的技术，不太成熟，但被生物启发计算的倡导者认为是非常有前途的。然而，传统的人工智能倡导者认为，DNN 相关的方法将是学习领域的未来赢家。哪种观点会占上风是一个激烈争论的问题。

但是，与其问谁对谁错，我们不如根据所寻求的学习的背景和目的来设计这个问题。我们是否在寻求创造一种通用形式的机器智能，来复制人类智能的能力，即从少数几个实例中进行推理，以用于各种各样的应用？或者，我们是否在追求计算高效的学习框架，这些框架可以用蛮力粉碎从物联网(IOT)收集的大数据，以开发能够预测和描述任务的模型？

由于研究人员尚未就人工智能的独特定义或评估学习算法的黄金指标达成一致，因此当前工作的目的是为读者提供这个快速扩展的深度学习子领域的一般背景和快照。

## 参考

鲍勃布鲁斯。使用阶层式时间记忆的手写数字辨识，2007。

Boone、Aidan R. W .、T. P. Karnowski、E. Chaum、L. Giancardo、Y. Li 和 K. W. Tobin Jr 用于自动视网膜分析的图像处理和分级时间记忆〉。2010 年生物医学科学与工程会议论文集。新泽西州皮斯卡塔韦:电气和电子工程师协会，2010 年。

布杰德，高拉夫。"使用脉冲神经网络的入侵检测."硕士论文，罗切斯特理工学院，2014。

德梅尔齐斯，康斯坦丁诺斯和拉扎罗斯伊利亚迪斯。"一种基于进化脉冲神经网络分类的混合网络异常和入侵检测方法."《数字世界中的电子民主、安全、隐私和信任:第五届国际会议，2013 年电子民主，希腊雅典，2013 年 12 月 5 日至 6 日，修订的论文选集》,由 Alexander B. Sideridis、Zoe Kardasiadou、Constantine P. Yialouris 和 Vasilios Zorkadis 编辑，11-23 页。瑞士查姆:施普林格，2014 年。

睡吧，乔治。"大脑如何工作:学习和识别的层次和时间模型."博士 diss。，斯坦福大学，2008 年。

迪利普、乔治和杰夫·霍金斯。"大脑皮层微电路的数学理论."《公共科学图书馆计算生物学》第 5 卷第 10 期(2009)。`http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000532`。

埃德尔曼，杰拉尔德 m，弗农 b .芒特卡斯尔。大脑皮层组织和高级大脑功能的群体选择理论。麻省剑桥:麻省理工学院出版社，1978 年。

加布里埃尔松，帕特里克，r .柯尼希，和乌尔夫·约翰逊。"基于分级时间记忆的金融市场算法交易."2012 年金融工程和经济学计算智能 IEEE 会议论文集，1–8。新泽西州皮斯卡塔韦:电气和电子工程师协会，2012 年。

格雷尔，迈克尔 R. J .和艾伦 s .德特斯基。"神经网络:它们是什么？"内科年鉴 115，第 11 期(1991):906–907。

古普塔、安库尔和莱尔·n·朗，“使用脉冲神经网络的字符识别”2007 年国际神经网络联合会议论文集，53-58。新泽西州皮斯卡塔韦:电气和电子工程师协会。

霍金斯杰夫。论智力。纽约:时代图书公司，2007 年。

霍金斯、杰夫、苏布泰·艾哈迈德和 d·杜宾斯基。"分级时间记忆，包括 HTM 皮层学习算法."技术报告，Numenta，2011 年。

赫伯特·耶格。“回声状态网络。”Scholarpedia 2，第 9 期:(2007): 2330。

斯蒂芬·约翰逊，《等级聚类方案》《心理测量学》32 卷，第 3 期(1967 年):241–254 页。

尼古拉·卡萨博夫。进化中的联结主义系统。伦敦:斯普林格，2007 年。

尼古拉·卡萨博夫。"尖峰还是不尖峰:一个概率尖峰神经元模型."神经网络 23，第 1 期(2010):16–19。

卡萨博夫、尼古拉、克什蒂伊·多布勒、纳塔波德·农塔利德和贾科莫·因迪韦利。"用于在线时空模式识别的动态进化脉冲神经网络."神经网络 41(2013):188–201。

托沃，科霍宁。"拓扑正确的特征图的自组织形成."生物控制论 43.1(1982):141–152。

科斯塔韦利斯、约安尼斯、拉扎罗斯·纳尔潘提迪斯和安东尼奥斯·加斯特拉托斯。"使用显著图和 HTM 学习的物体识别."2012 年 IEEE 成像系统和技术国际会议论文集，528–532。新泽西州皮斯卡塔韦:电气和电子工程师协会，2012 年。

马斯，沃尔夫冈。"液态机器:动机、理论和应用."《环境中的可计算性:现实世界中的计算和逻辑》, S. Barry Cooper 和 Andrea Sorbi 编辑，275–296 页。伦敦:帝国学院出版社，2011 年。

马斯、沃尔夫冈、托马斯·纳茨拉格和亨利·马克拉姆。"没有稳定状态的实时计算:基于扰动的神经计算的新框架."神经计算 14，第 11 期(2002):2531–2560。

麦卡洛克，W. S .和 W. H .皮茨。神经活动中固有观念的逻辑演算。数学生物物理学通报 5(1943):115–133。

梅夫塔、贝·本耶图、奥·莱佐雷和青香。"用脉冲神经元网络进行图像聚类."ij CNN 2008:IEEE 国际神经网络联合会议论文集，681–685。新泽西州皮斯卡塔韦:电气和电子工程师协会，2008 年。

孟、严、金耀初、。"模拟 BCM 脉冲神经网络中活动相关的可塑性并应用于人类行为识别."IEEE 神经网络汇刊 22，第 12 期(2011):1952–1966。

米斯拉、贾纳丹和英德拉尼尔·萨哈，《硬件中的人工神经网络:二十年进展综述》《神经计算》第 74 期，第 1–3 期(2010 年):239–255 页。

大宝，武德，久保田直之，谷口和彦，和山俊之。"智能传感器网络中基于脉冲神经网络的人体定位."2011 年 IEEE 信息结构空间中的机器人智能研讨会论文集，125–130。新泽西州皮斯卡塔韦:电气和电子工程师协会，2011 年。

奥哈罗兰、马丁、布莱恩·麦金莱、拉克尔·康塞桑、菲尔格尔·摩根、爱德华·琼斯和马丁·葛莱文。"介电异质性乳腺中乳腺癌分类的脉冲神经网络."电磁学研究进展 113(2011):413–428。

罗森布拉特，弗兰克。"感知器:一个感知和识别的自动机."项目第 85-460-1 号报告，康奈尔航空实验室，1957。

Schliebs，Stefan，Haza Nuzly Abdull Hamed 和 Nikola Kasabov。"基于储层的进化脉冲神经网络用于时空模式识别."神经信息处理(2011):160–168。

施里布、斯特凡和尼古拉·卡萨博夫。“进化中的脉冲神经网络——综述”进化的系统 4，第 2 期(2013):87–98。

Sinkevicius，s .，R. Simutis 和 V. Raudonis。"使用分级时间记忆算法监控人流量."电子与电气工程 115，第 9 期(2011):91–96。

索尔蒂奇、斯涅扎纳和尼古拉·卡萨博夫。"用秩序群体编码从进化脉冲神经网络中提取知识."国际神经系统杂志 20，第 6 期(2010):437–445。

坦克 D. W .和 J. J .霍普菲尔德。"简单的“神经”优化网络:一个 A/D 转换器，信号判定电路，和一个线性编程电路."IEEE 电路与系统汇刊 33，第 5 期(1986):533–541。

Thiruvarudchelvan、Vaenthan、James W. Crane 和 Terry R. Bossomaier。"用交替尖峰响应函数分析尖峰脉冲收敛."2013 年 IEEE 计算智能基础研讨会论文集，98–105。新泽西州皮斯卡塔韦:电气和电子工程师协会，2013 年。

Wysoski、Simei Gomes、Lubica Benuskova 和 Nikola Kasabov。"用于视觉模式识别的脉冲神经元网络的结构适应性在线学习."《人工神经网络——ICANN 2006:第 16 届国际会议论文集》,希腊雅典，2006 年 9 月 10 日至 14 日，由 Stefanos D. Kollias、Andreas Stafylopatis、wodzisaw Duch 和 Erkki Oja 编辑，61-70 页。柏林:施普林格，2006 年。

Wysoski、Simei Gomes、Lubica Benuskova 和 Nikola Kasabov。"基于脉冲神经网络的文本无关说话人认证."《人工神经网络——ICANN 2007:第 17 届国际会议记录》,葡萄牙波尔图，2007 年 9 月 9 日至 13 日，Joaquim Marques de Sá，Luís A. Alexandre，wodzisaw Duch 和 Danilo 曼迪克编辑，758-767。柏林:施普林格，2007 年。

支拓，夏，，王皓。"一个基于内容的图像检索系统使用多层次的时间记忆分类器."《第五届计算智能与设计国际研讨会论文集》, 438–441。新泽西州皮斯卡塔韦:电气和电子工程师协会，2012 年。

卓、文、、秦月明、于、。"使用 HTM 皮层学习算法的图像分类."《第 21 届国际模式识别会议论文集》, 2452–2455。新泽西州皮斯卡塔韦:电气和电子工程师协会，2012 年。