# 2.机器学习和知识发现

When you know a thing, to hold that you know it; and when you do not know a thing, to allow that you do not know it—this is knowledge. —Confucius, The Analects

近年来，数据挖掘领域取得了重大进展。由于其解决复杂问题的能力，数据挖掘已被应用于与工程、生物科学、社交媒体、医学和商业智能相关的各种领域。大多数应用程序的主要目标是描述复杂数据流中的模式。这些模式然后与知识发现和决策相结合。在互联网时代，信息收集和时空数据的动态分析是创新和开发更好的产品和流程的关键。当数据集庞大而复杂时，使用传统的统计方法处理和分析模式变得很困难。大数据是指收集的数据量如此之大，形式如此复杂和非结构化，以至于无法使用标准数据库管理系统(如 DBMS 和 RDBMS)进行处理。与大数据相关的新挑战不仅包括处理不断增长的数据量，还包括需要提取、转换、分析、存储和可视化的各种各样的复杂数据流。大数据分析使用推理统计从大量低信息密度的数据中得出与依赖关系、行为和预测相关的结论，这些数据会发生随机变化。这种系统被期望以一种格式对知识发现建模，当应用于广泛的情况时，该格式产生合理的答案。大数据的特征如下:

*   体积:产生大量的数据。在这么大的量中检测相关性和价值是一个挑战。
*   多样性:数据类型和来源的范围很广。
*   速度:数据生成速度快。及时做出反应可能要求很高。
*   可变性:由于季节性和事件驱动的高峰，数据流可能非常不一致且难以管理。
*   复杂性:数据需要链接、连接和关联，以推断非线性关系和因果关系。

现代技术进步使该行业得以进军大数据和大数据分析领域。价格合理的开源软件基础设施、更快的处理器、更便宜的存储、虚拟化、高吞吐量连接和非结构化数据管理工具的开发，以及云计算，为高质量的信息检索和更快的分析打开了大门，使企业能够降低开发具有定制产品的新产品所需的成本和时间。

可以集成大数据和强大的分析来提供有价值的服务，例如:

*   故障根本原因检测:由意外故障导致的计划外停机的成本可能高达数十亿美元。根本原因分析(RCA)确定了决定过去故障的位置、程度、时间和性质的因素，并学会了将可防止此类故障再次发生的行动、条件和行为联系起来。RCA 将故障缓解的被动方法转变为防患于未然的主动方法，避免不必要的升级。
*   动态优惠券系统:动态优惠券系统允许折扣优惠券以非常有选择性的方式交付，对应于最大化产品或服务提供商的战略利益的因素。根据现有的位置、对特定产品的评估兴趣、历史消费模式、动态定价、对购物地点的按时间顺序的访问、产品浏览模式以及过去优惠券的兑现，对调节向所选接收者递送优惠券的因素进行建模。这些因素中的每一个都作为竞争压力、转变行为、季节效应、外部因素和产品成熟度动态的函数进行加权和重新分析。优惠券是根据收件人的个人资料、背景和位置实时发送的。向大量移动接收者递送优惠券的速度、精确度和准确度是重要的考虑因素。
*   购物行为分析:一个产品制造商对理解其竞争对手产品在商场上的热图模式特别感兴趣。例如，一家大屏幕电视制造商希望确定买家对其他电视制造商提供的功能的兴趣。这只能通过评估潜在买家的活动和在商场上接近竞争对手产品的时间来分析。这种报告可以单独地、实时地或者以固定的时间间隔一起发送给制造商。这些报告可以提示制造商发送动态优惠券，以影响仍处于决策阶段的潜在买家，并帮助制造商改进、移除、保留或增加功能，这是通过买家对竞争对手产品的兴趣来衡量的。
*   检测欺诈行为:与保险、医疗保健、信用卡和身份盗窃相关的各种类型的欺诈给消费者和企业造成了数十亿美元的损失。大数据和智能分析为开发实时解决方案铺平了道路，以识别欺诈并在欺诈发生前加以防范。智能分析可生成模型，验证与消费行为、地理位置、高峰活动和保险索赔相关的模式。如果某个模式无法通过验证，就会启动纠正、预防或惩罚措施。此类行动的准确性、精确性和速度对于成功隔离欺诈行为至关重要。例如，每个事务可以使用一个或多个模型实时评估多达 500 个属性。
*   数据中心的工作负载资源调整和选择:在云服务管理环境中，服务级别协议(SLA)定义了在由计算资源池组成的给定服务托管环境中管理性能损失的服务质量(QoS)预期。通常，服务器系统中资源相互依赖的复杂性会导致次优行为，从而导致性能损失。一个表现良好的模型可以预测需求模式，并及时以优化的方式对动态压力做出主动反应。动态表征方法可以合成自校正工作负载指纹码本，该码本有助于相位预测，以通过主动工作负载分配和负载平衡来实现连续调整。换句话说，码本表征某些特征，这些特征被不断地重新评估以重塑工作负载行为，从而适应与预期输出的偏差。然而，有可能的是，码本中的最新模型没有经过更新的或未识别的模式。一个新的工作负载托管在一个计算节点(在数千个潜在节点中)上，其方式不仅可以减少热点，还可以通过降低资源瓶颈来提高性能。导致工作负载实时最佳托管的分析速度对于工作负载负载分配和平衡的成功至关重要。

## 知识发现

知识抽取从结构化和非结构化来源收集信息以构建知识数据库，用于从潜在的大型和语义模糊的数据集中识别有意义和有用的模式。模糊数据集是其元素具有隶属度的集合。隶属度由取值在 0 和 1 之间的隶属函数定义。

提取的知识与源数据一起被重用，以产生模式的枚举，这些模式被添加回知识库。知识发现的过程包括对大量数据进行有计划的探索，寻找可以被列举为知识的模式。获得的知识以模型的形式呈现，必要时可以对其进行特定的查询。知识发现将计算机科学和机器学习(如数据库和算法)的概念与统计学的概念结合起来，以解决面向用户的查询和问题。知识可以用不同的形式描述，比如参与者的类别、属性关联模型和依赖关系。大数据中的知识发现使用为分类、聚类、降维、协作过滤以及可扩展的分布式系统而设计的核心机器算法。本章讨论了当要处理的数据集对于单台机器来说非常大时有用的机器学习算法的类别。

### 分类

分类是开发能够复制人类决策的预测分析的核心。分类算法适用于边界明确的问题，在这些问题中，输入遵循一组特定的属性，并且输出是分类的。一般来说，分类过程开发了一个经验档案，通过将它们与以前观察到的模式进行匹配，对新的输入进行评估。如果模式可以匹配，则输入与预定义的预测行为模式相关联。如果某个模式无法匹配，它将被隔离以进行进一步评估，从而确定它是未被发现的有效模式还是异常模式。基于机器的分类算法遵循监督学习技术，其中算法通过精确决策的示例(也称为训练集)进行学习，使用精心准备的输入。分类中涉及的两个主要步骤是合成模型、使用学习算法和使用模型对新数据进行分类。

### 使聚集

聚类是一个知识发现过程，它根据相似的属性(或特征)对给定集合中的项目进行分组。相对于属于不同聚类的成员，同一聚类的成员共享相似的特征。通常，聚类涉及试错的迭代算法，该算法基于相似性(或不相似性)的假设进行操作，并在满足终止标准时停止。挑战在于找到一个函数，用数值来衡量两个项目(或数据点)之间的相似程度。聚类的参数(如聚类算法、距离函数、密度阈值和聚类数)取决于应用程序和单个数据集。

### 降维

降维是通过特征选择和特征提取来减少随机变量的过程。维数减少允许更短的训练时间和增强的泛化能力，并减少过拟合。特征选择是通过消除冗余或不相关的特征来合成用于模型构建的原始变量的子集的过程。相比之下，特征提取是通过组合属性将高维空间转换到更少维空间的过程。

### 协同过滤

协作过滤(CF)是在多个数据源之间使用协作方法过滤信息或模式的过程。CF 通过收集许多具有相似兴趣的用户的偏好并基于这些偏好进行推荐来探索感兴趣的领域。尽管数据非常稀疏，用户和项目的数量不断增加，同义词，数据噪声和隐私问题，CF 算法仍有望在短时间内做出令人满意的推荐。

机器学习基于从训练数据(模型)中学习到的既定属性来执行预测分析。机器学习通过将新信息与以模式形式存在的历史信息进行匹配，辅助探索有用的知识或以前未知的知识。这些模式用于过滤出新的信息或模式。一旦这些新信息被一组关联的行为模式验证，它就会被集成到现有的知识数据库中。新的信息也可以通过充当额外的训练数据来校正现有的模型。以下部分着眼于知识发现中采用的各种机器学习算法，与聚类、分类、维度减少和协同过滤相关。

## 机器学习:分类算法

### 逻辑回归

逻辑回归是预测事件发生概率的概率统计分类模型。逻辑回归对分类因变量 X 和二分分类结果或特征 y 之间的关系进行建模。逻辑函数可以表示为

![A978-1-4302-5990-9_2_Figa_HTML.jpg](A978-1-4302-5990-9_2_Figa_HTML.jpg)

逻辑函数可以被重写并转换为逻辑函数的逆函数——称为 logit 或 log-odds——这是生成逻辑回归系数的关键，

![A978-1-4302-5990-9_2_Figb_HTML.jpg](A978-1-4302-5990-9_2_Figb_HTML.jpg)

如图 [2-1](#Fig1) 所示，逻辑函数可以接收负无穷大和正无穷大之间的输入值范围(β <sub>0</sub> +β <sub>1</sub> X)，输出(P(Y|X)被约束为 0 到 1 之间的值。

![A978-1-4302-5990-9_2_Fig1_HTML.gif](A978-1-4302-5990-9_2_Fig1_HTML.gif)

图 2-1。

The logistic function

P(Y|X)的 logit 变换为线性回归提供了一个动态范围，并且可以转换回赔率。逻辑回归法使用回归系数β <sub>0</sub> 和β <sub>1</sub> 拟合回归曲线，如等式 2-1 所示，其中输出响应是一个二元(二分)变量，X 是数值。因为逻辑函数曲线是非线性的，所以使用 logit 变换(见等式 2-2)来执行线性回归，其中 P(Y |X)是给定 X 值的成功概率(Y)。使用广义线性模型，估计的逻辑回归方程可以表示为

![A978-1-4302-5990-9_2_Figc_HTML.jpg](A978-1-4302-5990-9_2_Figc_HTML.jpg)

系数β <sub>0</sub> 和β <sub>k</sub> (k = 1，2，...，n)被估计，使用最大似然估计(MLE)来模拟对于给定的 X <sub>k</sub> (k = 1，2，...，n)。

逻辑回归广泛应用于以二进制格式表示结果的领域。例如，要根据身体质量指数(身体质量指数)预测血液胆固醇，您可以使用线性回归，因为结果是连续的。如果你需要根据身体质量指数预测患糖尿病的几率，你可以使用逻辑回归，因为结果是二元的。

### 随机森林

随机森林(Breiman 2001)是一种用于分类的集成学习方法，其中“弱学习者”使用大量去相关决策树(随机森林)协作形成“强学习者”。然而，random forest 并没有基于单个深树的输出来开发解决方案，而是聚合了多个浅树的输出，形成了一个额外的装袋层。Bagging 使用独立的连续树，通过引导数据集的样本来构建 n 个预测器。n 个预测值被组合以通过平均来解决分类或估计问题。虽然单个分类器是弱学习器，但是所有分类器组合起来形成强学习器。单个决策树会经历高方差和高偏差，而随机森林会对多个决策树进行平均以提高估计性能。用系综术语来说，决策树代表一个弱分类器。术语“森林”表示使用许多决策树来做出分类决策。

随机森林算法可以总结如下:

To construct B trees, select n bootstrap samples from the original dataset.   For each bootstrap sample, grow a classification or regression tree.   At each node of the tree: – m predictor variables (or subset of features) are selected at random from all the predictor variables (random subspace). – The predictor variable that provides the best split performs the binary split on that node. – The next node randomly selects another set of m variables from all predictor variables and performs the preceding step.   Given a new dataset to be classified, take the majority vote of all the B subtrees.  

通过对所有树进行平均，可以减少最终估计的方差。随机森林提供了良好的准确性，并在大型数据集上高效运行。这是估计缺失数据的有效方法，即使大部分数据缺失，也能保持准确性。此外，随机森林可以估计分类变量的相对重要性。

### 隐马尔可夫模型

隐马尔可夫模型(HMM)是一个双随机过程，其中被建模的系统是一个具有不可观测(隐藏)状态的马尔可夫过程。虽然潜在的随机过程是隐藏的，不能直接观察到，但可以通过产生观察符号序列的另一组随机过程看到。在传统的马尔可夫模型中，状态对于观察者来说是可见的，并且状态转移使用转移概率来参数化。每个状态都有一个输出排放(观察变量)的概率分布。基于 HMM 的方法将系统观察和状态转换相关联，以预测最可能的状态序列。HMM 的状态只能从观察到的辐射中推断出来，因此使用了术语“隐藏”。HMM 生成的输出发射序列用于估计状态序列。hmm 是生成模型，其中对观察和隐藏状态的联合分布进行建模。要定义隐马尔可夫模型，必须指定以下属性(见图 [2-2](#Fig2) ):

![A978-1-4302-5990-9_2_Fig2_HTML.gif](A978-1-4302-5990-9_2_Fig2_HTML.gif)

图 2-2。

Attributes of an HMM

*   状态集:{S <sub>1</sub> ，S <sub>2</sub> ...，S <sub>n</sub> }
*   状态序列:Q=q <sub>1</sub> ，q <sub>2</sub> ，...，q <sub>t</sub>
*   马尔可夫链属性:![A978-1-4302-5990-9_2_Figd_HTML.jpg](A978-1-4302-5990-9_2_Figd_HTML.jpg)
*   一组观察值:O={o <sub>1</sub> ，o <sub>2</sub> ，o <sub>3</sub> ，...，o <sub>M</sub> }
*   转移概率矩阵:![A978-1-4302-5990-9_2_Fige_HTML.jpg](A978-1-4302-5990-9_2_Fige_HTML.jpg)
*   排放概率矩阵:![A978-1-4302-5990-9_2_Figf_HTML.jpg](A978-1-4302-5990-9_2_Figf_HTML.jpg)
*   初始概率矩阵:![A978-1-4302-5990-9_2_Figg_HTML.jpg](A978-1-4302-5990-9_2_Figg_HTML.jpg)
*   嗯:M = (A，B，π)

HMMs 解决的三个基本问题可以总结如下:

*   模型评估:评估给定 HMM (M=(A，B，π))的一系列观察值的似然性。
*   路径解码:对于给定的观察序列和 HMM 模型 M=(A，B，π)，评估模型状态(Q)(隐藏状态)的最优序列。
*   模型训练:确定最能解释观察到的信号的模型参数集。

hmm 尤其以其在时间模式识别中的应用而闻名，例如语音、手写、手势识别、词性标注、乐谱跟踪、局部放电和生物信息学。有关 HMM 的更多详细信息，请参见第 5 章。

### 多层感知器

多层感知器(MLP)是简单神经元的前馈网络，将输入数据集映射到一组输出。MLP 由多层完全由有向图连接的节点组成，其中每个节点(除输入节点外)是一个具有非线性激活函数的神经元。

MLP 的基本组成部分是神经元。在 MLP 中，一对神经元使用加权边连接在两个相邻层中。如图 [2-3](#Fig3) 所示，一个 MLP 至少包括三层神经元，包括一个输入层、一个或多个隐藏层和一个输出层。输入神经元的数量取决于输入特征的维数；输出神经元的数量由类的数量决定。隐藏层的数量和每个隐藏层中神经元的数量取决于要解决的问题的类型。神经元少导致学习效率低；大量的神经元导致低效的泛化。MLP 使用一种称为反向传播的监督学习技术来训练网络。在其简单实例中，感知器通过非线性激活函数处理加权实值输入的线性组合来计算输出 y，

![A978-1-4302-5990-9_2_Fig3_HTML.gif](A978-1-4302-5990-9_2_Fig3_HTML.gif)

图 2-3。

The MLP is fed the input features to the input layer and gets the result from the output layer; the results are calculated in a feedforward approach from the input layer to the output layer

![A978-1-4302-5990-9_2_Figh_HTML.jpg](A978-1-4302-5990-9_2_Figh_HTML.jpg)

其中 w 表示权重向量，x 是输入向量，b 是偏差，ϕ是激活函数。通常，MLP 系统选择逻辑 sigmoid 函数 1/(1+e<sup>–x</sup>)或双曲正切 tanh(x)作为激活函数。这些函数提供了统计上的便利，因为它们在原点附近是线性的，当远离原点时会很快饱和。

MLP 学习过程调整隐藏层的权重，从而减小输出误差。从随机权重开始，MLP 通过网络前馈输入模式信号，并从输出开始反向传播误差信号。反向传播误差信号由实际值(O <sub>n</sub> (t))和期望值(T <sub>n</sub> )之差组成。误差函数可以概括为

![A978-1-4302-5990-9_2_Figi_HTML.jpg](A978-1-4302-5990-9_2_Figi_HTML.jpg)

学习过程的目标是最小化误差函数。为了找到误差函数的最小值，相对于权重矩阵对其进行微分。该学习算法包括以下步骤:

Initialize random weights within the interval [1, –1].   Send an input pattern to the network.   Calculate the output of the network.   For each node n in the output layer: Calculate the error on output node n: E(O <sub>n</sub> (t))=T <sub>n</sub> –O <sub>n</sub> (t).   Add E(O <sub>n</sub> (t)) to all the weights that connect to node n.     Repeat step 2.  

为了影响收敛速度，从而减小权重经历自适应变化的步长，使用了学习参数η (< 1)。连接到第 j 个输出的第 I 个权重可以通过以下规则进行更新:

![A978-1-4302-5990-9_2_Figj_HTML.jpg](A978-1-4302-5990-9_2_Figj_HTML.jpg)

等式 2-6 表示迭代权重自适应，其中迭代(t + 1)的输出误差的一部分被添加到来自迭代 t 的现有权重

MLPs 通常用于监督学习模式识别过程。由于深度学习的成功，人们对 MLP 反向传播网络重新产生了兴趣。深度学习是一种使用多个隐藏层来有效训练 MLP 的方法。随着硅技术的现代进步，深度学习正在被开发，以在深度架构可以表示高度变化的功能的领域中释放巨大的大数据分析潜力。

## 机器学习:聚类算法

### k 均值聚类

k 均值聚类是矢量量化的无监督学习算法，将 n 个观察值划分为 k 个聚类。该算法定义了 k 个质心，作为它们各自聚类的原型。当用特定的距离度量测量时，每个对象被分配到具有最近质心的簇。当所有对象都被应用于 k 个聚类之一时，将对象分配给聚类的步骤完成。通过基于先前的 S={S <sub>1</sub> ，S <sub>1</sub> ，...，S <sub>k</sub> }分配，并将对象重新分配给最近的新质心。该过程继续，直到没有任何 k 簇的质心移动。通常，k-均值聚类算法通过最小化对象数据和聚类质心之间的距离的平方和，根据对象的特征将对象分类成 k 个组(或聚类)。

对于给定的一组 d 维观测向量(x <sub>1</sub> ，x <sub>2</sub> ，...，x <sub>n</sub> ，k-means 聚类将 n 个观测值划分为 k(≤n)个聚类集，以最小化平方和，

![A978-1-4302-5990-9_2_Figk_HTML.jpg](A978-1-4302-5990-9_2_Figk_HTML.jpg)

其中μ <sub>i</sub> 是 S <sub>i</sub> 中的点的平均值。

k-means 聚类算法易于在大型数据集上实现。它在市场细分、计算机视觉、分析应用程序和工作负载、光学字符识别和语音合成等领域有许多用途。该算法通常用作其他算法的预处理步骤，以便找到初始配置。

### 模糊 k 均值(模糊 c 均值)

模糊 k 均值(也称为模糊 c 均值[FCM])(Dunn 1973；Bezdek 1981)是合成软聚类的 k-means 算法的扩展，在软聚类中，一个对象可以以一定的概率属于多个聚类。这种算法在将数据对象分配给集群以及允许数据对象在多个相邻集群中保持部分成员资格方面提供了更大的灵活性。FCM 使用范围[1，n]中的模糊化参数 m，它决定了聚类中的模糊程度。m=1 表示清晰的聚类，而 m>1 表示决策空间中数据对象的模糊程度更高。FCM 算法基于目标函数的最小化

![A978-1-4302-5990-9_2_Figl_HTML.jpg](A978-1-4302-5990-9_2_Figl_HTML.jpg)

其中 x 是 d 维数据对象，c <sub>j</sub> 是聚类 j 的 d 维质心(见等式 2-10)，w <sub>k</sub> (x)是 x 在聚类 k 中的隶属度，取决于模糊化参数 m，模糊化参数 m 控制与最接近质心一致的权重:

![A978-1-4302-5990-9_2_Figm_HTML.jpg](A978-1-4302-5990-9_2_Figm_HTML.jpg)

利用 FCM，第 k 个聚类的 d 维形心(c <sub>k</sub> )是所有点的平均值，通过它们对该聚类的隶属度进行加权:

![A978-1-4302-5990-9_2_Fign_HTML.jpg](A978-1-4302-5990-9_2_Fign_HTML.jpg)

c 均值聚类算法综合了聚类中心和数据对象被分配给它们的程度。这并没有转化为硬性的隶属函数。FCM 在图像处理中用于聚类图像中的对象。

### 流式 k 均值算法

流式 k-means 是一个两步算法，由流式步骤和球形 k-means 步骤组成。流式传输步骤在一次通过中遍历大小为 n 的数据对象，并生成最佳数量的质心，这相当于 klog(n)个簇，其中 k 是簇的预期数量。这些聚类的属性被传递到球 k-means 步骤，该步骤将聚类的数量减少到 k。

#### 流动步骤

流式步进算法一次遍历一个数据对象，并决定是将该数据对象添加到现有的集群还是创建一个新的集群。如果聚类的质心与数据点之间的距离小于距离截止阈值，则该算法会将数据添加到现有的聚类中，或者以 d/(distancecutoff)的概率创建一个新的聚类。如果距离超过截止值，该算法将创建一个具有新质心的新聚类。随着更多数据对象被处理，现有聚类的质心可能改变它们的位置。该过程继续添加新的聚类，直到现有聚类的数量达到聚类截止极限。可以通过增加距离截止阈值来减少聚类的数量。这一步主要用于降维。这一步的输出是多个聚类形式的精简数据集，这些聚类是大量原始数据的代理。

#### 球 K-均值步

ball k-means 算法消耗流步骤(X =质心集> k)的输出，并执行多次独立运行，以通过选择最佳解决方案来合成 k 个聚类。每次运行使用播种机制选择 k 个质心，并迭代运行球 k-均值算法来改进解决方案。

播种过程可以调用 k-means++算法来优化 k 个簇的分布。k-means++播种算法总结如下:

Choose center c <sub>1</sub> uniformly at random from X.   Select a new center c <sub>i</sub> by choosing xϵX with probability, P(x), and add it to ![A978-1-4302-5990-9_2_Figo_HTML.jpg](A978-1-4302-5990-9_2_Figo_HTML.jpg),  

![A978-1-4302-5990-9_2_Figp_HTML.jpg](A978-1-4302-5990-9_2_Figp_HTML.jpg)

其中 D(x)是 x 和已经选定的最近中心之间的距离。

Repeat step 2 until k centers ![A978-1-4302-5990-9_2_Figq_HTML.jpg](A978-1-4302-5990-9_2_Figq_HTML.jpg) are selected.   Randomly pick two centers ![A978-1-4302-5990-9_2_Figr_HTML.jpg](A978-1-4302-5990-9_2_Figr_HTML.jpg) with probability proportional to norm ![A978-1-4302-5990-9_2_Figs_HTML.jpg](A978-1-4302-5990-9_2_Figs_HTML.jpg).   For each ![A978-1-4302-5990-9_2_Figt_HTML.jpg](A978-1-4302-5990-9_2_Figt_HTML.jpg), create a ball of radius ![A978-1-4302-5990-9_2_Figu_HTML.jpg](A978-1-4302-5990-9_2_Figu_HTML.jpg) around it.   Recompute the new centroids ![A978-1-4302-5990-9_2_Figv_HTML.jpg](A978-1-4302-5990-9_2_Figv_HTML.jpg) by using the elements of ![A978-1-4302-5990-9_2_Figw_HTML.jpg](A978-1-4302-5990-9_2_Figw_HTML.jpg) contained within the ball.  

该算法在具有大量数据对象的应用程序中特别有用。该算法通过采用流式操作并用由原始数据的 k 个 log(n)形心组成的简化代理数据替换该数据来降低原始数据集的维数。减少的数据作为球 k-均值算法的输入，该算法为它们各自的聚类合成和细化 k 个质心。

## 机器学习:降维

机器学习通过大量的特征来训练大多数回归或分类问题。这增加了复杂性，提高了计算要求，并增加了收敛到解所需的时间。减轻这些问题的一个有用的方法是通过合成低维空间来减少原始特征的维空间。在这个新的低维空间中，最重要的特征被保留，特征之间隐藏的相关性被暴露，不重要的特征被丢弃。最简单、最直接、最少监督的特征约简方法之一涉及矩阵分解的变体:奇异值分解、特征分解和非负矩阵分解。以下部分考虑了统计降维中常用的一些方法。

### 奇异值分解

奇异值分解(SVD)执行矩阵分析以合成高维矩阵的低维表示。SVD 有助于消除矩阵表示中不太重要的部分，从而得到具有所需维数的近似表示。这有助于创建与原始矩阵非常相似的矩阵的较小表示。由于以下特征，奇异值分解在维数减少中是有用的:

*   SVD 将相关变量转换成一组不相关的变量，从而揭示数据项之间的对应关系。
*   SVD 识别数据点表现出最大变化的维度。

一旦确定了具有明显差异的点，就可以用更少的维度来逼近原始数据点。您可以定义阈值，低于该阈值的变化可以被忽略，从而导致数据集大大减少，而不会降低与数据点内的固有关系和兴趣相关的信息的质量。

如果 M 是一个 m × n 矩阵，那么你可以把它分解成三个矩阵 U，∑，V <sup>T</sup> 的乘积，具有以下特征:

*   u 是列正交矩阵。U 的列是 MM <sup>T</sup> 的正交特征向量。
*   V <sup>T</sup> 是正交矩阵 V 的转置，V 的列是 M <sup>T</sup> M 的正交特征向量
*   ∑是对角矩阵，其中除对角线外的所有元素都是 0。∑包含 U 或 V 的特征值的平方根，按降序排列。

在其精确形式中，M 可以被重写为

![A978-1-4302-5990-9_2_Figx_HTML.jpg](A978-1-4302-5990-9_2_Figx_HTML.jpg)

在降维过程中，您合成 U 和 V，使它们包含原始数据中包含的元素，按照变化的降序排列。您可以删除表示没有有意义变化的尺寸的元素。这可以通过将最小特征值设置为 0 来实现。等式 2-11 可以改写成它的最佳 l 阶近似形式

![A978-1-4302-5990-9_2_Figy_HTML.jpg](A978-1-4302-5990-9_2_Figy_HTML.jpg)

其中 u <sub>i</sub> 和 v <sub>i</sub> 分别是 U 和 V 的第 I 列，λ <sub>i</sub> 是对角矩阵∑的第 I 个元素。

### 主成分分析

当空间中有一大群点时，用来表示这些点的坐标和轴是任意的。相对于所选轴的方向，这些点具有一定的方差，表示在该方向上平均值周围的分布。在二维系统中，模型受到第二个轴与第一个轴的垂直度的约束。但是，在三维和更高的情况下，您可以将第 n 个轴定位为垂直于由任意两个轴构成的平面。模型受第一个轴的位置约束，该轴位于方差最大的方向。这产生了一个新的特征空间，该特征空间将点群压缩到高方差的轴中。您可以选择方差较大的轴，并删除方差较小的轴。图 [2-4](#Fig4) 展示了新的特征空间，从 160 个特征减少到 59 个分量(轴)。相对于其他组件，每个组件都与某个百分比的方差相关联。第一个分量的方差最大，其次是第二个分量，依此类推。

![A978-1-4302-5990-9_2_Fig4_HTML.gif](A978-1-4302-5990-9_2_Fig4_HTML.gif)

图 2-4。

The percentage of variance of a principal component transform of a dataset with 160 features reduced to 59 components

主成分分析(PCA)是一种广泛使用的分析技术，它识别模式以降低数据集的维度，而不会显著丢失信息。PCA 的目标是将高维特征空间投影到一个更小的子集，以降低计算成本。PCA 计算称为主成分(PCs)的新特征，主成分是在更大可变性方向上投影的原始特征的不相关线性组合。关键是将一组特征映射到矩阵 M 中，并合成 MM <sup>T</sup> 或 M <sup>T</sup> M 的特征值和特征向量。特征向量有助于简化问题的解决方案，这些问题可以通过拉伸、压缩或翻转使用沿轴的线性变换来建模。特征值提供了发生这种变换的因素(特征向量的长度和大小)。在新的特征空间中选择具有较大特征值的特征向量，因为对于数据分布，它们比具有较低特征值的特征向量包含更多的信息。与下一个 PC(相对于第一个 PC 不相关)相比，第一个 PC 具有最大可能的方差(即，最大的特征值)，这是在与第一个分量正交的约束下计算的。本质上，第 I 个 PC 是与所有先前 PC 不相关的最大方差的线性组合。

PCA 包括以下步骤:

Compute the d-dimensional mean of the original dataset.   Compute the covariance matrix of the features.   Compute the eigenvectors and eigenvalues of the covariance matrix.   Sort the eigenvectors by decreasing eigenvalue.   Choose k eigenvectors with the largest eigenvalues.  

特征向量值代表每个变量对 PC 轴的贡献。PC 在 m 维点中的最大方差方向上定向。

PCA 是发现新的、信息丰富的、不相关的特征的最广泛使用的多元方法之一；它通过拒绝低方差特征来降低维数，并有助于降低分类和回归分析的计算要求。

### Lanczos 算法

Lanczos 算法是一种低成本的特征分解技术，与截断 SVD 相同，只是它不显式计算矩阵的奇异值/向量。Lanczos 算法使用少量的 Lanczos 向量，这些向量是 M <sup>T</sup> M 或 MM <sup>T</sup> 的特征向量，其中 M 是对称的 n × n 矩阵。

Lanczos 从播种任意非零向量 x <sub>0</sub> 开始，其基数等于矩阵 M 的列数。算法的第 M(M<T5】n)步将矩阵 M 转换为三对角矩阵 T <sub>mm</sub> 。迭代过程可总结如下:

#### 初始化

![A978-1-4302-5990-9_2_Figz_HTML.jpg](A978-1-4302-5990-9_2_Figz_HTML.jpg)

![A978-1-4302-5990-9_2_Figaa_HTML.jpg](A978-1-4302-5990-9_2_Figaa_HTML.jpg)

![A978-1-4302-5990-9_2_Figab_HTML.jpg](A978-1-4302-5990-9_2_Figab_HTML.jpg)

#### 算法

对于![A978-1-4302-5990-9_2_Figac_HTML.jpg](A978-1-4302-5990-9_2_Figac_HTML.jpg)

![A978-1-4302-5990-9_2_Figad_HTML.jpg](A978-1-4302-5990-9_2_Figad_HTML.jpg)

![A978-1-4302-5990-9_2_Figae_HTML.jpg](A978-1-4302-5990-9_2_Figae_HTML.jpg)

![A978-1-4302-5990-9_2_Figaf_HTML.jpg](A978-1-4302-5990-9_2_Figaf_HTML.jpg)

![A978-1-4302-5990-9_2_Figag_HTML.jpg](A978-1-4302-5990-9_2_Figag_HTML.jpg)

如果β <sub>i</sub> =0，则停止

![A978-1-4302-5990-9_2_Figah_HTML.jpg](A978-1-4302-5990-9_2_Figah_HTML.jpg)

结束

m 次迭代完成后，你得到α <sub>i</sub> 和β <sub>i</sub> ，分别是对称三对角矩阵 T <sub>mm</sub> 的对角和次对角项。得到的三对角矩阵正交类似于![A978-1-4302-5990-9_2_Figai_HTML.jpg](A978-1-4302-5990-9_2_Figai_HTML.jpg):

![A978-1-4302-5990-9_2_Figaj_HTML.jpg](A978-1-4302-5990-9_2_Figaj_HTML.jpg)

对称三对角矩阵表示给定矩阵在由相应的 Lanczos 向量集合 V <sub>m</sub> 所跨越的子空间上的投影。这些矩阵的特征值就是原矩阵的映射子空间的特征值。Lanczos 迭代本身不直接产生特征值或特征向量；相反，它们产生一个三对角矩阵(见方程 2-13)，其特征值和特征向量由另一种方法(如 QR 算法)计算，以产生 Ritz 值和向量。对于特征值，如果 Lanczos 迭代的次数比 k 大，可以计算 T <sub>mm</sub> 的 k 个最小或最大特征值。如此生成的 Lanczos 向量 v <sub>i</sub> 然后构造变换矩阵，

![A978-1-4302-5990-9_2_Figak_HTML.jpg](A978-1-4302-5990-9_2_Figak_HTML.jpg)

其可用于生成里兹特征向量(V <sub>m</sub> u <sub>m</sub> )，原始矩阵的近似特征向量。

## 机器学习:协同过滤

协同过滤(CF 由推荐系统使用，其目标是基于集体用户体验(协作)预测用户对给定项目的兴趣。主要目标是匹配具有相似兴趣的人，以生成个性化推荐。例如，假设有 M 个项目和 N 个用户。这给了我们一个 M × N 用户-项目矩阵 X，其中 x <sub>m，n</sub> 代表第 n <sup>个</sup>用户对项目 M 的推荐。下面的部分讨论了推荐系统中使用的一些 CF 系统。

### 基于用户的协同过滤

基于用户的 CF 根据来自相似用户简档的集体评分来预测用户对某个项目的兴趣。用户-项目矩阵可以写成

![A978-1-4302-5990-9_2_Figal_HTML.jpg](A978-1-4302-5990-9_2_Figal_HTML.jpg)

![A978-1-4302-5990-9_2_Figam_HTML.jpg](A978-1-4302-5990-9_2_Figam_HTML.jpg)

基于用户的 CF 的第一步是评估用户之间的相似性，并根据其最近邻进行排列。例如，为了评估两个用户之间的相似性，您可以使用余弦相似性矩阵 u <sub>n</sub> ，u <sub>a</sub> :

![A978-1-4302-5990-9_2_Figan_HTML.jpg](A978-1-4302-5990-9_2_Figan_HTML.jpg)

最后，测试用户 a 对测试项目 m 的预测评级![A978-1-4302-5990-9_2_Figao_HTML.jpg](A978-1-4302-5990-9_2_Figao_HTML.jpg)计算如下

![A978-1-4302-5990-9_2_Figap_HTML.jpg](A978-1-4302-5990-9_2_Figap_HTML.jpg)

其中![A978-1-4302-5990-9_2_Figaq_HTML.jpg](A978-1-4302-5990-9_2_Figaq_HTML.jpg)和![A978-1-4302-5990-9_2_Figar_HTML.jpg](A978-1-4302-5990-9_2_Figar_HTML.jpg)分别表示用户 n 和 a 的平均评分。从等式 2-14 和 2-15 可以看出，处理 CF 是一个计算密集型工作函数，可能需要大的资源池和更快的计算机器。因此，建议您利用 Hadoop 平台来获得更好的性能和可扩展性。

### 基于项目的协同过滤

基于项目的 CF 计算项目之间的相似性并选择最佳匹配。这个想法是隔离已经评论了这两个项目的用户，然后计算它们之间的相似性。用户-项目矩阵表示为

![A978-1-4302-5990-9_2_Figas_HTML.jpg](A978-1-4302-5990-9_2_Figas_HTML.jpg)

![A978-1-4302-5990-9_2_Figat_HTML.jpg](A978-1-4302-5990-9_2_Figat_HTML.jpg)

其中 i <sub>m</sub> 对应于所有用户 m 对项目的评级，这导致基于项目的推荐算法。

基于项目的 CF 的第一步是评估项目之间的相似性，并根据它们最近的邻居来排列它们。例如，您可以使用余弦相似性矩阵来评估两个项目 i <sub>m</sub> ，i <sub>b</sub> 之间的相似性。为了在计算相似性时消除用户之间的评级尺度差异，余弦相似性通过从每个共同评级对中减去用户的平均评级![A978-1-4302-5990-9_2_Figau_HTML.jpg](A978-1-4302-5990-9_2_Figau_HTML.jpg) (Sarwar 2001)来调整:

![A978-1-4302-5990-9_2_Figav_HTML.jpg](A978-1-4302-5990-9_2_Figav_HTML.jpg)

最后，测试用户 a 对测试项目 m 的预测评级![A978-1-4302-5990-9_2_Figaw_HTML.jpg](A978-1-4302-5990-9_2_Figaw_HTML.jpg)计算如下

![A978-1-4302-5990-9_2_Figax_HTML.jpg](A978-1-4302-5990-9_2_Figax_HTML.jpg)

用户对项目的评级可以通过对同一用户评估的类似项目的评级进行平均来估计。

### 加权λ正则化交替最小二乘

加权λ正则化交替最小二乘(ALS-WR)算法将用户-项目矩阵分解为用户-因素矩阵和项目-因素矩阵。该算法致力于揭示使观察到的用户项目评级合理化的潜在因素，并搜索最佳因素权重以最小化预测和实际评级之间的最小平方(周 2008)。

如果您有多个用户和项目，您将需要学习在特征空间中表示每个项目和每个用户的特征向量。目标是揭示将每个用户 u 与用户因素向量![A978-1-4302-5990-9_2_Figay_HTML.jpg](A978-1-4302-5990-9_2_Figay_HTML.jpg)相关联，并将每个项目 I 与项目因素向量![A978-1-4302-5990-9_2_Figaz_HTML.jpg](A978-1-4302-5990-9_2_Figaz_HTML.jpg)相关联的特征。评级由用户因素向量和项目因素向量的内点积![A978-1-4302-5990-9_2_Figba_HTML.jpg](A978-1-4302-5990-9_2_Figba_HTML.jpg)来描述。其思想是执行矩阵分解，这样用户和项目可以被映射到共同的潜在因素，从而可以直接比较它们。因为评级矩阵是稀疏的并且没有完全定义，所以只能使用已知的评级来进行因子分解。解决方案的质量不仅根据观察到的数据来衡量，而且根据未观察到的数据的概括来衡量。您必须找到一组用户和项目特征向量，以最小化以下成本函数:

![A978-1-4302-5990-9_2_Figbb_HTML.jpg](A978-1-4302-5990-9_2_Figbb_HTML.jpg)

其中![A978-1-4302-5990-9_2_Figbc_HTML.jpg](A978-1-4302-5990-9_2_Figbc_HTML.jpg)和![A978-1-4302-5990-9_2_Figbd_HTML.jpg](A978-1-4302-5990-9_2_Figbd_HTML.jpg)分别代表用户 u 和项目 I 的评分数。正则项λ(...)避免过拟合训练数据。参数λ取决于数据，并通过数据集中的交叉验证进行调整，以实现更好的泛化。因为搜索空间非常大(多个用户和项目)，它阻止了传统的直接优化技术的应用，例如随机梯度下降。

当用户因子或项目因子固定时，成本函数采用二次形式，这允许计算全局最小值。这反过来允许 ALS 优化，其中用户因素和项目因素通过固定彼此交替重新计算。该算法是为大数据集的大规模 CF 而设计的。

## 机器学习:相似矩阵

相似性矩阵对数据点之间的相似性进行评分。相似矩阵与它们的对应物密切相关:距离矩阵和替换矩阵。下面几节介绍一些常用的相似性计算方法。

### 皮尔逊相关系数

皮尔逊相关度量两个变量之间的线性相关性。皮尔逊相关系数是两个变量(X 和 Y)的协方差除以其标准差的乘积:

![A978-1-4302-5990-9_2_Figbe_HTML.jpg](A978-1-4302-5990-9_2_Figbe_HTML.jpg)

皮尔逊相关系数范围为 1 至 1。值 1 表示 X 和 Y 之间存在完美的线性关系，其中 X 的数据可变性跟踪 Y 的数据可变性。值 1 表示 X 和 Y 之间的关系相反，即 Y 的数据可变性与 X 相反。值 0 表示变量 X 和 Y 之间缺乏线性相关性。

虽然皮尔逊系数反映了线性关系的强度，但它对极值和异常值高度敏感。如果两个变量具有强曲线关系而不是强线性关系，则低关系强度可能会产生误导。如果 X 和 Y 没有按照它们的全范围进行分析，该系数也可能会产生误导。

### 斯皮尔曼等级相关系数

Spearman 相关系数对成对变量 X 和 y 之间的单调关系强度进行统计分析。Spearman 相关计算成对变量的等级值的 Pearson 相关。等级(从低到高)是通过将等级 1 分配给最低值、2 分配给下一个最低值等等来获得的，因此

![A978-1-4302-5990-9_2_Figbf_HTML.jpg](A978-1-4302-5990-9_2_Figbf_HTML.jpg)

其中 n 是样本大小，d 是变量对的统计等级之间的距离，由下式给出

![A978-1-4302-5990-9_2_Figbg_HTML.jpg](A978-1-4302-5990-9_2_Figbg_HTML.jpg)

Spearman 相关系数的符号表示因变量和自变量之间关联的方向。如果因变量 Y 与自变量 X 的增加(或减少)方向相同，则系数为正。如果因变量 Y 相对于自变量 X 的增加(或减少)方向相反，则系数为负。Spearman 相关性为 0 表示变量 Y 相对于 X 没有增加或减少的倾向。随着 X 和 Y 越来越接近完美单调函数，Spearman 相关性的大小会增加。只有在数据未被截断的情况下，才能计算 Spearman 相关性。虽然对极值不太敏感，但它只依赖于秩而不是观察。

### 欧几里得距离

欧几里德距离是两个变量的向量元素之间的平方差之和的平方根:

![A978-1-4302-5990-9_2_Figbh_HTML.jpg](A978-1-4302-5990-9_2_Figbh_HTML.jpg)

如果两个变量在同一尺度上测量，欧几里得距离是有效的。您可以将等式 2-21 中的距离转换为逆形式(见等式 2-22)，这样，如果 X 和 Y(X–Y = 0)相似，则它将返回值 1，如果相似性降低，则趋向于 0:

![A978-1-4302-5990-9_2_Figbi_HTML.jpg](A978-1-4302-5990-9_2_Figbi_HTML.jpg)

您可以验证如果距离 d(X，Y) = 0(表示相似)，则![A978-1-4302-5990-9_2_Figbj_HTML.jpg](A978-1-4302-5990-9_2_Figbj_HTML.jpg)计算到值 1，如果 d(X，Y)增加(表示不相似)，则![A978-1-4302-5990-9_2_Figbk_HTML.jpg](A978-1-4302-5990-9_2_Figbk_HTML.jpg)减小到 0。

### 雅克卡相似系数

Jaccard 相似性系数通过测量有限样本集 X 和 Y 之间的重叠来度量它们之间的相似性。集合 X 和 Y 的大小不必相同。在数学上，该系数可以定义为样本集(X，Y)的交集与并集之比:

![A978-1-4302-5990-9_2_Figbl_HTML.jpg](A978-1-4302-5990-9_2_Figbl_HTML.jpg)

Jaccard 距离度量样本集之间的不相似性，并通过从 1 中减去 Jaccard 系数来获得:

![A978-1-4302-5990-9_2_Figbm_HTML.jpg](A978-1-4302-5990-9_2_Figbm_HTML.jpg)

Jaccard 系数通常用于测量关键字相似性、文档相似性、新闻文章分类、自然语言处理(NLP)等。

## 摘要

复杂问题的解决依赖于机器学习技术的智能使用。解决方案的精确度、速度和准确性可以通过采用不仅降低特征维度，而且训练特定于独特行为的模型的技术来提高。通过使用一种聚类技术，例如 k-means，可以将不同的行为属性聚类成阶段。对应于每个聚类标签的减少的数据点被分离和训练以解决回归或分类问题。在正常的训练后操作中，一旦识别了阶段，就使用与该阶段相关联的训练模型来预测(或估计)反馈回路的输出。

图 [2-5](#Fig5) 总结了能够感应大量传感器的过程控制系统，以便控制环境过程(例如，数据中心的冷却)。第一步是降低数据的维数。新数据被输入到聚类方法中，该方法根据相似的属性和不同的特性，从给定的集合中发现一个组的项目。对应于每个聚类标签的数据被单独分离和训练用于分类。阶段识别允许应用与识别的阶段相关联的模型函数。特定相位模型的输出触发过程控制功能，该功能作用于环境并改变传感器输出。此外，该程序使我们能够主动预测当前阶段持续时间和即将到来的阶段，并相应地预测主动控制的输出。

![A978-1-4302-5990-9_2_Fig5_HTML.gif](A978-1-4302-5990-9_2_Fig5_HTML.gif)

图 2-5。

Machine learning–based feedback control system: features are transformed and fed into phase detectors; the data classification process employs models trained on the detected phase

## 参考

模糊目标函数演算法的模式辨识。马萨诸塞州诺威尔:克鲁沃，1981 年。

布雷曼，利奥。“随机森林。”《机器学习》45 卷，第 1 期(2001 年):第 5–32 页。

Isodata 过程的模糊相关及其在检测紧密的良好分离的聚类中的应用。控制论 3(1973):32–57。

萨瓦，巴德瑞尔，乔治·卡里皮，约瑟夫·康斯坦和约翰·里德。"基于项目的协同过滤推荐算法."《第十届万维网国际会议论文集》, 285–295 页。纽约:美国计算机学会，2001 年。

周、、丹尼斯·威尔金森、罗伯特·施雷伯和潘蓉。" Netflix 奖的大规模并行协同过滤."《信息和管理中的算法方面》，Rudof Fleischer 和徐进辉编辑，2008 年 6 月 23 日至 25 日在中国 AAIM 召开的第四届国际会议论文集，337-348 页。柏林:施普林格，2008 年。